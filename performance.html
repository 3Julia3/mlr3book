<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="author" content="Giuseppe Casalicchio">
<meta name="author" content="Lukas Burk">
<title>Flexible and Robust Machine Learning Using mlr3 in R - 3&nbsp; Evaluation, Resampling and Benchmarking</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./optimization.html" rel="next">
<link href="./basics.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script><style>html{ scroll-behavior: smooth; }</style>
<script async="" src="https://hypothes.is/embed.js"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
</head>
<body class="nav-sidebar floating slimcontent">


<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }"><div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation, Resampling and Benchmarking</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Flexible and Robust Machine Learning Using mlr3 in R</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/mlr-org/mlr3book/tree/main/book/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
    <a href="./Flexible-and-Robust-Machine-Learning-Using-mlr3-in-R.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Getting Started</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preface.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction and Overview</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./basics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./performance.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation, Resampling and Benchmarking</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./feature-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Feature Selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pipelines.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Pipelines</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./preprocessing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Preprocessing</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./special.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Special Tasks</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./technical.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Technical</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./interpretation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Model Interpretation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./extending.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Extending</span></a>
  </div>
</li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">Appendices</a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./solutions.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Solutions to exercises</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glossary.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Glossary</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tasks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">C</span>&nbsp; <span class="chapter-title">Tasks</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./overview-tables.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">D</span>&nbsp; <span class="chapter-title">Overview Tables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">E</span>&nbsp; <span class="chapter-title">Session Info</span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><!-- margin-sidebar --><div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#quick-start" id="toc-quick-start" class="nav-link active" data-scroll-target="#quick-start"><span class="toc-section-number">3.1</span>  Quick Start</a></li>
  <li>
<a href="#sec-resampling" id="toc-sec-resampling" class="nav-link" data-scroll-target="#sec-resampling"><span class="toc-section-number">3.2</span>  Resampling</a>
  <ul class="collapse">
<li><a href="#sec-resampling-strategies" id="toc-sec-resampling-strategies" class="nav-link" data-scroll-target="#sec-resampling-strategies"><span class="toc-section-number">3.2.1</span>  Query</a></li>
  <li><a href="#sec-resampling-construct" id="toc-sec-resampling-construct" class="nav-link" data-scroll-target="#sec-resampling-construct"><span class="toc-section-number">3.2.2</span>  Construction</a></li>
  <li><a href="#sec-resampling-inst" id="toc-sec-resampling-inst" class="nav-link" data-scroll-target="#sec-resampling-inst"><span class="toc-section-number">3.2.3</span>  Instantiation</a></li>
  <li><a href="#sec-resampling-exec" id="toc-sec-resampling-exec" class="nav-link" data-scroll-target="#sec-resampling-exec"><span class="toc-section-number">3.2.4</span>  Execution</a></li>
  <li><a href="#sec-resampling-inspect" id="toc-sec-resampling-inspect" class="nav-link" data-scroll-target="#sec-resampling-inspect"><span class="toc-section-number">3.2.5</span>  Inspect ResampleResult Objects</a></li>
  <li><a href="#sec-resamp-custom" id="toc-sec-resamp-custom" class="nav-link" data-scroll-target="#sec-resamp-custom"><span class="toc-section-number">3.2.6</span>  Custom Resampling</a></li>
  <li><a href="#resampling-with-stratification-and-grouping" id="toc-resampling-with-stratification-and-grouping" class="nav-link" data-scroll-target="#resampling-with-stratification-and-grouping"><span class="toc-section-number">3.2.7</span>  Resampling with Stratification and Grouping</a></li>
  <li><a href="#sec-autoplot-resampleresult" id="toc-sec-autoplot-resampleresult" class="nav-link" data-scroll-target="#sec-autoplot-resampleresult"><span class="toc-section-number">3.2.8</span>  Plotting Resample Results</a></li>
  </ul>
</li>
  <li>
<a href="#sec-benchmarking" id="toc-sec-benchmarking" class="nav-link" data-scroll-target="#sec-benchmarking"><span class="toc-section-number">3.3</span>  Benchmarking</a>
  <ul class="collapse">
<li><a href="#sec-bm-design" id="toc-sec-bm-design" class="nav-link" data-scroll-target="#sec-bm-design"><span class="toc-section-number">3.3.1</span>  Constructing Benchmarking Designs</a></li>
  <li><a href="#sec-bm-exec" id="toc-sec-bm-exec" class="nav-link" data-scroll-target="#sec-bm-exec"><span class="toc-section-number">3.3.2</span>  Execution of Benchmark Experiments</a></li>
  <li><a href="#sec-bm-resamp" id="toc-sec-bm-resamp" class="nav-link" data-scroll-target="#sec-bm-resamp"><span class="toc-section-number">3.3.3</span>  Inspect BenchmarkResult Objects</a></li>
  <li><a href="#statistical-tests" id="toc-statistical-tests" class="nav-link" data-scroll-target="#statistical-tests"><span class="toc-section-number">3.3.4</span>  Statistical Tests</a></li>
  </ul>
</li>
  <li>
<a href="#sec-roc" id="toc-sec-roc" class="nav-link" data-scroll-target="#sec-roc"><span class="toc-section-number">3.4</span>  ROC Analysis</a>
  <ul class="collapse">
<li><a href="#confusion-matrix-based-measures" id="toc-confusion-matrix-based-measures" class="nav-link" data-scroll-target="#confusion-matrix-based-measures"><span class="toc-section-number">3.4.1</span>  Confusion Matrix-based Measures</a></li>
  <li><a href="#roc-space" id="toc-roc-space" class="nav-link" data-scroll-target="#roc-space"><span class="toc-section-number">3.4.2</span>  ROC Space</a></li>
  </ul>
</li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span class="toc-section-number">3.5</span>  Conclusion</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">3.6</span>  Exercises</a></li>
  </ul><div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/mlr-org/mlr3book/edit/main/book/performance.qmd" class="toc-action">Edit this page</a></p><p><a href="https://github.com/mlr-org/mlr3book/issues/new" class="toc-action">Report an issue</a></p><p><a href="https://github.com/mlr-org/mlr3book/blob/main/book/performance.qmd" class="toc-action">View source</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title"><span id="sec-performance" class="quarto-section-identifier d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Evaluation, Resampling and Benchmarking</span></span></h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Authors</div>
  <div class="quarto-title-meta-heading">Affiliations</div>
  
    <div class="quarto-title-meta-contents">
    Giuseppe Casalicchio <a href="https://orcid.org/0000-0001-5324-5966" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Ludwig-Maximilians-Universität München
          </p>
        <p class="affiliation">
            Munich Center for Machine Learning (MCML)
          </p>
        <p class="affiliation">
            Essential Data Science Training GmbH
          </p>
      </div>
      <div class="quarto-title-meta-contents">
    Lukas Burk <a href="https://orcid.org/0000-0001-7528-3795" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            Ludwig-Maximilians-Universität München
          </p>
        <p class="affiliation">
            Leibniz Institute for Prevention Research and Epidemiology - BIPS
          </p>
        <p class="affiliation">
            Munich Center for Machine Learning (MCML)
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  
<div>
  <div class="abstract">
    <div class="abstract-title">Abstract</div>
    <p>Estimating the generalization performance of a machine learning algorithm on a given task requires additional data not used during training. Resampling refers to the process of repeatedly splitting the available data into training and test sets to enable unbiased performance estimation. This chapter introduces common resampling strategies and illustrates their use with the <code>mlr3</code> ecosystem. Benchmarking builds upon resampling, encompassing the fair comparison of multiple machine learning algorithms on at least one task. We show how benchmarking can be performed within the <code>mlr3</code> ecosystem, from the construction of benchmark designs to the statistical analysis of the benchmark results.</p>
  </div>
</div>

</header><!-- TODO: check links e.g. autoplot etc. --><!-- TODO: Check crossrefs, e.g. (#measures) lead so chapter "Special Tasks" --><!-- why performance estimation, what is a performance measure --><p>In supervised machine learning, a model which is deployed in practice is expected to generalize well to new, unseen data. Accurate estimation of this so-called generalization performance is crucial for many aspects of machine learning application and research — whether we want to fairly compare a novel algorithm with established ones or to find the best algorithm for a particular task after tuning — we always rely on this performance estimate. Hence, performance estimation is a fundamental concept used for model selection, model comparison, and hyperparameter tuning (which will be discussed in depth in <a href="optimization.html"><span>Chapter&nbsp;4</span></a>) in supervised machine learning. To properly assess the generalization performance of a model, we must first decide on a performance measure that is appropriate for our given task and evaluation goal. A performance measure typically computes a numeric score indicating, e.g., how well the model predictions match the ground truth. However, it may also reflect other qualities such as the time for training a model. An overview of some common performance measures implemented in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, including a short description and a basic mathematical definition, can be found by following the link provided in the overview table under <em>Measures overview</em> in Appendix <a href="overview-tables.html"><span>Appendix&nbsp;D</span></a>.</p>
<!-- motivate the need for splitting the data -->
<p>Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance. Unfortunately, using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate. For example, an overfitted model may perfectly fit the data on which it was trained, but may not generalize well to new data. Assessing its performance using the same data it was trained would misleadingly suggest a well-performing model. It is therefore common practice to test a model on independent data not used to train a model. However, we typically train a deployed model on all available data, which leaves no data to assess its generalization performance. To address this issue, existing performance estimation strategies withhold a subset of the available data for evaluation purposes. This so-called test set serves as unseen data and is used to estimate the generalization performance. <!-- is then used to mimic the presence of unseen data and to estimate the generalization performance. --> <!-- To overcome this issue, existing strategies for estimating the generalization performance preserve a subset of the data to mimic the presence of unseen data. --></p>
<!-- describe performance estimation procedure and desiderata -->
<p>A common simple strategy is the holdout method, which randomly partitions the data into a single training and test set using a pre-defined splitting ratio. The training set is used to create an intermediate model, whose sole purpose is to estimate the performance using the test set. This performance estimate is then used as a proxy for the performance of the final model trained on all available data and deployed in practice. Ideally, the training set should be as large as all available data so that the intermediate model represents the final model well. If the training data is much smaller, the intermediate model learns less complex relationships compared to the final model, resulting in a pessimistically biased performance estimate. On the other hand, we also want as much test data as possible to reliably estimate the generalization performance. However, both goals are not possible if we have only access to a limited amount of data.</p>
<!-- explain resampling -->
<p>To address this issue, resampling strategies (see <a href="#sec-resampling"><span>Section&nbsp;3.2</span></a>) repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>. An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration. The generalization performance is finally estimated by the averaged performance over multiple resampling iterations (see <a href="#fig-ml-abstraction">Figure&nbsp;<span>3.1</span></a> for an illustration). <!-- Removed reference to exercise since we discussed that this aspect is too important to leave to an exercise, maybe this should be a gallery post or even link to I2ML? --> Resampling methods allow using more data points for testing, while keeping the training sets as large as possible. Specifically, repeating the data splitting process allows using all available data points to assess the performance, as each data point can be ensured to be part of the test set in at least one resampling iteration. A higher number of resampling iterations can reduce the variance and result in a more reliable performance estimate. It also reduces the risk of the performance estimate being strongly affected by an unlucky split that does not reflect the original data distribution well, which is a known issue of the holdout method. However, since resampling strategies create multiple intermediate models trained on different parts of the available data and average their performance, they evaluate the performance of the learning algorithm that induced these models, rather than the performance of the final model which is deployed in practice. It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar. If we only have access to a limited amount of data, the best we can do is to use the performance of the learning algorithm as a proxy for the performance of the final model. In <a href="#sec-resampling"><span>Section&nbsp;3.2</span></a>, we will learn how to estimate the generalization performance of a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> using the <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> package. <!-- For example, the $k$-fold cross-validation method randomly partitions the data into $k$ subsets, called folds (see @fig-cv-illustration). --> <!-- Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations. --> <!-- The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate. --> <!-- Several variations of cross-validation exist, including repeated $k$-fold cross-validation where the entire process illustrated in @fig-cv-illustration is repeated multiple times and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.  --> <!-- Resampling strategies such as $k$-fold cross-validation are usually preferable to a single train-test split because they use all available data to assess the performance and thereby avoid the performance estimate from being biased by a particular split. --> <!-- These strategies evaluate the performance of the learning algorithm that induced the final model, rather than the performance of the final model itself. --> <!-- However, the best we can do if we only have access to a limited amount of data is to use the performance of the learning algorithm as a proxy for the performance of the final model. --></p>
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-ml-abstraction_c9ea7731e2072462787b43618acce0dc">
<div class="cell-output-display">
<div id="fig-ml-abstraction" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/ml_abstraction-2.svg" class="img-fluid figure-img" alt="A general abstraction of the performance estimation process: The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is applied to each training data and produces intermediate models (learning process). Each intermediate model along with its associated test data produces predictions. The performance measure compares these predictions with the associated actual target values from each test data and computes a performance value for each test data. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process)."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: A general abstraction of the performance estimation process: The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is applied to each training data and produces intermediate models (learning process). Each intermediate model along with its associated test data produces predictions. The performance measure compares these predictions with the associated actual target values from each test data and computes a performance value for each test data. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<!-- TODO: Add a section at the end showing some properties of resampling strategies e.g. bias-variance trade-off of different resamplings with different sizes of training-test set (see i2ml) -->
<!-- chapter outline -->
<section id="quick-start" class="level2" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="quick-start">
<span class="header-section-number">3.1</span> Quick Start</h2>
<p>In the previous chapter, we have applied the holdout method by manually partitioning the data contained in a <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a> object into a single training set (to train the model) and a single test set (to estimate the generalization performance). As a quick start into resampling and benchmarking with the <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> package, we show a short example of how to do this with the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> and <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> convenience functions. Specifically, we show how to estimate the generalization performance of a learner on a given task by the holdout method using <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> and how to use <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> to compare two learners on a task.</p>
<p>We first define the corresponding <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a> and <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> objects used throughout this chapter as follows:</p>
<div class="cell" data-hash="performance_cache/html/performance-003_991afdca2d0c975936dbe825337d3076">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb1-2"><a href="#cb1-2"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<!-- #### Further Reading -->
<!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. -->
<!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). -->
<p>The next obvious step is to select a suitable performance measure, which can be done as explained in <a href="basics.html#sec-eval"><span>Section&nbsp;2.3</span></a> using the <a href="https://mlr3.mlr-org.com/reference/mlr_measures.html"><code>mlr_measures</code></a> dictionary. Passing the dictionary to the <code>as.data.table</code> function provides an overview of implemented measures with additional information from which we can select a suitable performance measure, which we print here in two parts for compactness:</p>
<div class="cell" data-hash="performance_cache/html/performance-004_ee76646e3edf26bdd7f1f915662413f0">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a>msr_tbl <span class="ot">=</span> <span class="fu">as.data.table</span>(mlr_measures)</span>
<span id="cb2-2"><a href="#cb2-2"></a>msr_tbl[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, .(key, label, task_type)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            key                          label task_type
1:          aic   Akaika Information Criterion      &lt;NA&gt;
2:          bic Bayesian Information Criterion      &lt;NA&gt;
3:  classif.acc        Classification Accuracy   classif
4:  classif.auc       Area Under the ROC Curve   classif
5: classif.bacc              Balanced Accuracy   classif</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a>msr_tbl[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, .(key, packages, predict_type, task_properties)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            key          packages predict_type task_properties
1:          aic              mlr3     response                
2:          bic              mlr3     response                
3:  classif.acc mlr3,mlr3measures     response                
4:  classif.auc mlr3,mlr3measures         prob        twoclass
5: classif.bacc mlr3,mlr3measures     response                </code></pre>
</div>
</div>
<p>Depending on our task at hand, we will look for a measure that fits our <code>"task_type"</code> (<code>"classif"</code> for <code>penguins</code>) and <code>"task_properties"</code>. The latter is important since measures like AUC <code>"classif.auc"</code> are only defined for binary tasks, which is indicated by <code>"twoclass"</code> in the <code>"task_properties"</code> column — multiclass-generalizations are available, but need to be selected explicitly. Similarly, some measures require the learner to predict probabilities, while others require class predictions. In our learner above, we have already selected <code>predict_type = "prob"</code>, which is often required for measures that are not defined on class labels, such as the aforementioned AUC.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>More information about a performance measure, including its mathematical definition, can be obtained using the <code>$help()</code> method of a <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> object, which opens the help page of the corresponding measure, e.g., <code>msr("classif.acc")$help()</code> provides all information about the classification accuracy.</p>
</div>
</div>
<p>The code example below shows how to apply holdout (specified using <code>rsmp("holdout")</code>) on the previously specified <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html"><code>mlr_tasks_penguins</code></a> task to estimate classification accuracy (using <code>msr("classif.acc")</code>) of the previously defined decision tree learner from the <a href="https://cran.r-project.org/package=rpart"><code>rpart</code></a> package:</p>
<div class="cell" data-hash="performance_cache/html/performance-005_154584709ced8496dc81c046404637cf">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>)</span>
<span id="cb6-2"><a href="#cb6-2"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(<span class="at">task =</span> task, <span class="at">learner =</span> learner, <span class="at">resampling =</span> resampling)</span>
<span id="cb6-3"><a href="#cb6-3"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9391304 </code></pre>
</div>
</div>
<p>The <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> function internally uses the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function to estimate the performance based on a resampling strategy. For illustration, we show a minimal code example that compares the classification accuracy of the decision tree against a featureless learner which always predicts the majority class:</p>
<div class="cell" data-hash="performance_cache/html/performance-006_9c0f015b1c7ef3d749d896bb851a7a79">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>lrns <span class="ot">=</span> <span class="fu">c</span>(learner, <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>))</span>
<span id="cb8-2"><a href="#cb8-2"></a>d <span class="ot">=</span> <span class="fu">benchmark_grid</span>(<span class="at">task =</span> task, <span class="at">learner =</span> lrns, <span class="at">resampling =</span> resampling)</span>
<span id="cb8-3"><a href="#cb8-3"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="at">design =</span> d)</span>
<span id="cb8-4"><a href="#cb8-4"></a>acc <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb8-5"><a href="#cb8-5"></a>acc[, .(task_id, learner_id, classif.acc)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    task_id          learner_id classif.acc
1: penguins       classif.rpart   0.9565217
2: penguins classif.featureless   0.4695652</code></pre>
</div>
</div>
<p>Further details on resampling and benchmarking can be found in <a href="#sec-resampling"><span>Section&nbsp;3.2</span></a> and <a href="#sec-benchmarking"><span>Section&nbsp;3.3</span></a>.</p>
</section><section id="sec-resampling" class="level2" data-number="3.2"><h2 data-number="3.2" class="anchored" data-anchor-id="sec-resampling">
<span class="header-section-number">3.2</span> Resampling</h2>
<p>Existing resampling strategies differ in how they partition the available data into training and test set, and a comprehensive overview can be found in <span class="citation" data-cites="japkowicz2011evaluating">Japkowicz and Shah (<a href="references.html#ref-japkowicz2011evaluating" role="doc-biblioref">2011</a>)</span>. For example, the <span class="math inline">\(k\)</span>-fold cross-validation method randomly partitions the data into <span class="math inline">\(k\)</span> subsets, called folds (see <a href="#fig-cv-illustration">Figure&nbsp;<span>3.2</span></a>). Then <span class="math inline">\(k\)</span> models are trained on training data consisting of <span class="math inline">\(k-1\)</span> of the folds, with the remaining fold being used as test data exactly once in each of the <span class="math inline">\(k\)</span> iterations. The <span class="math inline">\(k\)</span> performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate. This makes cross-validation a popular strategy, as each observation is guaranteed to be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation. <!-- TODO: See BB comment, pro-cons of CV --> Several variations of cross-validation exist, including repeated <span class="math inline">\(k\)</span>-fold cross-validation where the entire process illustrated in <a href="#fig-cv-illustration">Figure&nbsp;<span>3.2</span></a> is repeated multiple times, and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.</p>
<p>Other well-known resampling strategies include subsampling and bootstrapping. Subsampling — also known as repeated holdout — repeats the holdout method and creates multiple train-test splits, taking into account the ratio of observations to be included in the training sets. Bootstrapping creates training sets by randomly drawing observations from all available data with replacement. Some observations in the training sets may appear more than once, while the other observations that do not appear at all are used as test set. The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment. <!-- Japkowicz and Shah (2011) make a distinction between resampling strategies that use observations from the test set only once and resampling strategies that use observations from the test set multiple times. --> Properties and pitfalls of different resampling techniques have been widely studied and discussed in the literature, see e.g., <span class="citation" data-cites="bengio2003no">Bengio and Grandvalet (<a href="references.html#ref-bengio2003no" role="doc-biblioref">2003</a>)</span>, <span class="citation" data-cites="molinaro2005prediction">Molinaro, Simon, and Pfeiffer (<a href="references.html#ref-molinaro2005prediction" role="doc-biblioref">2005</a>)</span>, <span class="citation" data-cites="kim2009estimating">Kim (<a href="references.html#ref-kim2009estimating" role="doc-biblioref">2009</a>)</span>, <span class="citation" data-cites="bischl2012resampling">Bischl et al. (<a href="references.html#ref-bischl2012resampling" role="doc-biblioref">2012</a>)</span>. <!-- TODO: See BB comment, summarize paper content a bit, maybe in separate section? --></p>
<!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing -->
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-cv-illustration_898e80f3e70ec9573fe406f8b96fd74e">
<div class="cell-output-display">
<div id="fig-cv-illustration" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/cross-validation.svg" class="img-fluid figure-img" alt="A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.2: Illustration of a 3-fold cross-validation.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, many resampling strategies have already been implemented so that users do not have to implement them from scratch, which can be tedious and error-prone. In this section, we cover how to use <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> to</p>
<ul>
<li>query (<a href="#sec-resampling-strategies"><span>Section&nbsp;3.2.1</span></a>) implemented resampling strategies,</li>
<li>construct (<a href="#sec-resampling-construct"><span>Section&nbsp;3.2.2</span></a>) resampling objects for a selected resampling strategy,</li>
<li>instantiate (<a href="#sec-resampling-inst"><span>Section&nbsp;3.2.3</span></a>) the train-test splits of a resampling object on a given task, and</li>
<li>execute (<a href="#sec-resampling-exec"><span>Section&nbsp;3.2.4</span></a>) the selected resampling strategy on a learning algorithm to obtain resampling results.</li>
</ul>
<section id="sec-resampling-strategies" class="level3" data-number="3.2.1"><h3 data-number="3.2.1" class="anchored" data-anchor-id="sec-resampling-strategies">
<span class="header-section-number">3.2.1</span> Query</h3>
<p>All implemented resampling strategies can be queried by looking at the <a href="https://mlr3.mlr-org.com/reference/mlr_resamplings.html"><code>mlr_resamplings</code></a> dictionary (also listed in Appendix <a href="overview-tables.html"><span>Appendix&nbsp;D</span></a>). Passing the dictionary to the <code>as.data.table</code> function provides a more structured output with additional information:</p>
<div class="cell" data-hash="performance_cache/html/performance-008_ae1f23878c953fce014b3ea01a6076f9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="fu">as.data.table</span>(mlr_resamplings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           key                         label        params iters
1:   bootstrap                     Bootstrap ratio,repeats    30
2:      custom                 Custom Splits                  NA
3:   custom_cv Custom Split Cross-Validation                  NA
4:          cv              Cross-Validation         folds    10
5:     holdout                       Holdout         ratio     1
6:    insample           Insample Resampling                   1
7:         loo                 Leave-One-Out                  NA
8: repeated_cv     Repeated Cross-Validation folds,repeats   100
9: subsampling                   Subsampling ratio,repeats    30</code></pre>
</div>
</div>
<p>For example, the column <code>params</code> shows the parameters of each resampling strategy (e.g., the train-test splitting <code>ratio</code> or the number of <code>repeats</code>) and the column <code>iters</code> shows the default value for the number of performed resampling iterations (i.e., the number of model fits).</p>
</section><section id="sec-resampling-construct" class="level3" data-number="3.2.2"><h3 data-number="3.2.2" class="anchored" data-anchor-id="sec-resampling-construct">
<span class="header-section-number">3.2.2</span> Construction</h3>
<p>Once we have decided on a resampling strategy, we have to construct a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object via the function <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>rsmp()</code></a> which will define the resampling strategy we want to employ. For example, to construct a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object for holdout, we use the value of the <code>key</code> column from the <a href="https://mlr3.mlr-org.com/reference/mlr_resamplings.html"><code>mlr_resamplings</code></a> dictionary and pass it to the convenience function <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>rsmp()</code></a>:</p>
<div class="cell" data-hash="performance_cache/html/performance-009_ba5dc34f2dde636a7f84685ed3268662">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>)</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">print</span>(resampling)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResamplingHoldout&gt;: Holdout
* Iterations: 1
* Instantiated: FALSE
* Parameters: ratio=0.6667</code></pre>
</div>
</div>
<p>By default, the holdout method will use 2/3 of the data as training set and 1/3 as test set. We can adjust this by specifying the <code>ratio</code> parameter for holdout either during construction or by updating the <code>ratio</code> parameter afterwards. For example, we construct a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):</p>
<div class="cell" data-hash="performance_cache/html/performance-010_176bc00b545e2b95a797cd5534c84852">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb14-2"><a href="#cb14-2"></a>resampling<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> <span class="fu">list</span>(<span class="at">ratio =</span> <span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Holdout only estimates the generalization performance using a single test set. To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies. For example, we could also set up a 10-fold cross-validation via</p>
<div class="cell" data-hash="performance_cache/html/performance-011_c865ab20949e09ec497815bebada2a1b">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>By default, the <code>$is_instantiated</code> field of a <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object constructed as shown above is set to <code>FALSE</code>. This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object. <!-- When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy. --></p>
<!-- In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue. -->
<!-- If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly. -->
</section><section id="sec-resampling-inst" class="level3" data-number="3.2.3"><h3 data-number="3.2.3" class="anchored" data-anchor-id="sec-resampling-inst">
<span class="header-section-number">3.2.3</span> Instantiation</h3>
<!-- In this section, we show how to instantiate a resampling strategy (i.e., how to generate the train-test splits) by applying it to a task. -->
<!-- To obtain the row indices for the training and the test splits, we need to call the `instantiate()` method on a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) object. -->
<!-- The resulting train-test indices are then stored in the [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) object: -->
<p>To generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the <code>$instantiate()</code> method of the previously constructed <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object on a <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a>. This will manifest a fixed partition and store the row indices for the training and test sets directly in the <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object. We can access these rows via the <code>$train_set()</code> and <code>$test_set()</code> methods:</p>
<div class="cell" data-hash="performance_cache/html/performance-012_bace4f5647a6540d6f568e8c09bfd612">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb16-2"><a href="#cb16-2"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb16-3"><a href="#cb16-3"></a>train_ids <span class="ot">=</span> resampling<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>)</span>
<span id="cb16-4"><a href="#cb16-4"></a>test_ids <span class="ot">=</span> resampling<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>)</span>
<span id="cb16-5"><a href="#cb16-5"></a><span class="fu">str</span>(train_ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> int [1:275] 2 3 4 5 6 7 8 9 10 11 ...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1"></a><span class="fu">str</span>(test_ids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> int [1:69] 1 12 16 22 26 28 30 32 37 38 ...</code></pre>
</div>
</div>
<p>Instantiation is especially relevant is when the aim is to fairly compare multiple learners. Here, it is crucial to use the same train-test splits to obtain comparable results. That is, we need to ensure that all learners to be compared use the same training data to build a model and that they use the same test data to evaluate the model performance.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In <a href="#sec-benchmarking"><span>Section&nbsp;3.3</span></a>, you will learn about the <code>ref ("benchmark()")</code> function, which automatically instantiates <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> objects on all tasks to ensure a fair comparison by making use of the exact same training and test sets for learning and evaluating the fitted intermediate models.</p>
</div>
</div>
<!-- Cut in favor of infobox, possibly reintegrate?
This can be achieved using the same instantiated [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) object for each learner or using the [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) function introduced in @sec-benchmarking which automatically instantiates the same train-test splits for each task.
-->
</section><section id="sec-resampling-exec" class="level3" data-number="3.2.4"><h3 data-number="3.2.4" class="anchored" data-anchor-id="sec-resampling-exec">
<span class="header-section-number">3.2.4</span> Execution</h3>
<!-- With a [`Task`](https://mlr3.mlr-org.com/reference/Task.html), a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), and a [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations. -->
<!-- For this, we call the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function which returns a [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) object. -->
<p>Calling the function <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> on a task, learner, and constructed resampling object returns a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object which contains all information needed to estimate the generalization performance. Specifically, the function will internally use the learner to train a model for each training set determined by the resampling strategy and store the model predictions of each test set. We can apply the <code>print</code> or <code>as.data.table</code> function to a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object to obtain some basic information: <!-- By default, these models are discarded after the prediction step to reduce memory consumption of the resulting [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) object and because we usually only need the stored predictions to calculate the performance measure. --> <!-- However, we can configure the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function to keep the fitted models (e.g. if we want to use or inspect the intermediate models later) by setting the `store_models` argument of the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function to `TRUE`: --></p>
<div class="cell" data-hash="performance_cache/html/performance-013_c35ca4e248866ee449a607bcc8652c7f">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">4</span>)</span>
<span id="cb20-2"><a href="#cb20-2"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling)</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="fu">print</span>(rr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; of 4 iterations
* Task: penguins
* Learner: classif.rpart
* Warnings: 0 in 0 iterations
* Errors: 0 in 0 iterations</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1"></a><span class="fu">as.data.table</span>(rr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                   learner         resampling iteration
1: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         1
2: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         2
3: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         3
4: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         4
1 variable not shown: [prediction]</code></pre>
</div>
</div>
<p>Here, we used 4-fold cross-validation as resampling strategy. The resulting <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object (stored as <code>rr</code>) provides various methods to access the stored information. The two most relevant methods for performance assessment are <code>$score()</code> and <code>$aggregate()</code>.</p>
<p>In <a href="basics.html#sec-eval"><span>Section&nbsp;2.3</span></a>, we learned that <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> objects contain both model predictions and ground truth values, which are used to calculate the performance measure using the <code>$score()</code> method. Similarly, we can use the <code>$score()</code> method of a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object to calculate the performance measure for each resampling iteration separately. This means that the <code>$score()</code> method produces one value per resampling iteration that reflects the performance estimate of the intermediate model trained in the corresponding iteration. <!-- Specifically, it extracts the model predictions of each resampling iteration and calculates the performance measure in each resampling iteration separately. --> By default, <code>$score()</code> uses the test set in each resampling iteration to calculate the performance measure.</p>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We are not limited to scoring predictions on the test set — if we set the argument <code>predict_sets = "train"</code> within the <code>$score()</code> method, we calculate the performance measure of each resampling iteration based on the training set instead of the test set.</p>
</div>
</div>
<p>In the code example below, we explicitly use the classification accuracy (<code>classif.acc</code>) as performance measure and pass it to the <code>$score()</code> method to obtain the estimated performance of each resampling iteration separately:</p>
<div class="cell" data-hash="performance_cache/html/performance-014_efba16aff08f1406d1b9da750181bd77">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1"></a>acc <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb24-2"><a href="#cb24-2"></a>acc[, .(iteration, classif.acc)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   iteration classif.acc
1:         1   0.9418605
2:         2   0.9767442
3:         3   0.9069767
4:         4   0.9534884</code></pre>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we do not explicitly pass a <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> object to the <code>$score()</code> method, the classification error (<code>classif.ce</code>) and the mean squared error (<code>regr.mse</code>) are used as defaults for classification and regression tasks respectively.</p>
</div>
</div>
<p>Similarly, we can pass <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> objects to the <code>$aggregate()</code> method to calculate an aggregated score across all resampling iterations. The type of aggregation is usually determined by the <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> object (see also the fields <code>$average</code> and <code>$aggregator</code> the in help page of <a href="https://mlr3.mlr-org.com/reference/Measure.html"><code>Measure</code></a> for more details). There are two approaches for aggregating scores across resampling iterations: The first is referred to as the macro average, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations. The second approach is the micro average, which pools all predictions across resampling iterations into one <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> object and computes the measure on this directly. The classification accuracy <code>msr("classif.acc")</code> uses the macro-average by default, but the micro-average can be computed as well by specifying the <code>average</code> argument:</p>
<div class="cell" data-hash="performance_cache/html/performance-015_ede45e961af74bf2a0742eb840f7fd96">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9447674 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>, <span class="at">average =</span> <span class="st">"micro"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9447674 </code></pre>
</div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The classification accuracy compares the predicted class and the ground truth class of a single observation (point-wise loss) and calculates the proportion of correctly classified observations (average of point-wise loss). For performance measures that simply take the (unweighted) average of point-wise losses such as the classification accuracy, macro-averaging and micro-averaging will be equivalent unless the test sets in each resampling iteration have different sizes. For example, in the code example above, macro-averaging and micro-averaging yield the same classification accuracy because the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html"><code>mlr_tasks_penguins</code></a> task (consisting of 344 observations) is split into 4 equally-sized test sets (consisting of 86 observations each) due to the 4-fold cross-validation. If we would use 5-fold cross-validation instead, macro-averaging and micro-averaging can lead to a (slightly) different performance estimate as the test sets can not have the exact same size:</p>
<div class="cell" data-hash="performance_cache/html/performance-016_21e133bed435e403f71a08610790c42a">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1"></a>rr5 <span class="ot">=</span> <span class="fu">resample</span>(task, learner, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>))</span>
<span id="cb30-2"><a href="#cb30-2"></a>rr5<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9415601 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1"></a>rr5<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>, <span class="at">average =</span> <span class="st">"micro"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9418605 </code></pre>
</div>
</div>
<p>For other performance measures that are not defined on observation level but rather on a set of observations such as the area under the ROC curve <code>msr("classif.auc")</code>, macro-averaging and micro-averaging will usually always lead to different values.</p>
</div>
</div>
<!-- By default, the classification accuracy uses the macro average, i.e., the performance measure is calculated in each resampling iteration separately and then averaged to obtain the macro-averaged performance estimate. -->
<!-- i.e., the `$average` field of a performance measure specifies whether micro or macro averaging is used and the `$aggregator` specifies the function used to aggregate the individual performance values that are calculated in each resampling iteration. -->
<p>The aggregated score (as returned by <code>$aggregate()</code>) refers to the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object. While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the <code>$score()</code> method) as well, e.g., to see if one (or more) of the iterations lead to very different performance results. <a href="#fig-score-aggregate-resampling">Figure&nbsp;<span>3.3</span></a> visualizes the relationship between <code>$score()</code> and <code>$aggregate()</code> for a small example based on the <code>"penguins"</code> task.</p>
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-score-aggregate-resampling_744e33881d0047db5d664e4b0984e175">
<div class="cell-output-display">
<div id="fig-score-aggregate-resampling" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/predict-score-aggregate-resampling.drawio.svg" class="img-fluid figure-img" alt="A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.3: An example of the difference between <code>$score()</code> and <code>$aggregate()</code>: The former aggregates predictions to a single score within each resampling iteration, and the former aggregates scores across all resampling folds</figcaption><p></p>
</figure>
</div>
</div>
</div>
</section><section id="sec-resampling-inspect" class="level3" data-number="3.2.5"><h3 data-number="3.2.5" class="anchored" data-anchor-id="sec-resampling-inspect">
<span class="header-section-number">3.2.5</span> Inspect ResampleResult Objects</h3>
<!-- {{< include _optional.qmd >}} -->
<p>In this section, we show how to inspect some important fields and methods of a <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object. We first take a glimpse at what is actually contained in the object by converting it to a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a>:</p>
<div class="cell" data-hash="performance_cache/html/performance-018_d90664e7bc69428c901119ce57b311f6">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1"></a>rrdt <span class="ot">=</span> <span class="fu">as.data.table</span>(rr)</span>
<span id="cb34-2"><a href="#cb34-2"></a>rrdt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                   learner         resampling iteration
1: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         1
2: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         2
3: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         3
4: &lt;TaskClassif[50]&gt; &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;         4
1 variable not shown: [prediction]</code></pre>
</div>
</div>
<p>We can see that the task, learner and resampling strategy which we previously passed to the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function is stored in list columns of the <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a>. In addition, we also have an integer column <code>iteration</code> that refers to the resampling iteration and another list column that contains the corresponding <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> objects of each iteration. <!-- we are already familiar with, and which are needed to calculate performance measures. --> We can access the respective <code>prediction</code> column or directly use the <code>$predictions()</code> method of the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object (without converting it to a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a> first) to obtain a list of <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> objects of each resampling iteration:</p>
<div class="cell" data-hash="performance_cache/html/performance-019_32f13ba4c7b3c214e472452d94eb434a">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1"></a>rrdt<span class="sc">$</span>prediction</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
&lt;PredictionClassif&gt; for 86 observations:
    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
          4    Adelie    Adelie   0.9722222     0.02777778        0.00
          7    Adelie    Adelie   0.9722222     0.02777778        0.00
          8    Adelie    Adelie   0.9722222     0.02777778        0.00
---                                                                   
        328 Chinstrap Chinstrap   0.0800000     0.90000000        0.02
        330 Chinstrap Chinstrap   0.0800000     0.90000000        0.02
        339 Chinstrap Chinstrap   0.0800000     0.90000000        0.02

[[2]]
&lt;PredictionClassif&gt; for 86 observations:
    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
          2    Adelie    Adelie  0.95575221     0.04424779  0.00000000
          3    Adelie    Adelie  0.95575221     0.04424779  0.00000000
         10    Adelie    Adelie  0.95575221     0.04424779  0.00000000
---                                                                   
        326 Chinstrap Chinstrap  0.06818182     0.90909091  0.02272727
        329 Chinstrap Chinstrap  0.06818182     0.90909091  0.02272727
        333 Chinstrap Chinstrap  0.06818182     0.90909091  0.02272727

[[3]]
&lt;PredictionClassif&gt; for 86 observations:
    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
          1    Adelie    Adelie  0.97500000     0.02500000   0.0000000
          5    Adelie    Adelie  0.97500000     0.02500000   0.0000000
          6    Adelie    Adelie  0.97500000     0.02500000   0.0000000
---                                                                   
        340 Chinstrap    Gentoo  0.02197802     0.03296703   0.9450549
        341 Chinstrap    Adelie  0.97500000     0.02500000   0.0000000
        342 Chinstrap Chinstrap  0.02127660     0.97872340   0.0000000

[[4]]
&lt;PredictionClassif&gt; for 86 observations:
    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
         11    Adelie    Adelie  0.97321429     0.02678571  0.00000000
         12    Adelie    Adelie  0.97321429     0.02678571  0.00000000
         13    Adelie    Adelie  0.97321429     0.02678571  0.00000000
---                                                                   
        338 Chinstrap Chinstrap  0.06122449     0.91836735  0.02040816
        343 Chinstrap Chinstrap  0.14285714     0.57142857  0.28571429
        344 Chinstrap Chinstrap  0.06122449     0.91836735  0.02040816</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a href="#cb38-1"></a><span class="fu">all.equal</span>(rrdt<span class="sc">$</span>prediction, rr<span class="sc">$</span><span class="fu">predictions</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
</div>
<p>This allows to analyze the predictions of individual intermediate models from each resampling iteration and, e.g., to manually compute a macro-averaged performance estimate. Instead, we can use the <code>$prediction()</code> method to extract a single <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> object that combines the predictions of each intermediate model arcoss all resampling iterations. The combined prediction object can be used to manually compute a micro-averaged performance estimate, for example:</p>
<div class="cell" data-hash="performance_cache/html/performance-020_378a5ab68edc447c518db354b8eb8dbd">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a href="#cb40-1"></a>pred <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">prediction</span>()</span>
<span id="cb40-2"><a href="#cb40-2"></a>pred</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;PredictionClassif&gt; for 344 observations:
    row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo
          4    Adelie    Adelie  0.97222222     0.02777778  0.00000000
          7    Adelie    Adelie  0.97222222     0.02777778  0.00000000
          8    Adelie    Adelie  0.97222222     0.02777778  0.00000000
---                                                                   
        338 Chinstrap Chinstrap  0.06122449     0.91836735  0.02040816
        343 Chinstrap Chinstrap  0.14285714     0.57142857  0.28571429
        344 Chinstrap Chinstrap  0.06122449     0.91836735  0.02040816</code></pre>
</div>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1"></a>pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>classif.acc 
  0.9447674 </code></pre>
</div>
</div>
<p>By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object and because only the predictions are required to calculate the performance measure. However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models. To do so, we can configure the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function to keep the fitted intermediate models by setting the <code>store_models</code> argument to <code>TRUE</code>. Each model trained in a specific resampling iteration is then stored in the resulting <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object and can be accessed via <code>$learners[[i]]$model</code>, where <code>i</code> refers to the <code>i</code>-th resampling iteration:</p>
<div class="cell" data-hash="performance_cache/html/performance-021_d6107b99c601a06742900fadde08712a">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb44-2"><a href="#cb44-2"></a>rr<span class="sc">$</span>learners[[<span class="dv">1</span>]]<span class="sc">$</span>model</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>n= 258 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 258 142 Adelie (0.449612403 0.189922481 0.360465116)  
  2) flipper_length&lt; 206.5 163  48 Adelie (0.705521472 0.288343558 0.006134969)  
    4) bill_length&lt; 43.35 116   4 Adelie (0.965517241 0.034482759 0.000000000) *
    5) bill_length&gt;=43.35 47   4 Chinstrap (0.063829787 0.914893617 0.021276596) *
  3) flipper_length&gt;=206.5 95   3 Gentoo (0.010526316 0.021052632 0.968421053) *</code></pre>
</div>
</div>
<p>Here, we see the model output of a decision tree fitted by the <a href="https://cran.r-project.org/package=rpart"><code>rpart</code></a> package. As models fitted by <a href="https://cran.r-project.org/package=rpart"><code>rpart</code></a> provide information on how important features are, we can inspect if the importance varies across the resampling iterations:</p>
<div class="cell" data-hash="performance_cache/html/performance-022_b3a4c8f6dca7fa6dd6aa64d298f45b67">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1"></a><span class="fu">lapply</span>(rr<span class="sc">$</span>learners, <span class="cf">function</span>(x) x<span class="sc">$</span>model<span class="sc">$</span>variable.importance)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
flipper_length    bill_length     bill_depth      body_mass         island 
      98.05424       91.31166       73.88313       68.27175       51.43762 

[[2]]
   bill_length flipper_length     bill_depth      body_mass         island 
      97.11402       96.05475       76.89306       65.51456       57.70309 

[[3]]
flipper_length    bill_length     bill_depth      body_mass         island 
      94.82768       87.05866       77.18128       61.34559       49.95060 

[[4]]
   bill_length flipper_length     bill_depth      body_mass         island 
      92.44981       92.39868       74.20080       69.36664       57.28430 </code></pre>
</div>
</div>
<!-- - Filter the result and keep only results of certain resampling iterations, e.g., use `$filter(c(1, 3))` to discard the results of the second resampling iteration. -->
<p>Each resampling iteration involves a training step and a prediction step. Learner-specific error or warning messages may occur at each of these two steps. If the learner passed to the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function runs in an encapsulated framework that allows logging (see the <code>$encapsulate</code> field of a <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> object), all potential warning or error messages will be stored in the <code>$warnings</code> and <code>$errors</code> fields of the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> object.</p>
</section><section id="sec-resamp-custom" class="level3" data-number="3.2.6"><h3 data-number="3.2.6" class="anchored" data-anchor-id="sec-resamp-custom">
<span class="header-section-number">3.2.6</span> Custom Resampling</h3>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Some readers may want to skip this section of the book.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds. A custom resampling strategy can be constructed using <code>rsmp("custom")</code>, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task. In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the <code>$train</code> and <code>$test</code> fields.</p>
<div class="cell" data-hash="performance_cache/html/performance-023_544f90648de65b1f4c0ed68318e12d59">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom"</span>)</span>
<span id="cb48-2"><a href="#cb48-2"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task,</span>
<span id="cb48-3"><a href="#cb48-3"></a>  <span class="at">train =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, <span class="dv">151</span><span class="sc">:</span><span class="dv">333</span>)),</span>
<span id="cb48-4"><a href="#cb48-4"></a>  <span class="at">test =</span> <span class="fu">list</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>)</span>
<span id="cb48-5"><a href="#cb48-5"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The resulting <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object can then be used like all other resampling strategies. To show that both sets contain the row indices we have defined, we can inspect the instantiated <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> object:</p>
<div class="cell" data-hash="performance_cache/html/performance-024_3de5cd8e9c80605a30e27242029916da">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a><span class="fu">str</span>(resampling<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> int [1:233] 1 2 3 4 5 6 7 8 9 10 ...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a><span class="fu">str</span>(resampling<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> int [1:100] 51 52 53 54 55 56 57 58 59 60 ...</code></pre>
</div>
</div>
<p>The above is equivalent to a single custom train-test split analogous to the holdout strategy. A custom version of the cross-validation strategy can be constructed using <code>rsmp("custom_cv")</code>. The important difference is that we now have to specify either a custom <code>factor</code> variable (using the <code>f</code> argument of the <code>$instantiate()</code> method) or a <code>factor</code> column (using the <code>col</code> argument of the <code>$instantiate()</code> method) from the data to determine the folds.</p>
<p>In the example below, we instantiate a custom 4-fold cross-validation strategy using a <code>factor</code> variable called <code>folds</code> that contains 4 equally sized levels to define the 4 folds, each with one quarter of the total size of the <code>"penguin"</code> task:</p>
<div class="cell" data-hash="performance_cache/html/performance-025_757314f8fef9ac7180a530a3f594462e">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a>custom_cv <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom_cv"</span>)</span>
<span id="cb53-2"><a href="#cb53-2"></a>folds <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">each =</span> task<span class="sc">$</span>nrow<span class="sc">/</span><span class="dv">4</span>))</span>
<span id="cb53-3"><a href="#cb53-3"></a>custom_cv<span class="sc">$</span><span class="fu">instantiate</span>(task, <span class="at">f =</span> folds)</span>
<span id="cb53-4"><a href="#cb53-4"></a>custom_cv</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResamplingCustomCV&gt;: Custom Split Cross-Validation
* Iterations: 4
* Instantiated: TRUE
* Parameters: list()</code></pre>
</div>
</div>
</section><section id="resampling-with-stratification-and-grouping" class="level3" data-number="3.2.7"><h3 data-number="3.2.7" class="anchored" data-anchor-id="resampling-with-stratification-and-grouping">
<span class="header-section-number">3.2.7</span> Resampling with Stratification and Grouping</h3>
<div class="callout-important callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
This section of the book might be complex for some readers.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>In <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, we can assign a special role to a feature contained in the data by configuring the corresponding <code>$col_roles</code> field of a <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a>. The two relevant column roles that will affect behavior of a resampling strategy are <code>"group"</code> or <code>"stratum"</code>.</p>
<p>In some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belong to a group, e.g., when the data contains repeated measurements of individuals (longitudinal studies) or when dealing with spatial or temporal data. When observations belong to groups, we want to ensure that all observations of the same group belong to either the training set or the test set to prevent any potential leakage of information between training and testing sets. For example, in a longitudinal study, measurements of a person are usually taken at multiple time points. Grouping ensures that the model is tested on data from each person that it has not seen during training, while maintaining the integrity of the person’s measurements across different time points. In this context, the leave-one-out cross-validation strategy can be coarsened to the “leave-one-object-out” cross-validation strategy, where not only a single observation is left out, but all observations associated with a certain group (see <a href="#fig-group">Figure&nbsp;<span>3.4</span></a> for an illustration).</p>
<!-- where it is not one measurement that is left out, but all measurements associated with one particular individual, institution, or broader object of measurement. -->
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-group_293a35fd1930318f20f8526d4e2f5409">
<div class="cell-output-display">
<div id="fig-group" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/loobject.svg" class="img-fluid figure-img" alt="Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors)."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.4: Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>In <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, the column role <code>"group"</code> allows to specify the column in the data that defines the group structure of the observations (see also the help page of <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> for more information on the column role <code>"group"</code>). The column role can be specified by assigning a feature to the <code>$col_roles$group</code> field which will then determine the group structure. The following code performs leave-one-object-out cross-validation using the feature <code>year</code> of the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html"><code>mlr_tasks_penguins</code></a> task to determine the grouping. Since the feature <code>year</code> contains only three distinct values (i.e., <code>2007</code>, <code>2008</code>, and <code>2009</code>), the corresponding test sets consist of observations from only one year:</p>
<!-- In [`mlr3`](https://mlr3.mlr-org.com), we can assign a special column role to a feature contained in the data either during task construction or afterwards by specifying the feature that defines the group in the `$col_roles$group` field.  -->
<!-- For example, the column role `"group"` specifies which column in the data should be used to define the group structure of the observations (see also the help section on [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) for more information on the column role `"group"`). -->
<!-- A possible use case for the need of grouping (or blocking) of observations during resampling is spatiotemporal modeling, where observations inherit a natural grouping, either in space or time or in both space and time that need to be considered during resampling. -->
<div class="cell" data-hash="performance_cache/html/performance-027_451ffa662ac3d3a3a5b6b685db21ea44">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1"></a>task_grp <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb55-2"><a href="#cb55-2"></a>task_grp<span class="sc">$</span>col_roles<span class="sc">$</span>group <span class="ot">=</span> <span class="st">"year"</span></span>
<span id="cb55-3"><a href="#cb55-3"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"loo"</span>)</span>
<span id="cb55-4"><a href="#cb55-4"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task_grp)</span>
<span id="cb55-5"><a href="#cb55-5"></a></span>
<span id="cb55-6"><a href="#cb55-6"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"year"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2007 2008 2009 
 110  114  120 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2007 
 110 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2009 
 120 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>year
2008 
 114 </code></pre>
</div>
</div>
<!-- TODO: Do we keep this note or relegate spatiotempcv to a "further reading" section at the end? When we mention spatiotemp in the text anyway, the note should maybe be just regular text as well? -->
<!-- :::{.callout-tip appearance="simple"} -->
<!-- Dedicated spatiotemporal resampling methods are available in [`mlr3spatiotempcv`](https://mlr3spatiotempcv.mlr-org.com) which implicitly take into account the spatiotemporal structure, see the [spatiotemporal resampling](special.html#spatiotemp-cv) section for more details. -->
<!-- ::: -->
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>If there are many groups, say 100 groups, we can limit the number of resampling iterations using k-fold cross-validation (or any other resampling strategy with a previously definable number of resampling iterations) instead of performing leave-one-object-out cross-validation. In this case, each group is considered as a single observation, so that the division into training and test sets is done as determined by the resampling strategy</p>
</div>
</div>
<p>Another column role available in <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> is <code>"stratum"</code>, which implements stratified sampling. Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations. This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration. Stratification is commonly used for imbalanced classification tasks where the classes of the target feature are imbalanced (see <a href="#fig-stratification">Figure&nbsp;<span>3.5</span></a> for an illustration). Stratification by the target feature ensures that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task. Otherwise it could happen that target classes are severely under- or over represented in individual resampling iterations, skewing the estimation of the generalization performance.</p>
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-stratification_96837859e94ca90ec42f84e7da1004f7">
<div class="cell-output-display">
<div id="fig-stratification" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/stratification.svg" class="img-fluid figure-img" alt="Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification)."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.5: Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification).</figcaption><p></p>
</figure>
</div>
</div>
</div>
<!-- Stratified sampling ensures that the training and test sets will have similar distribution regarding one or more discrete features as in the original task containing all observations. -->
<p>The <code>$col_roles$stratum</code> field of a <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a> can be set to one or multiple features (including the target in case of classification tasks). In case of multiple features, each combination of the values of all stratification features will form a strata. For example, the target column <code>species</code> of the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html"><code>mlr_tasks_penguins</code></a> task is imbalanced:</p>
<div class="cell" data-hash="performance_cache/html/performance-029_0312691363908a6613aa3641777c52db">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4418605 0.1976744 0.3604651 </code></pre>
</div>
</div>
<p>Without specifying a <code>"stratum"</code> column role, the <code>species</code> column may have quite different class distributions across the training and test sets of a 3-fold cross-validation strategy:</p>
<div class="cell" data-hash="performance_cache/html/performance-030_df285f4ca72afcf29253b9c70c039d39">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb65-2"><a href="#cb65-2"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb65-3"><a href="#cb65-3"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4000000 0.2086957 0.3913043 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4695652 0.1913043 0.3391304 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4561404 0.1929825 0.3508772 </code></pre>
</div>
</div>
<p>In the worst case, and especially for highly imbalanced classes, the minority class might be entirely left out of the training set in one or more resampling iterations. Consequently, the intermediate models within these resampling iterations will never predict the minority class, resulting in a misleading performance estimate for any resampling strategy without stratification. Relying on such a misleading performance estimate can have severe consequences for a deployed model, as it will perform poorly on the minority class in real-world scenarios. For example, misclassification of the minority class can have serious consequences in certain applications such as in medical diagnosis or fraud detection, where failing to identify the minority class may result in serious harm or financial losses. Therefore, it is important to be aware of the potential consequences of imbalanced class distributions in resampling and use stratification to mitigate highly unreliable performance estimates. The code below uses <code>species</code> as <code>"stratum"</code> column role to illustrate that the distribution of <code>species</code> in each test set will closely match the original distribution:</p>
<div class="cell" data-hash="performance_cache/html/performance-031_5df96711635c89fcc0fc819a4bbd0a5f">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a>task_str <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb71-2"><a href="#cb71-2"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum <span class="ot">=</span> <span class="st">"species"</span></span>
<span id="cb71-3"><a href="#cb71-3"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb71-4"><a href="#cb71-4"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task_str)</span>
<span id="cb71-5"><a href="#cb71-5"></a></span>
<span id="cb71-6"><a href="#cb71-6"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4396552 0.1982759 0.3620690 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4434783 0.2000000 0.3565217 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
0.4424779 0.1946903 0.3628319 </code></pre>
</div>
</div>
<p>Rather than assigning the <code>$col_roles$stratum</code> directly, it is also possible to use the <code>$set_col_roles()</code> method to add or remove columns to specific roles incrementally:</p>
<div class="cell" data-hash="performance_cache/html/performance-032_cf7c17ab82b60dee718668d77f5a40e4">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"species"</span>, <span class="at">remove_from =</span> <span class="st">"stratum"</span>)</span>
<span id="cb77-2"><a href="#cb77-2"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>character(0)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"species"</span>, <span class="at">add_to =</span> <span class="st">"stratum"</span>)</span>
<span id="cb79-2"><a href="#cb79-2"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "species"</code></pre>
</div>
</div>
<p>We can further inspect the current stratification via the <code>$strata</code> field, which returns a <code>data.table</code> of the number of observations (<code>N</code>) and row indices (<code>row_id</code>) of each stratum. Since we stratified by the <code>species</code> column, we expect to see the same class frequencies as when we tabulate the task by the <code>species</code> column:</p>
<div class="cell" data-hash="performance_cache/html/performance-033_0da3f9c0a8abf64d80611506269e3833">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a>task_str<span class="sc">$</span>strata</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     N                      row_id
1: 152             1,2,3,4,5,6,...
2: 124 153,154,155,156,157,158,...
3:  68 277,278,279,280,281,282,...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a><span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"species"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>species
   Adelie Chinstrap    Gentoo 
      152        68       124 </code></pre>
</div>
</div>
<p>Should we add another stratification column, the <code>$strata</code> field will show the same values as when we cross-tabulate the two variables of the task:</p>
<div class="cell" data-hash="performance_cache/html/performance-034_396017e8fbe5a6013bdda2dffffe1e51">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"year"</span>, <span class="at">add_to =</span> <span class="st">"stratum"</span>)</span>
<span id="cb85-2"><a href="#cb85-2"></a></span>
<span id="cb85-3"><a href="#cb85-3"></a>task_str<span class="sc">$</span>strata</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>    N                      row_id
1: 50             1,2,3,4,5,6,...
2: 50       51,52,53,54,55,56,...
3: 52 101,102,103,104,105,106,...
4: 34 153,154,155,156,157,158,...
5: 46 187,188,189,190,191,192,...
6: 44 233,234,235,236,237,238,...
7: 26 277,278,279,280,281,282,...
8: 18 303,304,305,306,307,308,...
9: 24 321,322,323,324,325,326,...</code></pre>
</div>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1"></a><span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"species"</span>, <span class="st">"year"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           year
species     2007 2008 2009
  Adelie      50   50   52
  Chinstrap   26   18   24
  Gentoo      34   46   44</code></pre>
</div>
</div>
</section><section id="sec-autoplot-resampleresult" class="level3" data-number="3.2.8"><h3 data-number="3.2.8" class="anchored" data-anchor-id="sec-autoplot-resampleresult">
<span class="header-section-number">3.2.8</span> Plotting Resample Results</h3>
<p><a href="https://mlr3viz.mlr-org.com"><code>mlr3viz</code></a> provides a <a href="https://mlr3viz.mlr-org.com/reference/reexports.html"><code>autoplot()</code></a> method to automatically visualize the resampling results either in a boxplot or histogram:</p>
<div>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"bootstrap"</span>)</span>
<span id="cb89-2"><a href="#cb89-2"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling)</span>
<span id="cb89-3"><a href="#cb89-3"></a></span>
<span id="cb89-4"><a href="#cb89-4"></a><span class="fu">library</span>(mlr3viz)</span>
<span id="cb89-5"><a href="#cb89-5"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"boxplot"</span>)</span>
<span id="cb89-6"><a href="#cb89-6"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"histogram"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
</div>
<div class="cell quarto-layout-panel" data-hash="performance_cache/html/performance-035_24b561f6a21f6b4280252d446315fbc9">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-035-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-035-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
<p>The histogram is useful to visually gauge the variance of the performance results across resampling iterations, whereas the boxplot is often used when multiple learners are compared side-by-side.</p>
<p>We can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:</p>
<div class="cell" data-hash="performance_cache/html/performance-036_a9fa631d76be02f75ff08e3bddd26a4c">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1"></a>task<span class="sc">$</span><span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"bill_length"</span>, <span class="st">"flipper_length"</span>))</span>
<span id="cb91-2"><a href="#cb91-2"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">4</span>)</span>
<span id="cb91-3"><a href="#cb91-3"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb91-4"><a href="#cb91-4"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"prediction"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 2 rows containing missing values (`geom_point()`).</code></pre>
</div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-036-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Prediction surfaces like this are a useful tool for model inspection, as they can help to identify the cause of unexpected performance result. Naturally, they are also popular for didactical purposes to illustrate the prediction behaviour of different learning algorithms, such as the classification tree in the example above with its characteristic orthogonal lines.</p>
</section></section><section id="sec-benchmarking" class="level2" data-number="3.3"><h2 data-number="3.3" class="anchored" data-anchor-id="sec-benchmarking">
<span class="header-section-number">3.3</span> Benchmarking</h2>
<!-- introduction from chapter intro moved to section intro -->
<p>Benchmarking in supervised machine learning refers to the comparison of different learners on a single task or multiple tasks. When comparing learners on a single task or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain. In an applied setting, benchmarking may be used to evaluate whether a deployed model used for a given task or domain can be replaced by a better alternative solution. When comparing multiple learners on multiple tasks, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameter of learners). For example, it is common practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners. <!-- In an applied setting, benchmarking is used to compare a production model to e.g. a novel method or re-trained model based on the original learner to evaluate whether the model is still suitable. --> In <a href="#sec-benchmarking"><span>Section&nbsp;3.3</span></a>, we provide code examples for conducting benchmark studies and performing statistical analysis of benchmark results using the <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> package.</p>
<!-- previous subsection intro
\index{Benchmarking}Benchmarking is used to compare the performance of different learning algorithms applied on one or more tasks using (potentially different) resampling strategies.
The purpose is to rank the learning algorithms regarding a performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks.
-->
<p>The <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> package offers the convenience function <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> to conduct a benchmark experiment. <!-- and repeatedly train and evaluate multiple learners under the same conditions. --> The function internally runs the <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> function on each task separately. The provided resampling strategy is instantiated on each task to ensure a fair comparison by training and evaluating multiple learners under the same conditions. This means that all provided learners use the same train-test splits for each task. <!-- Specifically, it repeatedly trains and evaluates all provided learners under the same conditions by the specified resampling strategy which is instantiated on each task to ensure a fair comparison. --> In this section, we cover how to</p>
<ul>
<li>construct a benchmark design (<a href="#sec-bm-design"><span>Section&nbsp;3.3.1</span></a>) to define the benchmark experiments to be performed,</li>
<li>run the benchmark experiments (<a href="#sec-bm-exec"><span>Section&nbsp;3.3.2</span></a>) and aggregate their results, and</li>
<li>convert benchmark objects (<a href="#sec-bm-resamp"><span>Section&nbsp;3.3.3</span></a>) to other types of objects that can be used for different purposes.</li>
</ul>
<section id="sec-bm-design" class="level3" data-number="3.3.1"><h3 data-number="3.3.1" class="anchored" data-anchor-id="sec-bm-design">
<span class="header-section-number">3.3.1</span> Constructing Benchmarking Designs</h3>
<p>In <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a>, we can define a design to perform benchmark experiments via the <a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html"><code>benchmark_grid()</code></a> convenience function. The design is essentially a table of scenarios to be evaluated and usually consists of unique combinations of <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a>, <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> and <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> triplets.</p>
<p>The <a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html"><code>benchmark_grid()</code></a> function constructs an exhaustive design to describe which combinations of learner, task and resampling should be used in a benchmark experiment. It properly instantiates the used resampling strategies so that all learners are evaluated on the same train-test splits for each task, ensuring a fair comparison. To construct a list of <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a>, <a href="https://mlr3.mlr-org.com/reference/Learner.html"><code>Learner</code></a> and <a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a> objects, we can use the convenience functions <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>tsks()</code></a>, <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>lrns()</code></a>, and <a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>rsmps()</code></a>.</p>
<!-- To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure. -->
<!-- We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data. -->
<p>We design an exemplary benchmark experiment and train a classification tree from the <a href="https://cran.r-project.org/package=rpart"><code>rpart</code></a> package, a random forest from the <a href="https://cran.r-project.org/package=ranger"><code>ranger</code></a> package and a featureless learner serving as a baseline on four different binary classification tasks. The constructed benchmark design is a <code>data.table</code> containing the task, learner, and resampling combinations in each row that should be performed:</p>
<div class="cell" data-hash="performance_cache/html/performance-037_f46d8fdde5c395d23fb4e4726cacca62">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1"></a><span class="fu">library</span>(<span class="st">"mlr3verse"</span>)</span>
<span id="cb93-2"><a href="#cb93-2"></a></span>
<span id="cb93-3"><a href="#cb93-3"></a>tsks <span class="ot">=</span> <span class="fu">tsks</span>(<span class="fu">c</span>(<span class="st">"german_credit"</span>, <span class="st">"sonar"</span>, <span class="st">"breast_cancer"</span>))</span>
<span id="cb93-4"><a href="#cb93-4"></a>lrns <span class="ot">=</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.ranger"</span>, <span class="st">"classif.rpart"</span>, <span class="st">"classif.featureless"</span>),</span>
<span id="cb93-5"><a href="#cb93-5"></a>  <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb93-6"><a href="#cb93-6"></a>rsmp <span class="ot">=</span> <span class="fu">rsmps</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb93-7"><a href="#cb93-7"></a></span>
<span id="cb93-8"><a href="#cb93-8"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tsks, lrns, rsmp)</span>
<span id="cb93-9"><a href="#cb93-9"></a><span class="fu">head</span>(design)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                         learner         resampling
1: &lt;TaskClassif[50]&gt;      &lt;LearnerClassifRanger[38]&gt; &lt;ResamplingCV[20]&gt;
2: &lt;TaskClassif[50]&gt;       &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
3: &lt;TaskClassif[50]&gt; &lt;LearnerClassifFeatureless[38]&gt; &lt;ResamplingCV[20]&gt;
4: &lt;TaskClassif[50]&gt;      &lt;LearnerClassifRanger[38]&gt; &lt;ResamplingCV[20]&gt;
5: &lt;TaskClassif[50]&gt;       &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
6: &lt;TaskClassif[50]&gt; &lt;LearnerClassifFeatureless[38]&gt; &lt;ResamplingCV[20]&gt;</code></pre>
</div>
</div>
<!-- Note: Raphael's chapter comments were against printing the design grid, whereas Bernd in a later meeting was explicitly for printing it. -->
<p>Since the <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a> contains R6 columns within list-columns, we unfortunately can not infer too much about <code>task</code> column, but the <a href="https://mlr3misc.mlr-org.com/reference/ids.html"><code>ids</code></a> utility function can be used for quick inspection or subsetting:</p>
<div class="cell" data-hash="performance_cache/html/performance-038_06f9883bf6d9d20eaafb2f129389fce5">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1"></a>mlr3misc<span class="sc">::</span><span class="fu">ids</span>(design<span class="sc">$</span>task)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "german_credit" "german_credit" "german_credit" "sonar"        
[5] "sonar"         "sonar"         "breast_cancer" "breast_cancer"
[9] "breast_cancer"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1"></a>design[mlr3misc<span class="sc">::</span><span class="fu">ids</span>(task) <span class="sc">==</span> <span class="st">"sonar"</span>, ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                task                         learner         resampling
1: &lt;TaskClassif[50]&gt;      &lt;LearnerClassifRanger[38]&gt; &lt;ResamplingCV[20]&gt;
2: &lt;TaskClassif[50]&gt;       &lt;LearnerClassifRpart[38]&gt; &lt;ResamplingCV[20]&gt;
3: &lt;TaskClassif[50]&gt; &lt;LearnerClassifFeatureless[38]&gt; &lt;ResamplingCV[20]&gt;</code></pre>
</div>
</div>
<p>It is also possible to subset the design, e.g., to exclude a specific task-learner combination by manually removing a certain row from the design which is a <code>data.table</code>. Alternatively, we can also construct a custom benchmark design by manually defining a <code>data.table</code> containing task, learner, and resampling objects (see also the examples section in the help page of <a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html"><code>benchmark_grid()</code></a>).</p>
<!-- :::{.callout-tip} -->

<!-- Note that if you construct a custom design with [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package), the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design. -->
<!-- ::: -->
<!-- ```{r performance-021} -->
<!-- #| echo: false -->
<!-- # Creating a grid using a cross join -->

<!-- design_manual = data.table::CJ( -->
<!--   task = tsks(c("german_credit", "sonar")), -->
<!--   learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"), -->
<!--                  predict_type = "prob", predict_sets = c("train", "test")), -->
<!--   resampling = rsmps("cv", folds = 3), -->
<!--   sorted = FALSE -->
<!-- ) -->
<!-- # Manually remove e.g. the third combination from the grid -->
<!-- design_manual = design_manual[-3] -->
<!-- # Manually instantiate the resamplings -->
<!-- Map(function(task, resampling) { -->
<!--   resampling$instantiate(task) -->
<!-- }, task = design_manual$task, resampling = design_manual$resampling) -->
<!-- ``` -->
</section><section id="sec-bm-exec" class="level3" data-number="3.3.2"><h3 data-number="3.3.2" class="anchored" data-anchor-id="sec-bm-exec">
<span class="header-section-number">3.3.2</span> Execution of Benchmark Experiments</h3>
<p>To run the benchmark experiment, we can pass the constructed benchmark design to the <a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a> function, which will internally call <a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a> for all the combinations of task, learner, and resampling strategy in our benchmark design:</p>
<div class="cell" data-hash="performance_cache/html/performance-039_05a853550cd6c8d868dca129c7a14f78">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb99-2"><a href="#cb99-2"></a><span class="fu">print</span>(bmr)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;BenchmarkResult&gt; of 45 rows with 9 resampling runs
 nr       task_id          learner_id resampling_id iters warnings errors
  1 german_credit      classif.ranger            cv     5        0      0
  2 german_credit       classif.rpart            cv     5        0      0
  3 german_credit classif.featureless            cv     5        0      0
  4         sonar      classif.ranger            cv     5        0      0
  5         sonar       classif.rpart            cv     5        0      0
  6         sonar classif.featureless            cv     5        0      0
  7 breast_cancer      classif.ranger            cv     5        0      0
  8 breast_cancer       classif.rpart            cv     5        0      0
  9 breast_cancer classif.featureless            cv     5        0      0</code></pre>
</div>
</div>
<p>Once the benchmarking is finished (this can take some time, depending on the size of your design), we can aggregate the performance results with the <code>$aggregate()</code> method of the returned <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a>:</p>
<div class="cell" data-hash="performance_cache/html/performance-040_0ac376bf8813f804e46ad2d146ea2c52">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1"></a>acc <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb101-2"><a href="#cb101-2"></a>acc[, .(task_id, learner_id, classif.acc)]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         task_id          learner_id classif.acc
1: german_credit      classif.ranger   0.7640000
2: german_credit       classif.rpart   0.7230000
3: german_credit classif.featureless   0.7000000
4:         sonar      classif.ranger   0.8082462
5:         sonar       classif.rpart   0.6535424
6:         sonar classif.featureless   0.5336818
7: breast_cancer      classif.ranger   0.9721662
8: breast_cancer       classif.rpart   0.9443431
9: breast_cancer classif.featureless   0.6500537</code></pre>
</div>
</div>
<p>As the results are shown in a <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a>, we can easily aggregate the results even further. For example, if we are interested in the learner that performed best across all tasks, we could average the performance of each individual learner across all tasks. Please note that averaging accuracy scores across multiple tasks as in this example is not always appropriate for comparison purposes. A more common alternative to compare the overall algorithm performance across multiple tasks is to first compute the ranks of each learner on each task separately and then compute the average ranks. For illustration purposes, we show how to average the performance of each individual learner across all tasks:</p>
<div class="cell" data-hash="performance_cache/html/performance-041_ae0cb43a03f66f20d8e95797bd59c2ac">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1"></a>acc[, <span class="fu">list</span>(<span class="at">mean_accuracy =</span> <span class="fu">mean</span>(classif.acc)), by <span class="ot">=</span> <span class="st">"learner_id"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            learner_id mean_accuracy
1:      classif.ranger     0.8481375
2:       classif.rpart     0.7736285
3: classif.featureless     0.6279118</code></pre>
</div>
</div>
<p>Ranking the performance scores can either be done via standard <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a> syntax, or more conveniently with the <a href="https://mlr3benchmark.mlr-org.com/reference/mlr3benchmark-package.html"><code>mlr3benchmark</code></a> package. We first use <a href="https://mlr3benchmark.mlr-org.com/reference/as.BenchmarkAggr.html"><code>as.BenchmarkAggr</code></a> to aggregate the <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> using our measure, after which we use the <code>$rank_data()</code> method to convert the performance scores to ranks. The <code>minimize</code> argument is used to indicate that the classification accuracy should not be minimized, i.e.&nbsp;a higher score is better.</p>
<div class="cell" data-hash="performance_cache/html/performance-042_38208931c144eb0e978fe6d39d8599ad">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1"></a><span class="fu">library</span>(<span class="st">"mlr3benchmark"</span>)</span>
<span id="cb105-2"><a href="#cb105-2"></a></span>
<span id="cb105-3"><a href="#cb105-3"></a>bma <span class="ot">=</span> <span class="fu">as.BenchmarkAggr</span>(bmr, <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb105-4"><a href="#cb105-4"></a>bma<span class="sc">$</span><span class="fu">rank_data</span>(<span class="at">minimize =</span> <span class="cn">FALSE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            german_credit sonar breast_cancer
ranger                  1     1             1
rpart                   2     2             2
featureless             3     3             3</code></pre>
</div>
</div>
<p>This results in per-task rankings of the three learners. Unsurprisingly, the featureless learner ranks last, as it always predicts the majority class. However, it is common practice to include it as a baseline in benchmarking experiments to easily gauge the relative performance of other algorithms. In this simple benchmark experiment, the random forest ranked first, outperforming a single classification tree as one would expect.</p>
<!-- We construct two measures to calculate the area under the curve (AUC) for the training and the test set: -->
<!-- ```{r performance-023} -->
<!-- measures = list( -->
<!--   msr("classif.auc", predict_sets = "train", id = "auc_train"), -->
<!--   msr("classif.auc", id = "auc_test") -->
<!-- ) -->
<!-- tab = bmr$aggregate(measures) -->
<!-- print(tab[, .(task_id, learner_id, auc_train, auc_test)]) -->
<!-- ``` -->
<!-- Simply aggregating the performances with the mean is usually not statistically sound. -->
<!-- Instead, we calculate the rank statistic for each learner, grouped by task. -->

<!-- Then the calculated ranks, grouped by the learner, are aggregated with the [`data.table`](https://cran.r-project.org/package=data.table) package. -->
<!-- As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$. -->
<!-- ```{r performance-024} -->

<!-- library("data.table") -->
<!-- # group by levels of task_id, return columns: -->
<!-- # - learner_id -->
<!-- # - rank of col '-auc_train' (per level of learner_id) -->
<!-- # - rank of col '-auc_test' (per level of learner_id) -->
<!-- ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id] -->
<!-- print(ranks) -->
<!-- # group by levels of learner_id, return columns: -->
<!-- # - mean rank of col 'rank_train' (per level of learner_id) -->
<!-- # - mean rank of col 'rank_test' (per level of learner_id) -->
<!-- ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id] -->

<!-- # print the final table, ordered by mean rank of AUC test -->
<!-- ranks[order(mrank_test)] -->
<!-- ``` -->
</section><section id="sec-bm-resamp" class="level3" data-number="3.3.3"><h3 data-number="3.3.3" class="anchored" data-anchor-id="sec-bm-resamp">
<span class="header-section-number">3.3.3</span> Inspect BenchmarkResult Objects</h3>
<!-- {{< include _optional.qmd >}} -->
<p>A <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> object is a collection of multiple <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> objects. We can analogously use <a href="https://www.rdocumentation.org/packages/data.table/topics/as.data.table"><code>as.data.table</code></a> to take a look at the contents and compare them to the <a href="https://www.rdocumentation.org/packages/data.table/topics/data.table-package"><code>data.table</code></a> of the <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> from the previous section (<code>rrdt</code>):</p>
<div class="cell" data-hash="performance_cache/html/performance-043_2c34410db199d95b240c4ab6f9ca49ac">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1"></a>bmrdt <span class="ot">=</span> <span class="fu">as.data.table</span>(bmr)</span>
<span id="cb107-2"><a href="#cb107-2"></a></span>
<span id="cb107-3"><a href="#cb107-3"></a><span class="fu">names</span>(bmrdt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "uhash"      "task"       "learner"    "resampling" "iteration" 
[6] "prediction"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1"></a><span class="fu">names</span>(rrdt)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "task"       "learner"    "resampling" "iteration"  "prediction"</code></pre>
</div>
</div>
<p>By the column names alone, we see that the general contents of a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> and <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> which we specified in <a href="#sec-resampling-inspect"><span>Section&nbsp;3.2.5</span></a> is very similar, with the additional unique identification column <code>"uhash"</code> in the former being the only difference.</p>
<p>The stored <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>s can be extracted via the <code>$resample_result(i)</code> method, where <code>i</code> is the index of the performed benchmark experiment. This allows us to investigate the extracted <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> or individual resampling iterations as shown previously (see <a href="#sec-resampling"><span>Section&nbsp;3.2</span></a>).</p>
<div class="cell" data-hash="performance_cache/html/performance-044_d69f17b0aed7c7cbe8ba2a653c0f862a">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1"></a>rr1 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">1</span>)</span>
<span id="cb111-2"><a href="#cb111-2"></a>rr2 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">2</span>)</span>
<span id="cb111-3"><a href="#cb111-3"></a>rr1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; of 5 iterations
* Task: german_credit
* Learner: classif.ranger
* Warnings: 0 in 0 iterations
* Errors: 0 in 0 iterations</code></pre>
</div>
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1"></a>rr2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;ResampleResult&gt; of 5 iterations
* Task: german_credit
* Learner: classif.rpart
* Warnings: 0 in 0 iterations
* Errors: 0 in 0 iterations</code></pre>
</div>
</div>
<p>Multiple <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a> can be again converted to a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> with the function <a href="https://mlr3.mlr-org.com/reference/as_benchmark_result.html"><code>as_benchmark_result()</code></a> and combined with <code><a href="https://rdrr.io/r/base/c.html">c()</a></code>:</p>
<div class="cell" data-hash="performance_cache/html/performance-045_1c2cbc8869ea71ad5c2b565abed931d7">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1"></a>bmr1 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr1)</span>
<span id="cb115-2"><a href="#cb115-2"></a>bmr2 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr2)</span>
<span id="cb115-3"><a href="#cb115-3"></a></span>
<span id="cb115-4"><a href="#cb115-4"></a>bmr_combined <span class="ot">=</span> <span class="fu">c</span>(bmr1, bmr2)</span>
<span id="cb115-5"><a href="#cb115-5"></a>bmr_combined<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   nr      resample_result       task_id     learner_id resampling_id iters
1:  1 &lt;ResampleResult[21]&gt; german_credit classif.ranger            cv     5
2:  2 &lt;ResampleResult[21]&gt; german_credit  classif.rpart            cv     5
1 variable not shown: [classif.acc]</code></pre>
</div>
</div>
<p>Combining multiple <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a>s into a larger result object can be useful if related benchmarks where computed on different machines.</p>
<!-- Note: Removing ROC stuff from this section made it too small to justify a heading I think.
### Plotting Benchmark Results {#autoplot-benchmarkresult}
-->
<p>Similar to creating automated visualizations for tasks, <a href="#autoplot-prediction">predictions</a>, or <a href="#autoplot-resampleresult">resample results</a>, the <a href="https://mlr3viz.mlr-org.com"><code>mlr3viz</code></a> package also provides a <a href="https://mlr3viz.mlr-org.com/reference/reexports.html"><code>autoplot()</code></a> method to visualize benchmark results, by default as a boxplot:</p>
<div class="cell" data-hash="performance_cache/html/performance-046_6b5bb5b02fa8ec1b9501468bcfdb05f1">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb117-1"><a href="#cb117-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-046-1.png" class="img-fluid" width="576"></p>
</div>
</div>
<p>Such a plot summarizes the benchmark experiment across all tasks and learners. Visualizing performance scores across all learners and tasks in a benchmark helps identifying potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task. In the case of our example above, the three learners show consistent relative performance to each other, in the order we would expect.</p>
</section><section id="statistical-tests" class="level3" data-number="3.3.4"><h3 data-number="3.3.4" class="anchored" data-anchor-id="statistical-tests">
<span class="header-section-number">3.3.4</span> Statistical Tests</h3>
<div class="callout-note callout callout-style-simple callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Some readers may want to skip this section of the book.
</div>
</div>
<div class="callout-body-container callout-body">

</div>
</div>
<p>The package <a href="https://mlr3benchmark.mlr-org.com/reference/mlr3benchmark-package.html"><code>mlr3benchmark</code></a> we previously used for ranking also provides infrastructure for applying statistical significance tests on <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> objects. Currently, Friedman tests and pairwise Friedman-Nemenyi tests <span class="citation" data-cites="demsar2006">(<a href="references.html#ref-demsar2006" role="doc-biblioref">Demšar 2006</a>)</span> are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.</p>
<p><code>$friedman_posthoc()</code> can be used for a pairwise comparison:</p>
<div class="cell" data-hash="performance_cache/html/performance-047_3e62acb0b6af24cc697f5cbfd1219cfb">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1"></a>bma <span class="ot">=</span> <span class="fu">as.BenchmarkAggr</span>(bmr, <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb118-2"><a href="#cb118-2"></a>bma<span class="sc">$</span><span class="fu">friedman_posthoc</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
    Pairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>data: acc and learner_id and task_id</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>            ranger rpart
rpart       0.438  -    
featureless 0.038  0.438</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
P value adjustment method: single-step</code></pre>
</div>
</div>
<p>These results would indicate a statistically significant difference between the <code>"featureless"</code> learner and <code>"ranger"</code>, assuming a 95% confidence level.</p>
<p>The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:</p>
<div class="cell" data-hash="performance_cache/html/performance-048_83b903f772dd922a2f827f4f5fa56ff5">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1"></a><span class="fu">autoplot</span>(bma, <span class="at">type =</span> <span class="st">"cd"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-048-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Similar to the test output before, this visualization leads to the conclusion that the <code>"featureless"</code> learner and <code>"ranger"</code> are significantly different, whereas the critical rank difference of 1.66 is not exceed for the comparison of the <code>"featureless"</code> learner, <code>"rpart"</code> and <code>"ranger"</code>, respectively.</p>
</section></section><section id="sec-roc" class="level2 page-columns page-full" data-number="3.4"><h2 data-number="3.4" class="anchored" data-anchor-id="sec-roc">
<span class="header-section-number">3.4</span> ROC Analysis</h2>
<!-- So far we have focused on methods applicable to general classification and regression tasks. -->
<!-- For the special case of binary classification, there are specialized performance measures and methods to analyze and compare the performance. -->
<!-- Binary classification is unique because of the presence of a positive and negative class and a threshold probability to distinguish between the two. -->
<p>ROC (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers. Although extensions for multiclass classifiers exist (see e.g., <span class="citation" data-cites="hand2001simple">Hand and Till (<a href="references.html#ref-hand2001simple" role="doc-biblioref">2001</a>)</span>), we will only cover the much easier binary classification case here. For binary classifiers that predict discrete classes, we can compute a confusion matrix which computes the following quantities (see also <a href="#fig-confusion">Figure&nbsp;<span>3.6</span></a>):</p>
<ul>
<li>
<strong>True positives (TP)</strong>: Instances that are actually positive and correctly classified as positive.</li>
<li>
<strong>True negatives (TN)</strong>: Instances that are actually negative and correctly classified as negative.</li>
<li>
<strong>False positives (FP)</strong>: Instances that are actually negative but incorrectly classified as positive.</li>
<li>
<strong>False negatives (FN)</strong>: Instances that are actually positive but incorrectly classified as negative.</li>
</ul>
<div class="page-columns page-full"><p>There are a multitude of performance measures that can be derived from a confusion matrix. Unfortunately, many of them have different names for historical reasons, originating from different fields. For a good overview of common confusion matrix-based measures, see the comprehensive table on <a href="https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion">Wikipedia</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> which also provides many common aliases for each measure.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<a href="https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion">https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion</a></p></li></div></div>
<section id="confusion-matrix-based-measures" class="level3" data-number="3.4.1"><h3 data-number="3.4.1" class="anchored" data-anchor-id="confusion-matrix-based-measures">
<span class="header-section-number">3.4.1</span> Confusion Matrix-based Measures</h3>
<p>Some common performance measures that are based on the confusion matrix and measure the ability of a classifier to separate the two classes (i.e., discrimination performance) include (see also <a href="#fig-confusion">Figure&nbsp;<span>3.6</span></a> for their definition based on TP, FP, TN and FN):</p>
<ul>
<li>
<strong>True Positive Rate (TPR)</strong>, <strong>Sensitivity</strong> or <strong>Recall</strong>: How many of the true positives did we predict as positive?</li>
<li>
<strong>True Negative Rate (TNR)</strong> or <strong>Specificity</strong>: How many of the true negatives did we predict as negative?</li>
<li>
<strong>False Positive Rate (FPR)</strong>, or 1 - <strong>Specificity</strong>: How many of the true negatives did we predict as positive?</li>
<li>
<strong>Positive Predictive Value (PPV)</strong> or <strong>Precision</strong>: If we predict positive how likely is it a true positive?</li>
<li>
<strong>Negative Predictive Value (NPV)</strong>: If we predict negative how likely is it a true negative?</li>
<li>
<strong>Accuracy (ACC)</strong>: The proportion of correctly classified instances out of the total number of instances.</li>
<li>
<strong>F1-score</strong>: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as <span class="math inline">\(2 \times \frac{Precision \times Recall}{Precision + Recall}\)</span>.</li>
</ul>
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-confusion_e495871e7147de5a0340f45f7039b1b9">
<div class="cell-output-display">
<div id="fig-confusion" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="Figures/confusion_matrix.svg" class="img-fluid figure-img" alt="Binary confusion matrix of ground truth class vs. predicted class."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.6: Binary confusion matrix of ground truth class vs.&nbsp;predicted class.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<!-- |       |     | True                                              | Class                                                   |                                                          | -->
<!-- |:-----:|:---:|:------------------------------------------------------:|:------------------------------------------------------:|:--------------------------------------------------------:| -->
<!-- |       |     | $+$                                                    | $-$                                                    |                                                          | -->
<!-- | Pred. | $+$ | TP                                                     | FP                                                     | $\text{PPV} = \frac{ \text{TP}}{\text{TP} + \text{FP}}$    | -->
<!-- | $\yh$ | $-$ | FN                                                     | TN                                                     | $\text{NPV} = \frac{\text{TN}}{\text{FN} + \text{TN}}$   | -->
<!-- |       |     | $\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$ | $\text{TNR} = \frac{\text{TN}}{\text{FP} + \text{TN}}$ | $\text{ACC} = \frac{\text{TP}+ \text{TN}}{\text{TP+FP+FN+TN}}$ | -->
<!-- In Chapter @sec-basics, we have already seen how we can obtain the confusion matrix of a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) by accessing the `$confusion` field. -->
<p>In the code example below, we first retrieve the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_german_credit.html"><code>mlr_tasks_german_credit</code></a> task which is a binary classification task and construct a random forest learner using <code>classif.ranger</code> that predicts probabilities using the <code>predict_type = "prob"</code> option. Next, we use the <a href="https://mlr3.mlr-org.com/reference/partition.html"><code>partition()</code></a> helper function which acts as a convenience shortcut function to the <code>"holdout"</code> resampling strategy to randomly partition the contained data into two disjoint set. We train the learner on the training set and use the trained model to generate predictions on the test set. Finally, we retrieve the confusion matrix from the resulting <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> object by accessing the <code>$confusion</code> field (see also <a href="basics.html#sec-classif-eval"><span>Section&nbsp;2.4.3</span></a>):</p>
<div class="cell" data-hash="performance_cache/html/performance-050_941352304ca63878b765c25a61479f83">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb124-1"><a href="#cb124-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>)</span>
<span id="cb124-2"><a href="#cb124-2"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb124-3"><a href="#cb124-3"></a>splits <span class="ot">=</span> <span class="fu">partition</span>(task, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb124-4"><a href="#cb124-4"></a></span>
<span id="cb124-5"><a href="#cb124-5"></a>learner<span class="sc">$</span><span class="fu">train</span>(task, splits<span class="sc">$</span>train)</span>
<span id="cb124-6"><a href="#cb124-6"></a>pred <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">predict</span>(task, splits<span class="sc">$</span>test)</span>
<span id="cb124-7"><a href="#cb124-7"></a>pred<span class="sc">$</span>confusion</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  125  31
    bad    15  29</code></pre>
</div>
</div>
<p>The <a href="https://cran.r-project.org/package=mlr3measures"><code>mlr3measures</code></a> package allows to additionally compute several common confusion matrix-based measures using the <a href="https://www.rdocumentation.org/packages/mlr3measures/topics/confusion_matrix"><code>confusion_matrix</code></a> function:</p>
<div class="cell" data-hash="performance_cache/html/performance-051_227f34d9d86c6ef09b69812c120cbce9">
<div class="sourceCode cell-code" id="cb126"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(<span class="at">truth =</span> pred<span class="sc">$</span>truth,</span>
<span id="cb126-2"><a href="#cb126-2"></a>  <span class="at">response =</span> pred<span class="sc">$</span>response, <span class="at">positive =</span> task<span class="sc">$</span>positive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  125  31
    bad    15  29
acc :  0.7700; ce  :  0.2300; dor :  7.7957; f1  :  0.8446 
fdr :  0.1987; fnr :  0.1071; fomr:  0.3409; fpr :  0.5167 
mcc :  0.4162; npv :  0.6591; ppv :  0.8013; tnr :  0.4833 
tpr :  0.8929 </code></pre>
</div>
</div>
<p>If a binary classifier predicts probabilities instead of discrete classes, we could arbitrarily set a threshold to cut-off the probabilities and assign them to the positive and negative class. When it comes to classification performance, it is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates. Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions and fewer positive predictions. As a consequence, the FPR is usually better (lower), but at the cost of a worse (lower) TPR. For example, in the special case where the threshold is set too high and no instance is predicted as positive, the confusion matrix shows zero true positives (no instances that are actually positive and correctly classified as positive) and zero false positives (no instances that are actually negative but incorrectly classified as positive). Therefore, the FPR and TPR are also zero since there are zero false positives and zero true positives. <!-- However, the TPR is also zero since there are no true positives, which means that the model fails to identify any positive cases, even if there are positive cases in the dataset.  --> <!-- Consider a binary classifier that predicts whether an email is spam (positive class) or not (negative class). --> <!-- Increasing the threshold for identifying positive cases means that the model will require more evidence (i.e., a higher predicted probability) before classifying an email as spam.  --> <!-- This can result in fewer emails being classified as spam (fewer positive predictions) and more emails being classified as not spam (more negative predictions). --> <!-- This can increase the model's specificity (the proportion of true negatives among all negative predictions) which leads to a better (lower) FPR. --> <!-- However, it may also reduce the model's sensitivity and TPR (i.e., the proportion of true positives among all positive predictions). --> Conversely, lowering the threshold for identifying positive cases may never predict the negative class and can increase (improve) TPR, but at the cost of a worse (higher) FPR. For example, below we set the threshold to <code>0.99</code> and <code>0.01</code> for the <a href="https://mlr3.mlr-org.com/reference/mlr_tasks_german_credit.html"><code>mlr_tasks_german_credit</code></a> task to illustrate the two special cases explained above where zero positives and where zero negatives are predicted and inspect the resulting confusion matrix-based measures (some measures can not be computed due to division by 0 and therefore will produce <code>NaN</code> values):</p>
<div class="cell" data-hash="performance_cache/html/performance-052_e5a4080c1886b44aa9b8649f7e6baeb3">
<div class="sourceCode cell-code" id="cb128"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb128-1"><a href="#cb128-1"></a>pred<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="fl">0.99</span>)</span>
<span id="cb128-2"><a href="#cb128-2"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(pred<span class="sc">$</span>truth, pred<span class="sc">$</span>response, task<span class="sc">$</span>positive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good    0   0
    bad   140  60
acc :  0.3000; ce  :  0.7000; dor :  NaN; f1  :  NaN 
fdr :  NaN; fnr :  1.0000; fomr:  0.7000; fpr :  0.0000 
mcc :  0.0000; npv :  0.3000; ppv :  NaN; tnr :  1.0000 
tpr :  0.0000 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1"></a>pred<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="fl">0.01</span>)</span>
<span id="cb130-2"><a href="#cb130-2"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(pred<span class="sc">$</span>truth, pred<span class="sc">$</span>response, task<span class="sc">$</span>positive)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        truth
response good bad
    good  140  60
    bad     0   0
acc :  0.7000; ce  :  0.3000; dor :  NaN; f1  :  0.8235 
fdr :  0.3000; fnr :  0.0000; fomr:  NaN; fpr :  1.0000 
mcc :  0.0000; npv :  NaN; ppv :  0.7000; tnr :  0.0000 
tpr :  1.0000 </code></pre>
</div>
</div>
</section><section id="roc-space" class="level3" data-number="3.4.2"><h3 data-number="3.4.2" class="anchored" data-anchor-id="roc-space">
<span class="header-section-number">3.4.2</span> ROC Space</h3>
<p>ROC analysis aims at evaluating the performance of classifiers by visualizing the trade-off between the TPR and the FPR which can be obtained from a confusion matrix. Each classifier that predicts discrete classes, will be a single point in the ROC space (see <a href="#fig-roc">Figure&nbsp;<span>3.7</span></a>, panel (a)). The best classifier lies on the top-left corner where the TPR is 1 and the FPR is 0. Classifiers on the diagonal predict class labels randomly (possibly with different class proportions). For example, if each positive instance will be randomly classified with 25% as to the positive class, we get a TPR of 0.25. If we assign each negative instance randomly to the positive class, we get a FPR of 0.25. In practice, we should never obtain a classifier clearly below the diagonal. Swapping the predicted classes of a classifier would results in points in the ROC space being mirrored at the diagonal baseline. A point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.</p>
<p>Using different thresholds to cut-off predicted probabilities and assign them to the positive and negative class may lead to different confusion matrices. In this case, we can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values — this is the ROC curve. For example, we can use the previous <a href="https://mlr3.mlr-org.com/reference/Prediction.html"><code>Prediction</code></a> object, compute all possible TPR and FPR combinations if we use all predicted probabilities as possible threshold, and visualize them to manually create a ROC curve:</p>
<div class="cell" data-hash="performance_cache/html/performance-053_416ee905dfa179e925aea04f32c47d25">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1"></a>thresholds <span class="ot">=</span> <span class="fu">sort</span>(pred<span class="sc">$</span>prob[,<span class="dv">1</span>])</span>
<span id="cb132-2"><a href="#cb132-2"></a></span>
<span id="cb132-3"><a href="#cb132-3"></a>rocvals <span class="ot">=</span> data.table<span class="sc">::</span><span class="fu">rbindlist</span>(<span class="fu">lapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb132-4"><a href="#cb132-4"></a>  pred<span class="sc">$</span><span class="fu">set_threshold</span>(t)</span>
<span id="cb132-5"><a href="#cb132-5"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb132-6"><a href="#cb132-6"></a>    <span class="at">threshold =</span> t,</span>
<span id="cb132-7"><a href="#cb132-7"></a>    <span class="at">FPR =</span> pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.fpr"</span>)),</span>
<span id="cb132-8"><a href="#cb132-8"></a>    <span class="at">TPR =</span> pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.tpr"</span>))</span>
<span id="cb132-9"><a href="#cb132-9"></a>  )</span>
<span id="cb132-10"><a href="#cb132-10"></a>}))</span>
<span id="cb132-11"><a href="#cb132-11"></a></span>
<span id="cb132-12"><a href="#cb132-12"></a><span class="fu">head</span>(rocvals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   threshold       FPR TPR
1: 0.2351119 0.9833333   1
2: 0.2357222 0.9666667   1
3: 0.2404476 0.9500000   1
4: 0.2616540 0.9500000   1
5: 0.2799230 0.9166667   1
6: 0.2829881 0.9166667   1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb134-2"><a href="#cb134-2"></a><span class="fu">ggplot</span>(rocvals, <span class="fu">aes</span>(FPR, TPR)) <span class="sc">+</span></span>
<span id="cb134-3"><a href="#cb134-3"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb134-4"><a href="#cb134-4"></a>  <span class="fu">geom_path</span>(<span class="at">color =</span> <span class="st">"darkred"</span>) <span class="sc">+</span></span>
<span id="cb134-5"><a href="#cb134-5"></a>  <span class="fu">geom_abline</span>(<span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb134-6"><a href="#cb134-6"></a>  <span class="fu">coord_fixed</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb134-7"><a href="#cb134-7"></a>  <span class="fu">labs</span>(</span>
<span id="cb134-8"><a href="#cb134-8"></a>    <span class="at">title =</span> <span class="st">"Manually constructed ROC curve"</span>,</span>
<span id="cb134-9"><a href="#cb134-9"></a>    <span class="at">x =</span> <span class="st">"1 - Specificity (FPR)"</span>,</span>
<span id="cb134-10"><a href="#cb134-10"></a>    <span class="at">y =</span> <span class="st">"Sensitivity (TPR)"</span></span>
<span id="cb134-11"><a href="#cb134-11"></a>  ) <span class="sc">+</span></span>
<span id="cb134-12"><a href="#cb134-12"></a>  <span class="fu">theme_bw</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-053-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC). The higher the AUC value, the better the performance, whereas a random classifier would result in an AUC of 0.5 (see <a href="#fig-roc">Figure&nbsp;<span>3.7</span></a>, panel (b) for an illustration). The AUC can be interpreted as the probability that a randomly chosen positive instance is ranked higher (in the sense that it gets a higher predicted probability of belonging to the positive class) by the classification model than a randomly chosen negative instance. <!-- ROC curves for different labels are symmetric with respect to the diagonal, so  --></p>
<div class="cell" data-layout-align="center" data-hash="performance_cache/html/fig-roc_5ad21ac24985d3ba0899b600711347f5">
<div class="cell-output-display">
<div id="fig-roc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure"><p><img src="performance_files/figure-html/fig-roc-1.png" class="img-fluid figure-img" alt="Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2." width="768"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.7: Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>For <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> prediction objects, the ROC curve can be constructed with the previously seen <a href="https://mlr3viz.mlr-org.com/reference/autoplot.PredictionClassif.html"><code>autoplot.PredictionClassif</code></a> from <a href="https://mlr3viz.mlr-org.com"><code>mlr3viz</code></a>. The x-axis showing the FPR is labelled “1 - Specificity” by convention, whereas the y-axis shows “Sensitivity” for the TPR.</p>
<div class="cell" data-hash="performance_cache/html/performance-055_f98cda477a20a5ac97cc836c137a0264">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"roc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-055-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can also plot the precision-recall (PR) curve which visualize the PPV vs.&nbsp;TPR. The main difference between ROC curves and PR curves is that the number of true-negatives are not used to produce a PR curve. PR curves are preferred over ROC curves for imbalanced populations. This is because the positive class is usually rare in imbalanced classification tasks. Hence, the FPR is often low even for a random classifier. As a result, the ROC curve may not provide a good assessment of the classifier’s performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations). See also <span class="citation" data-cites="davis2006relationship">Davis and Goadrich (<a href="references.html#ref-davis2006relationship" role="doc-biblioref">2006</a>)</span> for a detailed discussion about the relationship between the PRC and ROC curves.</p>
<div class="cell" data-hash="performance_cache/html/performance-056_cc611929554c9591ea5ae936e0a3d38f">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"prc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="performance_files/figure-html/performance-056-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Another useful way to think about the performance of a classifier is to visualize the relationship of the set threshold with the performance metric at the given threshold. For example, if we want to see the FPR and accuracy across all possible thresholds:</p>
<div>
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.fpr"</span>))</span>
<span id="cb137-2"><a href="#cb137-2"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="performance_cache/html/performance-057_ae3edb57ffeecb1498a91dc90e91fa18">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-057-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-057-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
<p>This visualization would show us that it would not matter if we picked a threshold of 0.5 or 0.75, since neither FPR nor accuracy changes in that range.</p>
<p>These visualizations are also available for <a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a>. Here, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro-averaged):</p>
<div>
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb138-1"><a href="#cb138-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(</span>
<span id="cb138-2"><a href="#cb138-2"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb138-3"><a href="#cb138-3"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb138-4"><a href="#cb138-4"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb138-5"><a href="#cb138-5"></a>)</span>
<span id="cb138-6"><a href="#cb138-6"></a></span>
<span id="cb138-7"><a href="#cb138-7"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb138-8"><a href="#cb138-8"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"prc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="performance_cache/html/performance-058_22dc720a4605e1aff1726982828b1831">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-058-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-058-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
<p>We can also visualize a <a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a> to compare multiple learners on the same <a href="https://mlr3.mlr-org.com/reference/Task.html"><code>Task</code></a>:</p>
<div>
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode numberSource r numberLines code-with-copy"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb139-2"><a href="#cb139-2"></a>  <span class="at">tasks =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb139-3"><a href="#cb139-3"></a>  <span class="at">learners =</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span>), <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb139-4"><a href="#cb139-4"></a>  <span class="at">resamplings =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb139-5"><a href="#cb139-5"></a>)</span>
<span id="cb139-6"><a href="#cb139-6"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb139-7"><a href="#cb139-7"></a></span>
<span id="cb139-8"><a href="#cb139-8"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb139-9"><a href="#cb139-9"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"prc"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell quarto-layout-panel" data-hash="performance_cache/html/performance-059_bdef058919db4a746d1a6867e6e50600">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-059-1.png" class="img-fluid" width="672"></p>
</div>
<div class="cell-output-display quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="performance_files/figure-html/performance-059-2.png" class="img-fluid" width="672"></p>
</div>
</div>
</div>
</div>
</section></section><section id="conclusion" class="level2 page-columns page-full" data-number="3.5"><h2 data-number="3.5" class="anchored" data-anchor-id="conclusion">
<span class="header-section-number">3.5</span> Conclusion</h2>
<p>In this chapter, we learned how to estimate the generalization performance of a model via resampling. We also learned about benchmarking to fairly compare the estimated generalization performance of different learners across multiple tasks. Performance calculations underpin these concepts, and we have seen some of them applied to classification tasks, with a more in-depth look at the special case of binary classification and ROC analysis. We also learned how to visualize confusion matrix-based performance measures with regards to different thresholds as well as resampling and benchmark results with <a href="https://mlr3viz.mlr-org.com"><code>mlr3viz</code></a>. <!-- that cut-off the predicted probabilities and assign the predictions to the positive and negative class  --> The discussed topics belong to the fundamental concepts of supervised machine learning. <a href="optimization.html"><span>Chapter&nbsp;4</span></a> builds on these concepts and applies them for tuning (i.e., to automatically choose the optimal hyperparameters of a learner) through nested resampling (<a href="optimization.html#sec-nested-resampling"><span>Section&nbsp;4.5</span></a>). In <a href="special.html"><span>Chapter&nbsp;8</span></a>, we will also take a look at specialized tasks that require different resampling strategies. Finally, <a href="#tbl-api-performance">Table&nbsp;<span>3.1</span></a> provides an overview of some important <a href="https://mlr3.mlr-org.com"><code>mlr3</code></a> functions and the corresponding R6 classes that were most frequently used throughout this chapter.</p>
<div id="tbl-api-performance" class="anchored">
<table class="table">
<caption>Table&nbsp;3.1: Core S3 ‘sugar’ functions for resampling and benchmarking in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions.</caption>
<thead><tr class="header">
<th>S3 function</th>
<th>R6 Class</th>
<th>Summary</th>
</tr></thead>
<tbody>
<tr class="odd">
<td><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html"><code>rsmp()</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/Resampling.html"><code>Resampling</code></a></td>
<td>Assigns observations to train- and test sets</td>
</tr>
<tr class="even">
<td><a href="https://mlr3.mlr-org.com/reference/resample.html"><code>resample()</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/ResampleResult.html"><code>ResampleResult</code></a></td>
<td>Evaluates learners on given tasks using a resampling strategy</td>
</tr>
<tr class="odd">
<td><a href="https://mlr3.mlr-org.com/reference/benchmark_grid.html"><code>benchmark_grid()</code></a></td>
<td>-</td>
<td>Constructs a design grid of learners, tasks, and resamplings</td>
</tr>
<tr class="even">
<td><a href="https://mlr3.mlr-org.com/reference/benchmark.html"><code>benchmark()</code></a></td>
<td><a href="https://mlr3.mlr-org.com/reference/BenchmarkResult.html"><code>BenchmarkResult</code></a></td>
<td>Evaluates learners on a given design grid</td>
</tr>
</tbody>
</table>
</div>
<section id="resources" class="level3 unnumbered unlisted page-columns page-full"><h3 class="unnumbered unlisted anchored" data-anchor-id="resources">Resources</h3>
<ul>
<li>Learn more about advanced resampling techniques in: <a href="https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/">Resampling - Stratified, Blocked and Predefined</a><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</li>
<li>Check out the blog post <a href="https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/">mlr3 Basics on “Iris” - Hello World!</a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> to see minimal examples on using resampling and benchmarking on the iris dataset.</li>
<li>Use resampling and benchmarking for the <a href="https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/">comparison of decision boundaries of classification learners</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</li>
<li>Learn how to effectively pick thresholds by applying tuning and pipelines (Chapters <a href="optimization.html"><span>4</span></a> and <a href="pipelines.html"><span>6</span></a>) in <a href="https://mlr-org.com/gallery/optimization/2020-10-14-threshold-tuning/index.html">this post on threshold tuning</a><a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>.</li>
</ul><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;<a href="https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/">https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/</a></p></li><li id="fn3"><p><sup>3</sup>&nbsp;<a href="https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/">https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/</a></p></li><li id="fn4"><p><sup>4</sup>&nbsp;<a href="https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/">https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/</a></p></li><li id="fn5"><p><sup>5</sup>&nbsp;<a href="https://mlr-org.com/gallery/optimization/2020-10-14-threshold-tuning/index.html">https://mlr-org.com/gallery/optimization/2020-10-14-threshold-tuning/index.html</a></p></li></div></section></section><section id="exercises" class="level2" data-number="3.6"><h2 data-number="3.6" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">3.6</span> Exercises</h2>
<ol type="1">
<li><p>Use the <code>spam</code> task and 5-fold cross-validation to benchmark Random Forest (<code>classif.ranger</code>), Logistic Regression (<code>classif.log_reg</code>), and XGBoost (<code>classif.xgboost</code>) with regards to AUC. Which learner appears to do best? How confident are you in your conclusion? How would you improve upon this?</p></li>
<li><p>A colleague claims to have achieved a 93.1% classification accuracy using the <code>classif.rpart</code> learner on the <code>penguins_simple</code> task. You want to reproduce their results and ask them about their resampling strategy. They said they used 3-fold cross-validation, and they assigned rows using the task’s <code>row_id</code> modulo 3 to generate three evenly sized folds. Reproduce their results using the custom CV strategy.</p></li>
</ol>


<!-- -->

<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography" style="display: none">
<div id="ref-bengio2003no" class="csl-entry" role="doc-biblioentry">
Bengio, Yoshua, and Yves Grandvalet. 2003. <span>“No Unbiased Estimator of the Variance of k-Fold Cross-Validation.”</span> <em>Advances in Neural Information Processing Systems</em> 16.
</div>
<div id="ref-bischl2012resampling" class="csl-entry" role="doc-biblioentry">
Bischl, Bernd, Olaf Mersmann, Heike Trautmann, and Claus Weihs. 2012. <span>“Resampling Methods for Meta-Model Validation with Recommendations for Evolutionary Computation.”</span> <em>Evolutionary Computation</em> 20 (2): 249–75.
</div>
<div id="ref-davis2006relationship" class="csl-entry" role="doc-biblioentry">
Davis, Jesse, and Mark Goadrich. 2006. <span>“The Relationship Between Precision-Recall and ROC Curves.”</span> In <em>Proceedings of the 23rd International Conference on Machine Learning</em>, 233–40.
</div>
<div id="ref-demsar2006" class="csl-entry" role="doc-biblioentry">
Demšar, Janez. 2006. <span>“Statistical Comparisons of Classifiers over Multiple Data Sets.”</span> <em>Journal of Machine Learning Research</em> 7 (1): 1–30. <a href="https://jmlr.org/papers/v7/demsar06a.html">https://jmlr.org/papers/v7/demsar06a.html</a>.
</div>
<div id="ref-hand2001simple" class="csl-entry" role="doc-biblioentry">
Hand, David J, and Robert J Till. 2001. <span>“A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems.”</span> <em>Machine Learning</em> 45: 171–86.
</div>
<div id="ref-japkowicz2011evaluating" class="csl-entry" role="doc-biblioentry">
Japkowicz, Nathalie, and Mohak Shah. 2011. <em>Evaluating Learning Algorithms: A Classification Perspective</em>. Cambridge University Press.
</div>
<div id="ref-kim2009estimating" class="csl-entry" role="doc-biblioentry">
Kim, Ji-Hyun. 2009. <span>“Estimating Classification Error Rate: Repeated Cross-Validation, Repeated Hold-Out and Bootstrap.”</span> <em>Computational Statistics &amp; Data Analysis</em> 53 (11): 3735–45.
</div>
<div id="ref-molinaro2005prediction" class="csl-entry" role="doc-biblioentry">
Molinaro, Annette M, Richard Simon, and Ruth M Pfeiffer. 2005. <span>“Prediction Error Estimation: A Comparison of Resampling Methods.”</span> <em>Bioinformatics</em> 21 (15): 3301–7.
</div>
</div>
</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./basics.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Fundamentals</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./optimization.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Hyperparameter Optimization</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb140" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb140-1"><a href="#cb140-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb140-2"><a href="#cb140-2" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span></span>
<span id="cb140-3"><a href="#cb140-3" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Giuseppe Casalicchio</span></span>
<span id="cb140-4"><a href="#cb140-4" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0001-5324-5966</span></span>
<span id="cb140-5"><a href="#cb140-5" aria-hidden="true" tabindex="-1"></a><span class="co">    email: giuseppe.casalicchio@stat.uni-muenchen.de</span></span>
<span id="cb140-6"><a href="#cb140-6" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb140-7"><a href="#cb140-7" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Ludwig-Maximilians-Universität München</span></span>
<span id="cb140-8"><a href="#cb140-8" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Munich Center for Machine Learning (MCML)</span></span>
<span id="cb140-9"><a href="#cb140-9" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Essential Data Science Training GmbH</span></span>
<span id="cb140-10"><a href="#cb140-10" aria-hidden="true" tabindex="-1"></a><span class="co">  - name: Lukas Burk</span></span>
<span id="cb140-11"><a href="#cb140-11" aria-hidden="true" tabindex="-1"></a><span class="co">    orcid: 0000-0001-7528-3795</span></span>
<span id="cb140-12"><a href="#cb140-12" aria-hidden="true" tabindex="-1"></a><span class="co">    email: Lukas.Burk@stat.uni-muenchen.de</span></span>
<span id="cb140-13"><a href="#cb140-13" aria-hidden="true" tabindex="-1"></a><span class="co">    affiliations:</span></span>
<span id="cb140-14"><a href="#cb140-14" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Ludwig-Maximilians-Universität München</span></span>
<span id="cb140-15"><a href="#cb140-15" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Leibniz Institute for Prevention Research and Epidemiology - BIPS</span></span>
<span id="cb140-16"><a href="#cb140-16" aria-hidden="true" tabindex="-1"></a><span class="co">      - name: Munich Center for Machine Learning (MCML)</span></span>
<span id="cb140-17"><a href="#cb140-17" aria-hidden="true" tabindex="-1"></a><span class="an">abstract:</span><span class="co"> |</span></span>
<span id="cb140-18"><a href="#cb140-18" aria-hidden="true" tabindex="-1"></a><span class="co">  Estimating the generalization performance of a machine learning algorithm on a given task requires additional data not used during training.</span></span>
<span id="cb140-19"><a href="#cb140-19" aria-hidden="true" tabindex="-1"></a><span class="co">  Resampling refers to the process of repeatedly splitting the available data into training and test sets to enable unbiased performance estimation.</span></span>
<span id="cb140-20"><a href="#cb140-20" aria-hidden="true" tabindex="-1"></a><span class="co">  This chapter introduces common resampling strategies and illustrates their use with the `mlr3` ecosystem.</span></span>
<span id="cb140-21"><a href="#cb140-21" aria-hidden="true" tabindex="-1"></a><span class="co">  Benchmarking builds upon resampling, encompassing the fair comparison of multiple machine learning algorithms on at least one task.</span></span>
<span id="cb140-22"><a href="#cb140-22" aria-hidden="true" tabindex="-1"></a><span class="co">  We show how benchmarking can be performed within the `mlr3` ecosystem, from the construction of benchmark designs to the statistical analysis of the benchmark results.</span></span>
<span id="cb140-23"><a href="#cb140-23" aria-hidden="true" tabindex="-1"></a><span class="an">editor_options:</span></span>
<span id="cb140-24"><a href="#cb140-24" aria-hidden="true" tabindex="-1"></a><span class="co">  chunk_output_type: console</span></span>
<span id="cb140-25"><a href="#cb140-25" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb140-26"><a href="#cb140-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-27"><a href="#cb140-27" aria-hidden="true" tabindex="-1"></a><span class="fu"># Evaluation, Resampling and Benchmarking {#sec-performance}</span></span>
<span id="cb140-28"><a href="#cb140-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-29"><a href="#cb140-29" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-001}</span></span>
<span id="cb140-30"><a href="#cb140-30" aria-hidden="true" tabindex="-1"></a><span class="co">#| include: false</span></span>
<span id="cb140-31"><a href="#cb140-31" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">3</span>)</span>
<span id="cb140-32"><a href="#cb140-32" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-33"><a href="#cb140-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-34"><a href="#cb140-34" aria-hidden="true" tabindex="-1"></a>{{&lt; include _setup.qmd &gt;}}</span>
<span id="cb140-35"><a href="#cb140-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-36"><a href="#cb140-36" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: check links e.g. autoplot etc. --&gt;</span></span>
<span id="cb140-37"><a href="#cb140-37" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: Check crossrefs, e.g. (#measures) lead so chapter "Special Tasks" --&gt;</span></span>
<span id="cb140-38"><a href="#cb140-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-39"><a href="#cb140-39" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- why performance estimation, what is a performance measure --&gt;</span></span>
<span id="cb140-40"><a href="#cb140-40" aria-hidden="true" tabindex="-1"></a>In supervised machine learning, a model which is deployed in practice is expected to generalize well to new, unseen data.</span>
<span id="cb140-41"><a href="#cb140-41" aria-hidden="true" tabindex="-1"></a>Accurate estimation of this so-called <span class="in">`r index("generalization performance")`</span> is crucial for many aspects of machine learning application and research --- whether we want to fairly compare a novel algorithm with established ones or to find the best algorithm for a particular task after tuning --- we always rely on this performance estimate.</span>
<span id="cb140-42"><a href="#cb140-42" aria-hidden="true" tabindex="-1"></a>Hence, performance estimation is a fundamental concept used for model selection, model comparison, and hyperparameter tuning (which will be discussed in depth in @sec-optimization) in supervised machine learning.</span>
<span id="cb140-43"><a href="#cb140-43" aria-hidden="true" tabindex="-1"></a>To properly assess the generalization performance of a model, we must first decide on a <span class="in">`r index("performance measure")`</span> that is appropriate for our given task and evaluation goal.</span>
<span id="cb140-44"><a href="#cb140-44" aria-hidden="true" tabindex="-1"></a>A performance measure typically computes a numeric score indicating, e.g., how well the model predictions match the ground truth.</span>
<span id="cb140-45"><a href="#cb140-45" aria-hidden="true" tabindex="-1"></a>However, it may also reflect other qualities such as the time for training a model.</span>
<span id="cb140-46"><a href="#cb140-46" aria-hidden="true" tabindex="-1"></a>An overview of some common performance measures implemented in <span class="in">`r mlr3`</span>, including a short description and a basic mathematical definition, can be found by following the link provided in the overview table under *Measures overview* in Appendix @sec-appendix-overview-tables.</span>
<span id="cb140-47"><a href="#cb140-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-48"><a href="#cb140-48" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- motivate the need for splitting the data --&gt;</span></span>
<span id="cb140-49"><a href="#cb140-49" aria-hidden="true" tabindex="-1"></a>Once we have decided on a performance measure, the next step is to adopt a strategy that defines how to use the available data to estimate the generalization performance.</span>
<span id="cb140-50"><a href="#cb140-50" aria-hidden="true" tabindex="-1"></a>Unfortunately, using the same data to train and test a model is a bad strategy as it would lead to an overly optimistic performance estimate.</span>
<span id="cb140-51"><a href="#cb140-51" aria-hidden="true" tabindex="-1"></a>For example, an overfitted model may perfectly fit the data on which it was trained, but may not generalize well to new data.</span>
<span id="cb140-52"><a href="#cb140-52" aria-hidden="true" tabindex="-1"></a>Assessing its performance using the same data it was trained would misleadingly suggest a well-performing model.</span>
<span id="cb140-53"><a href="#cb140-53" aria-hidden="true" tabindex="-1"></a>It is therefore common practice to test a model on independent data not used to train a model.</span>
<span id="cb140-54"><a href="#cb140-54" aria-hidden="true" tabindex="-1"></a>However, we typically train a deployed model on all available data, which leaves no data to assess its generalization performance.</span>
<span id="cb140-55"><a href="#cb140-55" aria-hidden="true" tabindex="-1"></a>To address this issue, existing performance estimation strategies withhold a subset of the available data for evaluation purposes.</span>
<span id="cb140-56"><a href="#cb140-56" aria-hidden="true" tabindex="-1"></a>This so-called test set serves as unseen data and is used to estimate the generalization performance.</span>
<span id="cb140-57"><a href="#cb140-57" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- is then used to mimic the presence of unseen data and to estimate the generalization performance. --&gt;</span></span>
<span id="cb140-58"><a href="#cb140-58" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To overcome this issue, existing strategies for estimating the generalization performance preserve a subset of the data to mimic the presence of unseen data. --&gt;</span></span>
<span id="cb140-59"><a href="#cb140-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-60"><a href="#cb140-60" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- describe performance estimation procedure and desiderata --&gt;</span></span>
<span id="cb140-61"><a href="#cb140-61" aria-hidden="true" tabindex="-1"></a>A common simple strategy is the <span class="in">`r index("holdout")`</span> method, which randomly partitions the data into a single training and test set using a pre-defined splitting ratio.</span>
<span id="cb140-62"><a href="#cb140-62" aria-hidden="true" tabindex="-1"></a>The training set is used to create an intermediate model, whose sole purpose is to estimate the performance using the test set.</span>
<span id="cb140-63"><a href="#cb140-63" aria-hidden="true" tabindex="-1"></a>This performance estimate is then used as a proxy for the performance of the final model trained on all available data and deployed in practice.</span>
<span id="cb140-64"><a href="#cb140-64" aria-hidden="true" tabindex="-1"></a>Ideally, the training set should be as large as all available data so that the intermediate model represents the final model well.</span>
<span id="cb140-65"><a href="#cb140-65" aria-hidden="true" tabindex="-1"></a>If the training data is much smaller, the intermediate model learns less complex relationships compared to the final model, resulting in a pessimistically biased performance estimate.</span>
<span id="cb140-66"><a href="#cb140-66" aria-hidden="true" tabindex="-1"></a>On the other hand, we also want as much test data as possible to reliably estimate the generalization performance.</span>
<span id="cb140-67"><a href="#cb140-67" aria-hidden="true" tabindex="-1"></a>However, both goals are not possible if we have only access to a limited amount of data.</span>
<span id="cb140-68"><a href="#cb140-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-69"><a href="#cb140-69" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- explain resampling --&gt;</span></span>
<span id="cb140-70"><a href="#cb140-70" aria-hidden="true" tabindex="-1"></a>To address this issue, <span class="in">`r index("resampling")`</span> strategies (see @sec-resampling) repeatedly split all available data into multiple training and test sets, with one repetition corresponding to what is called a resampling iteration in <span class="in">`r mlr3`</span>.</span>
<span id="cb140-71"><a href="#cb140-71" aria-hidden="true" tabindex="-1"></a>An intermediate model is then trained on each training set and the remaining test set is used to measure the performance in each resampling iteration.</span>
<span id="cb140-72"><a href="#cb140-72" aria-hidden="true" tabindex="-1"></a>The generalization performance is finally estimated by the averaged performance over multiple resampling iterations (see @fig-ml-abstraction for an illustration).</span>
<span id="cb140-73"><a href="#cb140-73" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Removed reference to exercise since we discussed that this aspect is too important to leave to an exercise, maybe this should be a gallery post or even link to I2ML? --&gt;</span></span>
<span id="cb140-74"><a href="#cb140-74" aria-hidden="true" tabindex="-1"></a>Resampling methods allow using more data points for testing, while keeping the training sets as large as possible.</span>
<span id="cb140-75"><a href="#cb140-75" aria-hidden="true" tabindex="-1"></a>Specifically, repeating the data splitting process allows using all available data points to assess the performance, as each data point can be ensured to be part of the test set in at least one resampling iteration.</span>
<span id="cb140-76"><a href="#cb140-76" aria-hidden="true" tabindex="-1"></a>A higher number of resampling iterations can reduce the variance and result in a more reliable performance estimate.</span>
<span id="cb140-77"><a href="#cb140-77" aria-hidden="true" tabindex="-1"></a>It also reduces the risk of the performance estimate being strongly affected by an unlucky split that does not reflect the original data distribution well, which is a known issue of the holdout method.</span>
<span id="cb140-78"><a href="#cb140-78" aria-hidden="true" tabindex="-1"></a>However, since resampling strategies create multiple intermediate models trained on different parts of the available data and average their performance, they evaluate the performance of the learning algorithm that induced these models, rather than the performance of the final model which is deployed in practice.</span>
<span id="cb140-79"><a href="#cb140-79" aria-hidden="true" tabindex="-1"></a>It is therefore important to train the intermediate models on nearly all data points from the same distribution so that the intermediate models and the final model are similar.</span>
<span id="cb140-80"><a href="#cb140-80" aria-hidden="true" tabindex="-1"></a>If we only have access to a limited amount of data, the best we can do is to use the performance of the learning algorithm as a proxy for the performance of the final model.</span>
<span id="cb140-81"><a href="#cb140-81" aria-hidden="true" tabindex="-1"></a>In @sec-resampling, we will learn how to estimate the generalization performance of a <span class="in">`r ref("Learner")`</span> using the <span class="in">`r mlr3`</span> package.</span>
<span id="cb140-82"><a href="#cb140-82" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- For example, the $k$-fold cross-validation method randomly partitions the data into $k$ subsets, called folds (see @fig-cv-illustration). --&gt;</span></span>
<span id="cb140-83"><a href="#cb140-83" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations. --&gt;</span></span>
<span id="cb140-84"><a href="#cb140-84" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate. --&gt;</span></span>
<span id="cb140-85"><a href="#cb140-85" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Several variations of cross-validation exist, including repeated $k$-fold cross-validation where the entire process illustrated in @fig-cv-illustration is repeated multiple times and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.  --&gt;</span></span>
<span id="cb140-86"><a href="#cb140-86" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Resampling strategies such as $k$-fold cross-validation are usually preferable to a single train-test split because they use all available data to assess the performance and thereby avoid the performance estimate from being biased by a particular split. --&gt;</span></span>
<span id="cb140-87"><a href="#cb140-87" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- These strategies evaluate the performance of the learning algorithm that induced the final model, rather than the performance of the final model itself. --&gt;</span></span>
<span id="cb140-88"><a href="#cb140-88" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- However, the best we can do if we only have access to a limited amount of data is to use the performance of the learning algorithm as a proxy for the performance of the final model. --&gt;</span></span>
<span id="cb140-89"><a href="#cb140-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-90"><a href="#cb140-90" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-002, echo=FALSE}</span></span>
<span id="cb140-91"><a href="#cb140-91" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-ml-abstraction</span></span>
<span id="cb140-92"><a href="#cb140-92" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "A general abstraction of the performance estimation process: The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is applied to each training data and produces intermediate models (learning process). Each intermediate model along with its associated test data produces predictions. The performance measure compares these predictions with the associated actual target values from each test data and computes a performance value for each test data. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process)."</span></span>
<span id="cb140-93"><a href="#cb140-93" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-94"><a href="#cb140-94" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A general abstraction of the performance estimation process: The available data is (repeatedly) split into (a set of) training data and test data (data splitting / resampling process). The learner is applied to each training data and produces intermediate models (learning process). Each intermediate model along with its associated test data produces predictions. The performance measure compares these predictions with the associated actual target values from each test data and computes a performance value for each test data. All performance values are aggregated into a scalar value to estimate the generalization performance (evaluation process)."</span></span>
<span id="cb140-95"><a href="#cb140-95" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/ml_abstraction-2.svg"</span>)</span>
<span id="cb140-96"><a href="#cb140-96" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-97"><a href="#cb140-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-98"><a href="#cb140-98" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: Add a section at the end showing some properties of resampling strategies e.g. bias-variance trade-off of different resamplings with different sizes of training-test set (see i2ml) --&gt;</span></span>
<span id="cb140-99"><a href="#cb140-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-100"><a href="#cb140-100" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- chapter outline --&gt;</span></span>
<span id="cb140-101"><a href="#cb140-101" aria-hidden="true" tabindex="-1"></a><span class="fu">## Quick Start</span></span>
<span id="cb140-102"><a href="#cb140-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-103"><a href="#cb140-103" aria-hidden="true" tabindex="-1"></a>In the previous chapter, we have applied the holdout method by manually partitioning the data contained in a <span class="in">`r ref("Task")`</span> object into a single training set (to train the model) and a single test set (to estimate the generalization performance).</span>
<span id="cb140-104"><a href="#cb140-104" aria-hidden="true" tabindex="-1"></a>As a quick start into resampling and benchmarking with the <span class="in">`r mlr3`</span> package, we show a short example of how to do this with the <span class="in">`r ref("resample()")`</span> and <span class="in">`r ref("benchmark()")`</span> convenience functions.</span>
<span id="cb140-105"><a href="#cb140-105" aria-hidden="true" tabindex="-1"></a>Specifically, we show how to estimate the generalization performance of a learner on a given task by the holdout method using <span class="in">`r ref("resample()")`</span> and how to use <span class="in">`r ref("benchmark()")`</span> to compare two learners on a task.</span>
<span id="cb140-106"><a href="#cb140-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-107"><a href="#cb140-107" aria-hidden="true" tabindex="-1"></a>We first define the corresponding <span class="in">`r ref("Task")`</span> and <span class="in">`r ref("Learner")`</span> objects used throughout this chapter as follows:</span>
<span id="cb140-108"><a href="#cb140-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-109"><a href="#cb140-109" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-003}</span></span>
<span id="cb140-110"><a href="#cb140-110" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb140-111"><a href="#cb140-111" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.rpart"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb140-112"><a href="#cb140-112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-113"><a href="#cb140-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-114"><a href="#cb140-114" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #### Further Reading --&gt;</span></span>
<span id="cb140-115"><a href="#cb140-115" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. --&gt;</span></span>
<span id="cb140-116"><a href="#cb140-116" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). --&gt;</span></span>
<span id="cb140-117"><a href="#cb140-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-118"><a href="#cb140-118" aria-hidden="true" tabindex="-1"></a>The next obvious step is to select a suitable performance measure, which can be done as explained in @sec-eval using the <span class="in">`r ref("mlr_measures")`</span> dictionary.</span>
<span id="cb140-119"><a href="#cb140-119" aria-hidden="true" tabindex="-1"></a>Passing the dictionary to the <span class="in">`as.data.table`</span> function provides an overview of implemented measures with additional information from which we can select a suitable performance measure, which we print here in two parts for compactness:</span>
<span id="cb140-120"><a href="#cb140-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-121"><a href="#cb140-121" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-004}</span></span>
<span id="cb140-122"><a href="#cb140-122" aria-hidden="true" tabindex="-1"></a>msr_tbl <span class="ot">=</span> <span class="fu">as.data.table</span>(mlr_measures)</span>
<span id="cb140-123"><a href="#cb140-123" aria-hidden="true" tabindex="-1"></a>msr_tbl[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, .(key, label, task_type)]</span>
<span id="cb140-124"><a href="#cb140-124" aria-hidden="true" tabindex="-1"></a>msr_tbl[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, .(key, packages, predict_type, task_properties)]</span>
<span id="cb140-125"><a href="#cb140-125" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-126"><a href="#cb140-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-127"><a href="#cb140-127" aria-hidden="true" tabindex="-1"></a>Depending on our task at hand, we will look for a measure that fits our <span class="in">`"task_type"`</span> (<span class="in">`"classif"`</span> for <span class="in">`penguins`</span>) and <span class="in">`"task_properties"`</span>.</span>
<span id="cb140-128"><a href="#cb140-128" aria-hidden="true" tabindex="-1"></a>The latter is important since measures like AUC <span class="in">`"classif.auc"`</span> are only defined for binary tasks, which is indicated by <span class="in">`"twoclass"`</span> in the <span class="in">`"task_properties"`</span> column --- multiclass-generalizations are available, but need to be selected explicitly.</span>
<span id="cb140-129"><a href="#cb140-129" aria-hidden="true" tabindex="-1"></a>Similarly, some measures require the learner to predict probabilities, while others require class predictions.</span>
<span id="cb140-130"><a href="#cb140-130" aria-hidden="true" tabindex="-1"></a>In our learner above, we have already selected <span class="in">`predict_type = "prob"`</span>, which is often required for measures that are not defined on class labels, such as the aforementioned AUC.</span>
<span id="cb140-131"><a href="#cb140-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-132"><a href="#cb140-132" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb140-133"><a href="#cb140-133" aria-hidden="true" tabindex="-1"></a>More information about a performance measure, including its mathematical definition, can be obtained using the <span class="in">`$help()`</span> method of a <span class="in">`r ref("Measure")`</span> object, which opens the help page of the corresponding measure, e.g., <span class="in">`msr("classif.acc")$help()`</span> provides all information about the classification accuracy.</span>
<span id="cb140-134"><a href="#cb140-134" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-135"><a href="#cb140-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-136"><a href="#cb140-136" aria-hidden="true" tabindex="-1"></a>The code example below shows how to apply holdout (specified using <span class="in">`rsmp("holdout")`</span>) on the previously specified <span class="in">`r ref("mlr_tasks_penguins")`</span> task to estimate classification accuracy (using <span class="in">`msr("classif.acc")`</span>) of the previously defined decision tree learner from the <span class="in">`r ref_pkg("rpart")`</span> package:</span>
<span id="cb140-137"><a href="#cb140-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-138"><a href="#cb140-138" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-005}</span></span>
<span id="cb140-139"><a href="#cb140-139" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>)</span>
<span id="cb140-140"><a href="#cb140-140" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(<span class="at">task =</span> task, <span class="at">learner =</span> learner, <span class="at">resampling =</span> resampling)</span>
<span id="cb140-141"><a href="#cb140-141" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-142"><a href="#cb140-142" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-143"><a href="#cb140-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-144"><a href="#cb140-144" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("benchmark()")`</span> function internally uses the <span class="in">`r ref("resample()")`</span> function to estimate the performance based on a resampling strategy.</span>
<span id="cb140-145"><a href="#cb140-145" aria-hidden="true" tabindex="-1"></a>For illustration, we show a minimal code example that compares the classification accuracy of the decision tree against a featureless learner which always predicts the majority class:</span>
<span id="cb140-146"><a href="#cb140-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-147"><a href="#cb140-147" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-006}</span></span>
<span id="cb140-148"><a href="#cb140-148" aria-hidden="true" tabindex="-1"></a>lrns <span class="ot">=</span> <span class="fu">c</span>(learner, <span class="fu">lrn</span>(<span class="st">"classif.featureless"</span>))</span>
<span id="cb140-149"><a href="#cb140-149" aria-hidden="true" tabindex="-1"></a>d <span class="ot">=</span> <span class="fu">benchmark_grid</span>(<span class="at">task =</span> task, <span class="at">learner =</span> lrns, <span class="at">resampling =</span> resampling)</span>
<span id="cb140-150"><a href="#cb140-150" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(<span class="at">design =</span> d)</span>
<span id="cb140-151"><a href="#cb140-151" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-152"><a href="#cb140-152" aria-hidden="true" tabindex="-1"></a>acc[, .(task_id, learner_id, classif.acc)]</span>
<span id="cb140-153"><a href="#cb140-153" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-154"><a href="#cb140-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-155"><a href="#cb140-155" aria-hidden="true" tabindex="-1"></a>Further details on resampling and benchmarking can be found in @sec-resampling and @sec-benchmarking.</span>
<span id="cb140-156"><a href="#cb140-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-157"><a href="#cb140-157" aria-hidden="true" tabindex="-1"></a><span class="fu">## Resampling {#sec-resampling}</span></span>
<span id="cb140-158"><a href="#cb140-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-159"><a href="#cb140-159" aria-hidden="true" tabindex="-1"></a>Existing resampling strategies differ in how they partition the available data into training and test set, and a comprehensive overview can be found in @japkowicz2011evaluating.</span>
<span id="cb140-160"><a href="#cb140-160" aria-hidden="true" tabindex="-1"></a>For example, the $k$-fold <span class="in">`r index("cross-validation")`</span> method randomly partitions the data into $k$ subsets, called folds (see @fig-cv-illustration).</span>
<span id="cb140-161"><a href="#cb140-161" aria-hidden="true" tabindex="-1"></a>Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations.</span>
<span id="cb140-162"><a href="#cb140-162" aria-hidden="true" tabindex="-1"></a>The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate.</span>
<span id="cb140-163"><a href="#cb140-163" aria-hidden="true" tabindex="-1"></a>This makes cross-validation a popular strategy, as each observation is guaranteed to be used in one of the test sets throughout the procedure, making efficient use of the available data for performance estimation.</span>
<span id="cb140-164"><a href="#cb140-164" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: See BB comment, pro-cons of CV --&gt;</span></span>
<span id="cb140-165"><a href="#cb140-165" aria-hidden="true" tabindex="-1"></a>Several variations of cross-validation exist, including repeated $k$-fold cross-validation where the entire process illustrated in @fig-cv-illustration is repeated multiple times, and leave-one-out cross-validation where the test set in each fold consists of exactly one observation.</span>
<span id="cb140-166"><a href="#cb140-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-167"><a href="#cb140-167" aria-hidden="true" tabindex="-1"></a>Other well-known resampling strategies include <span class="in">`r index("subsampling")`</span> and <span class="in">`r index("bootstrapping")`</span>.</span>
<span id="cb140-168"><a href="#cb140-168" aria-hidden="true" tabindex="-1"></a>Subsampling --- also known as repeated holdout --- repeats the holdout method and creates multiple train-test splits, taking into account the ratio of observations to be included in the training sets.</span>
<span id="cb140-169"><a href="#cb140-169" aria-hidden="true" tabindex="-1"></a>Bootstrapping creates training sets by randomly drawing observations from all available data with replacement.</span>
<span id="cb140-170"><a href="#cb140-170" aria-hidden="true" tabindex="-1"></a>Some observations in the training sets may appear more than once, while the other observations that do not appear at all are used as test set.</span>
<span id="cb140-171"><a href="#cb140-171" aria-hidden="true" tabindex="-1"></a>The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment.</span>
<span id="cb140-172"><a href="#cb140-172" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Japkowicz and Shah (2011) make a distinction between resampling strategies that use observations from the test set only once and resampling strategies that use observations from the test set multiple times. --&gt;</span></span>
<span id="cb140-173"><a href="#cb140-173" aria-hidden="true" tabindex="-1"></a>Properties and pitfalls of different resampling techniques have been widely studied and discussed in the literature, see e.g., @bengio2003no, @molinaro2005prediction, @kim2009estimating, @bischl2012resampling.</span>
<span id="cb140-174"><a href="#cb140-174" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: See BB comment, summarize paper content a bit, maybe in separate section? --&gt;</span></span>
<span id="cb140-175"><a href="#cb140-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-176"><a href="#cb140-176" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Source: https://docs.google.com/presentation/d/1BJXJ365C9TWelojV93IeQJAtEiD3uZMFSfkhzgYH-n8/edit?usp=sharing --&gt;</span></span>
<span id="cb140-177"><a href="#cb140-177" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-007, echo=FALSE}</span></span>
<span id="cb140-178"><a href="#cb140-178" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-cv-illustration</span></span>
<span id="cb140-179"><a href="#cb140-179" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of a 3-fold cross-validation."</span></span>
<span id="cb140-180"><a href="#cb140-180" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-181"><a href="#cb140-181" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A diagram illustration 3-fold cross-validation. Each row of the diagram represents one iteration. In each iteration the available data is split into 3 parts, where in each row a different part is marked as the test set. The two remaining parts are the train set, which is used to train a model. Each iteration results in one performance estimate, and all 3 are averaged in the end."</span></span>
<span id="cb140-182"><a href="#cb140-182" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/cross-validation.svg"</span>)</span>
<span id="cb140-183"><a href="#cb140-183" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-184"><a href="#cb140-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-185"><a href="#cb140-185" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3`</span>, many resampling strategies have already been implemented so that users do not have to implement them from scratch, which can be tedious and error-prone. In this section, we cover how to use <span class="in">`r mlr3`</span> to</span>
<span id="cb140-186"><a href="#cb140-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-187"><a href="#cb140-187" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>query (@sec-resampling-strategies) implemented resampling strategies,</span>
<span id="cb140-188"><a href="#cb140-188" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>construct (@sec-resampling-construct) resampling objects for a selected resampling strategy,</span>
<span id="cb140-189"><a href="#cb140-189" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>instantiate (@sec-resampling-inst) the train-test splits of a resampling object on a given task, and</span>
<span id="cb140-190"><a href="#cb140-190" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>execute (@sec-resampling-exec) the selected resampling strategy on a learning algorithm to obtain resampling results.</span>
<span id="cb140-191"><a href="#cb140-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-192"><a href="#cb140-192" aria-hidden="true" tabindex="-1"></a><span class="fu">### Query {#sec-resampling-strategies}</span></span>
<span id="cb140-193"><a href="#cb140-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-194"><a href="#cb140-194" aria-hidden="true" tabindex="-1"></a>All implemented resampling strategies can be queried by looking at the <span class="in">`r ref("mlr_resamplings")`</span> dictionary (also listed in Appendix @sec-appendix-overview-tables).</span>
<span id="cb140-195"><a href="#cb140-195" aria-hidden="true" tabindex="-1"></a>Passing the dictionary to the <span class="in">`as.data.table`</span> function provides a more structured output with additional information:</span>
<span id="cb140-196"><a href="#cb140-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-197"><a href="#cb140-197" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-008}</span></span>
<span id="cb140-198"><a href="#cb140-198" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(mlr_resamplings)</span>
<span id="cb140-199"><a href="#cb140-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-200"><a href="#cb140-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-201"><a href="#cb140-201" aria-hidden="true" tabindex="-1"></a>For example, the column <span class="in">`params`</span> shows the parameters of each resampling strategy (e.g., the train-test splitting <span class="in">`ratio`</span> or the number of <span class="in">`repeats`</span>) and the column <span class="in">`iters`</span> shows the default value for the number of performed resampling iterations (i.e., the number of model fits).</span>
<span id="cb140-202"><a href="#cb140-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-203"><a href="#cb140-203" aria-hidden="true" tabindex="-1"></a><span class="fu">### Construction {#sec-resampling-construct}</span></span>
<span id="cb140-204"><a href="#cb140-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-205"><a href="#cb140-205" aria-hidden="true" tabindex="-1"></a>Once we have decided on a resampling strategy, we have to construct a <span class="in">`r ref("Resampling")`</span> object via the function <span class="in">`r ref("rsmp()")`</span> which will define the resampling strategy we want to employ.</span>
<span id="cb140-206"><a href="#cb140-206" aria-hidden="true" tabindex="-1"></a>For example, to construct a <span class="in">`r ref("Resampling")`</span> object for holdout, we use the value of the <span class="in">`key`</span> column from the <span class="in">`r ref("mlr_resamplings")`</span> dictionary and pass it to the convenience function <span class="in">`r ref("rsmp()")`</span>:</span>
<span id="cb140-207"><a href="#cb140-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-208"><a href="#cb140-208" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-009}</span></span>
<span id="cb140-209"><a href="#cb140-209" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>)</span>
<span id="cb140-210"><a href="#cb140-210" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(resampling)</span>
<span id="cb140-211"><a href="#cb140-211" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-212"><a href="#cb140-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-213"><a href="#cb140-213" aria-hidden="true" tabindex="-1"></a>By default, the holdout method will use 2/3 of the data as training set and 1/3 as test set.</span>
<span id="cb140-214"><a href="#cb140-214" aria-hidden="true" tabindex="-1"></a>We can adjust this by specifying the <span class="in">`ratio`</span> parameter for holdout either during construction or by updating the <span class="in">`ratio`</span> parameter afterwards.</span>
<span id="cb140-215"><a href="#cb140-215" aria-hidden="true" tabindex="-1"></a>For example, we construct a <span class="in">`r ref("Resampling")`</span> object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):</span>
<span id="cb140-216"><a href="#cb140-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-217"><a href="#cb140-217" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-010}</span></span>
<span id="cb140-218"><a href="#cb140-218" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb140-219"><a href="#cb140-219" aria-hidden="true" tabindex="-1"></a>resampling<span class="sc">$</span>param_set<span class="sc">$</span>values <span class="ot">=</span> <span class="fu">list</span>(<span class="at">ratio =</span> <span class="fl">0.5</span>)</span>
<span id="cb140-220"><a href="#cb140-220" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-221"><a href="#cb140-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-222"><a href="#cb140-222" aria-hidden="true" tabindex="-1"></a>Holdout only estimates the generalization performance using a single test set.</span>
<span id="cb140-223"><a href="#cb140-223" aria-hidden="true" tabindex="-1"></a>To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies.</span>
<span id="cb140-224"><a href="#cb140-224" aria-hidden="true" tabindex="-1"></a>For example, we could also set up a 10-fold cross-validation via</span>
<span id="cb140-225"><a href="#cb140-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-226"><a href="#cb140-226" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-011}</span></span>
<span id="cb140-227"><a href="#cb140-227" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">10</span>)</span>
<span id="cb140-228"><a href="#cb140-228" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-229"><a href="#cb140-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-230"><a href="#cb140-230" aria-hidden="true" tabindex="-1"></a>By default, the <span class="in">`$is_instantiated`</span> field of a <span class="in">`r ref("Resampling")`</span> object constructed as shown above is set to <span class="in">`FALSE`</span>.</span>
<span id="cb140-231"><a href="#cb140-231" aria-hidden="true" tabindex="-1"></a>This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the <span class="in">`r ref("Resampling")`</span> object.</span>
<span id="cb140-232"><a href="#cb140-232" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy. --&gt;</span></span>
<span id="cb140-233"><a href="#cb140-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-234"><a href="#cb140-234" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue. --&gt;</span></span>
<span id="cb140-235"><a href="#cb140-235" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly. --&gt;</span></span>
<span id="cb140-236"><a href="#cb140-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-237"><a href="#cb140-237" aria-hidden="true" tabindex="-1"></a><span class="fu">### Instantiation {#sec-resampling-inst}</span></span>
<span id="cb140-238"><a href="#cb140-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-239"><a href="#cb140-239" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In this section, we show how to instantiate a resampling strategy (i.e., how to generate the train-test splits) by applying it to a task. --&gt;</span></span>
<span id="cb140-240"><a href="#cb140-240" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To obtain the row indices for the training and the test splits, we need to call the `instantiate()` method on a `r ref("Task")` object. --&gt;</span></span>
<span id="cb140-241"><a href="#cb140-241" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- The resulting train-test indices are then stored in the `r ref("Resampling")` object: --&gt;</span></span>
<span id="cb140-242"><a href="#cb140-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-243"><a href="#cb140-243" aria-hidden="true" tabindex="-1"></a>To generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the <span class="in">`$instantiate()`</span> method of the previously constructed <span class="in">`r ref("Resampling")`</span> object on a <span class="in">`r ref("Task")`</span>.</span>
<span id="cb140-244"><a href="#cb140-244" aria-hidden="true" tabindex="-1"></a>This will manifest a fixed partition and store the row indices for the training and test sets directly in the <span class="in">`r ref("Resampling")`</span> object.</span>
<span id="cb140-245"><a href="#cb140-245" aria-hidden="true" tabindex="-1"></a>We can access these rows via the <span class="in">`$train_set()`</span> and <span class="in">`$test_set()`</span> methods:</span>
<span id="cb140-246"><a href="#cb140-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-247"><a href="#cb140-247" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-012}</span></span>
<span id="cb140-248"><a href="#cb140-248" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"holdout"</span>, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb140-249"><a href="#cb140-249" aria-hidden="true" tabindex="-1"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb140-250"><a href="#cb140-250" aria-hidden="true" tabindex="-1"></a>train_ids <span class="ot">=</span> resampling<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>)</span>
<span id="cb140-251"><a href="#cb140-251" aria-hidden="true" tabindex="-1"></a>test_ids <span class="ot">=</span> resampling<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>)</span>
<span id="cb140-252"><a href="#cb140-252" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(train_ids)</span>
<span id="cb140-253"><a href="#cb140-253" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(test_ids)</span>
<span id="cb140-254"><a href="#cb140-254" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-255"><a href="#cb140-255" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-256"><a href="#cb140-256" aria-hidden="true" tabindex="-1"></a>Instantiation is especially relevant is when the aim is to fairly compare multiple learners.</span>
<span id="cb140-257"><a href="#cb140-257" aria-hidden="true" tabindex="-1"></a>Here, it is crucial to use the same train-test splits to obtain comparable results.</span>
<span id="cb140-258"><a href="#cb140-258" aria-hidden="true" tabindex="-1"></a>That is, we need to ensure that all learners to be compared use the same training data to build a model and that they use the same test data to evaluate the model performance.</span>
<span id="cb140-259"><a href="#cb140-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-260"><a href="#cb140-260" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb140-261"><a href="#cb140-261" aria-hidden="true" tabindex="-1"></a>In @sec-benchmarking, you will learn about the <span class="in">`ref ("benchmark()")`</span> function, which automatically instantiates <span class="in">`r ref("Resampling")`</span> objects on all tasks to ensure a fair comparison by making use of the exact same training and test sets for learning and evaluating the fitted intermediate models.</span>
<span id="cb140-262"><a href="#cb140-262" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-263"><a href="#cb140-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-264"><a href="#cb140-264" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Cut in favor of infobox, possibly reintegrate?</span></span>
<span id="cb140-265"><a href="#cb140-265" aria-hidden="true" tabindex="-1"></a><span class="co">This can be achieved using the same instantiated `r ref("Resampling")` object for each learner or using the `r ref("benchmark()")` function introduced in @sec-benchmarking which automatically instantiates the same train-test splits for each task.</span></span>
<span id="cb140-266"><a href="#cb140-266" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb140-267"><a href="#cb140-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-268"><a href="#cb140-268" aria-hidden="true" tabindex="-1"></a><span class="fu">### Execution {#sec-resampling-exec}</span></span>
<span id="cb140-269"><a href="#cb140-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-270"><a href="#cb140-270" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations. --&gt;</span></span>
<span id="cb140-271"><a href="#cb140-271" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object. --&gt;</span></span>
<span id="cb140-272"><a href="#cb140-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-273"><a href="#cb140-273" aria-hidden="true" tabindex="-1"></a>Calling the function <span class="in">`r ref("resample()")`</span> on a task, learner, and constructed resampling object returns a <span class="in">`r ref("ResampleResult")`</span> object which contains all information needed to estimate the generalization performance.</span>
<span id="cb140-274"><a href="#cb140-274" aria-hidden="true" tabindex="-1"></a>Specifically, the function will internally use the learner to train a model for each training set determined by the resampling strategy and store the model predictions of each test set.</span>
<span id="cb140-275"><a href="#cb140-275" aria-hidden="true" tabindex="-1"></a>We can apply the <span class="in">`print`</span> or <span class="in">`as.data.table`</span> function to a <span class="in">`r ref("ResampleResult")`</span> object to obtain some basic information:</span>
<span id="cb140-276"><a href="#cb140-276" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- By default, these models are discarded after the prediction step to reduce memory consumption of the resulting `r ref("ResampleResult")` object and because we usually only need the stored predictions to calculate the performance measure. --&gt;</span></span>
<span id="cb140-277"><a href="#cb140-277" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- However, we can configure the `r ref("resample()")` function to keep the fitted models (e.g. if we want to use or inspect the intermediate models later) by setting the `store_models` argument of the `r ref("resample()")` function to `TRUE`: --&gt;</span></span>
<span id="cb140-278"><a href="#cb140-278" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-279"><a href="#cb140-279" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-013}</span></span>
<span id="cb140-280"><a href="#cb140-280" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">4</span>)</span>
<span id="cb140-281"><a href="#cb140-281" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling)</span>
<span id="cb140-282"><a href="#cb140-282" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(rr)</span>
<span id="cb140-283"><a href="#cb140-283" aria-hidden="true" tabindex="-1"></a><span class="fu">as.data.table</span>(rr)</span>
<span id="cb140-284"><a href="#cb140-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-285"><a href="#cb140-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-286"><a href="#cb140-286" aria-hidden="true" tabindex="-1"></a>Here, we used 4-fold cross-validation as resampling strategy.</span>
<span id="cb140-287"><a href="#cb140-287" aria-hidden="true" tabindex="-1"></a>The resulting <span class="in">`r ref("ResampleResult")`</span> object (stored as <span class="in">`rr`</span>) provides various methods to access the stored information.</span>
<span id="cb140-288"><a href="#cb140-288" aria-hidden="true" tabindex="-1"></a>The two most relevant methods for performance assessment are <span class="in">`$score()`</span> and <span class="in">`$aggregate()`</span>.</span>
<span id="cb140-289"><a href="#cb140-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-290"><a href="#cb140-290" aria-hidden="true" tabindex="-1"></a>In @sec-eval, we learned that <span class="in">`r ref("Prediction")`</span> objects contain both model predictions and ground truth values, which are used to calculate the performance measure using the <span class="in">`$score()`</span> method.</span>
<span id="cb140-291"><a href="#cb140-291" aria-hidden="true" tabindex="-1"></a>Similarly, we can use the <span class="in">`$score()`</span> method of a <span class="in">`r ref("ResampleResult")`</span> object to calculate the performance measure for each resampling iteration separately.</span>
<span id="cb140-292"><a href="#cb140-292" aria-hidden="true" tabindex="-1"></a>This means that the <span class="in">`$score()`</span> method produces one value per resampling iteration that reflects the performance estimate of the intermediate model trained in the corresponding iteration.</span>
<span id="cb140-293"><a href="#cb140-293" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Specifically, it extracts the model predictions of each resampling iteration and calculates the performance measure in each resampling iteration separately. --&gt;</span></span>
<span id="cb140-294"><a href="#cb140-294" aria-hidden="true" tabindex="-1"></a>By default, <span class="in">`$score()`</span> uses the test set in each resampling iteration to calculate the performance measure.</span>
<span id="cb140-295"><a href="#cb140-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-296"><a href="#cb140-296" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb140-297"><a href="#cb140-297" aria-hidden="true" tabindex="-1"></a>We are not limited to scoring predictions on the test set --- if we set the argument <span class="in">`predict_sets = "train"`</span> within the <span class="in">`$score()`</span> method, we calculate the performance measure of each resampling iteration based on the training set instead of the test set.</span>
<span id="cb140-298"><a href="#cb140-298" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-299"><a href="#cb140-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-300"><a href="#cb140-300" aria-hidden="true" tabindex="-1"></a>In the code example below, we explicitly use the classification accuracy (<span class="in">`classif.acc`</span>) as performance measure and pass it to the <span class="in">`$score()`</span> method to obtain the estimated performance of each resampling iteration separately:</span>
<span id="cb140-301"><a href="#cb140-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-302"><a href="#cb140-302" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-014}</span></span>
<span id="cb140-303"><a href="#cb140-303" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-304"><a href="#cb140-304" aria-hidden="true" tabindex="-1"></a>acc[, .(iteration, classif.acc)]</span>
<span id="cb140-305"><a href="#cb140-305" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-306"><a href="#cb140-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-307"><a href="#cb140-307" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb140-308"><a href="#cb140-308" aria-hidden="true" tabindex="-1"></a>If we do not explicitly pass a <span class="in">`r ref("Measure")`</span> object to the <span class="in">`$score()`</span> method, the classification error (<span class="in">`classif.ce`</span>) and the mean squared error (<span class="in">`regr.mse`</span>) are used as defaults for classification and regression tasks respectively.</span>
<span id="cb140-309"><a href="#cb140-309" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-310"><a href="#cb140-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-311"><a href="#cb140-311" aria-hidden="true" tabindex="-1"></a>Similarly, we can pass <span class="in">`r ref("Measure")`</span> objects to the <span class="in">`$aggregate()`</span> method to calculate an aggregated score across all resampling iterations.</span>
<span id="cb140-312"><a href="#cb140-312" aria-hidden="true" tabindex="-1"></a>The type of aggregation is usually determined by the <span class="in">`r ref("Measure")`</span> object (see also the fields <span class="in">`$average`</span> and <span class="in">`$aggregator`</span> the in help page of <span class="in">`r ref("Measure")`</span> for more details).</span>
<span id="cb140-313"><a href="#cb140-313" aria-hidden="true" tabindex="-1"></a>There are two approaches for aggregating scores across resampling iterations: The first is referred to as the <span class="in">`r index("macro average")`</span>, which first calculates the measure in each resampling iteration separately, and then averages these scores across all iterations.</span>
<span id="cb140-314"><a href="#cb140-314" aria-hidden="true" tabindex="-1"></a>The second approach is the <span class="in">`r index("micro average")`</span>, which pools all predictions across resampling iterations into one <span class="in">`r ref("Prediction")`</span> object and computes the measure on this directly.</span>
<span id="cb140-315"><a href="#cb140-315" aria-hidden="true" tabindex="-1"></a>The classification accuracy <span class="in">`msr("classif.acc")`</span> uses the macro-average by default, but the micro-average can be computed as well by specifying the <span class="in">`average`</span> argument:</span>
<span id="cb140-316"><a href="#cb140-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-317"><a href="#cb140-317" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-015}</span></span>
<span id="cb140-318"><a href="#cb140-318" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb140-319"><a href="#cb140-319" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-320"><a href="#cb140-320" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>, <span class="at">average =</span> <span class="st">"micro"</span>))</span>
<span id="cb140-321"><a href="#cb140-321" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-322"><a href="#cb140-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-323"><a href="#cb140-323" aria-hidden="true" tabindex="-1"></a>::: {.callout-tip}</span>
<span id="cb140-324"><a href="#cb140-324" aria-hidden="true" tabindex="-1"></a>The classification accuracy compares the predicted class and the ground truth class of a single observation (point-wise loss) and calculates the proportion of correctly classified observations (average of point-wise loss).</span>
<span id="cb140-325"><a href="#cb140-325" aria-hidden="true" tabindex="-1"></a>For performance measures that simply take the (unweighted) average of point-wise losses such as the classification accuracy, macro-averaging and micro-averaging will be equivalent unless the test sets in each resampling iteration have different sizes.</span>
<span id="cb140-326"><a href="#cb140-326" aria-hidden="true" tabindex="-1"></a>For example, in the code example above, macro-averaging and micro-averaging yield the same classification accuracy because the <span class="in">`r ref("mlr_tasks_penguins")`</span> task (consisting of 344 observations) is split into 4 equally-sized test sets (consisting of 86 observations each) due to the 4-fold cross-validation.</span>
<span id="cb140-327"><a href="#cb140-327" aria-hidden="true" tabindex="-1"></a>If we would use 5-fold cross-validation instead, macro-averaging and micro-averaging can lead to a (slightly) different performance estimate as the test sets can not have the exact same size:</span>
<span id="cb140-328"><a href="#cb140-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-329"><a href="#cb140-329" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-016}</span></span>
<span id="cb140-330"><a href="#cb140-330" aria-hidden="true" tabindex="-1"></a>rr5 <span class="ot">=</span> <span class="fu">resample</span>(task, learner, <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>))</span>
<span id="cb140-331"><a href="#cb140-331" aria-hidden="true" tabindex="-1"></a>rr5<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-332"><a href="#cb140-332" aria-hidden="true" tabindex="-1"></a>rr5<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>, <span class="at">average =</span> <span class="st">"micro"</span>))</span>
<span id="cb140-333"><a href="#cb140-333" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-334"><a href="#cb140-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-335"><a href="#cb140-335" aria-hidden="true" tabindex="-1"></a>For other performance measures that are not defined on observation level but rather on a set of observations such as the area under the ROC curve <span class="in">`msr("classif.auc")`</span>, macro-averaging and micro-averaging will usually always lead to different values.</span>
<span id="cb140-336"><a href="#cb140-336" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-337"><a href="#cb140-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-338"><a href="#cb140-338" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- By default, the classification accuracy uses the macro average, i.e., the performance measure is calculated in each resampling iteration separately and then averaged to obtain the macro-averaged performance estimate. --&gt;</span></span>
<span id="cb140-339"><a href="#cb140-339" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- i.e., the `$average` field of a performance measure specifies whether micro or macro averaging is used and the `$aggregator` specifies the function used to aggregate the individual performance values that are calculated in each resampling iteration. --&gt;</span></span>
<span id="cb140-340"><a href="#cb140-340" aria-hidden="true" tabindex="-1"></a>The aggregated score (as returned by <span class="in">`$aggregate()`</span>) refers to the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the <span class="in">`r ref("Resampling")`</span> object.</span>
<span id="cb140-341"><a href="#cb140-341" aria-hidden="true" tabindex="-1"></a>While we are usually interested in this aggregated score, it can be useful to look at the individual performance values of each resampling iteration (as returned by the <span class="in">`$score()`</span> method) as well, e.g., to see if one (or more) of the iterations lead to very different performance results.</span>
<span id="cb140-342"><a href="#cb140-342" aria-hidden="true" tabindex="-1"></a>@fig-score-aggregate-resampling visualizes the relationship between <span class="in">`$score()`</span> and <span class="in">`$aggregate()`</span> for a small example based on the <span class="in">`"penguins"`</span> task.</span>
<span id="cb140-343"><a href="#cb140-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-344"><a href="#cb140-344" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-017}</span></span>
<span id="cb140-345"><a href="#cb140-345" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb140-346"><a href="#cb140-346" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-score-aggregate-resampling</span></span>
<span id="cb140-347"><a href="#cb140-347" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "An example of the difference between `$score()` and `$aggregate()`: The former aggregates predictions to a single score within each resampling iteration, and the former aggregates scores across all resampling folds"</span></span>
<span id="cb140-348"><a href="#cb140-348" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-349"><a href="#cb140-349" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "A funnel-shaped diagram. Left: Each resampling iteration contains multiple rows of predictions, with 3 iterations total. Middle: $score() reduces those to one performance score per resampling iteration, which leaves 3 scores. Right: $aggregate() reduces predictions across all resampling iterations to a single performance score."</span></span>
<span id="cb140-350"><a href="#cb140-350" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/predict-score-aggregate-resampling.drawio.svg"</span>)</span>
<span id="cb140-351"><a href="#cb140-351" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-352"><a href="#cb140-352" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-353"><a href="#cb140-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-354"><a href="#cb140-354" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inspect ResampleResult Objects {#sec-resampling-inspect}</span></span>
<span id="cb140-355"><a href="#cb140-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-356"><a href="#cb140-356" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {{&lt; include _optional.qmd &gt;}} --&gt;</span></span>
<span id="cb140-357"><a href="#cb140-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-358"><a href="#cb140-358" aria-hidden="true" tabindex="-1"></a>In this section, we show how to inspect some important fields and methods of a <span class="in">`r ref("ResampleResult")`</span> object.</span>
<span id="cb140-359"><a href="#cb140-359" aria-hidden="true" tabindex="-1"></a>We first take a glimpse at what is actually contained in the object by converting it to a <span class="in">`r ref("data.table")`</span>:</span>
<span id="cb140-360"><a href="#cb140-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-361"><a href="#cb140-361" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-018}</span></span>
<span id="cb140-362"><a href="#cb140-362" aria-hidden="true" tabindex="-1"></a>rrdt <span class="ot">=</span> <span class="fu">as.data.table</span>(rr)</span>
<span id="cb140-363"><a href="#cb140-363" aria-hidden="true" tabindex="-1"></a>rrdt</span>
<span id="cb140-364"><a href="#cb140-364" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-365"><a href="#cb140-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-366"><a href="#cb140-366" aria-hidden="true" tabindex="-1"></a>We can see that the task, learner and resampling strategy which we previously passed to the <span class="in">`r ref("resample()")`</span> function is stored in list columns of the <span class="in">`r ref("data.table")`</span>.</span>
<span id="cb140-367"><a href="#cb140-367" aria-hidden="true" tabindex="-1"></a>In addition, we also have an integer column <span class="in">`iteration`</span> that refers to the resampling iteration and another list column that contains the corresponding <span class="in">`r ref("Prediction")`</span> objects of each iteration.</span>
<span id="cb140-368"><a href="#cb140-368" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- we are already familiar with, and which are needed to calculate performance measures. --&gt;</span></span>
<span id="cb140-369"><a href="#cb140-369" aria-hidden="true" tabindex="-1"></a>We can access the respective <span class="in">`prediction`</span> column or directly use the <span class="in">`$predictions()`</span> method of the <span class="in">`r ref("ResampleResult")`</span> object (without converting it to a <span class="in">`r ref("data.table")`</span> first) to obtain a list of <span class="in">`r ref("Prediction")`</span> objects of each resampling iteration:</span>
<span id="cb140-370"><a href="#cb140-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-371"><a href="#cb140-371" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-019}</span></span>
<span id="cb140-372"><a href="#cb140-372" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb140-373"><a href="#cb140-373" aria-hidden="true" tabindex="-1"></a>rrdt<span class="sc">$</span>prediction</span>
<span id="cb140-374"><a href="#cb140-374" aria-hidden="true" tabindex="-1"></a><span class="fu">all.equal</span>(rrdt<span class="sc">$</span>prediction, rr<span class="sc">$</span><span class="fu">predictions</span>())</span>
<span id="cb140-375"><a href="#cb140-375" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-376"><a href="#cb140-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-377"><a href="#cb140-377" aria-hidden="true" tabindex="-1"></a>This allows to analyze the predictions of individual intermediate models from each resampling iteration and, e.g., to manually compute a macro-averaged performance estimate.</span>
<span id="cb140-378"><a href="#cb140-378" aria-hidden="true" tabindex="-1"></a>Instead, we can use the <span class="in">`$prediction()`</span> method to extract a single <span class="in">`r ref("Prediction")`</span> object that combines the predictions of each intermediate model arcoss all resampling iterations.</span>
<span id="cb140-379"><a href="#cb140-379" aria-hidden="true" tabindex="-1"></a>The combined prediction object can be used to manually compute a micro-averaged performance estimate, for example:</span>
<span id="cb140-380"><a href="#cb140-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-381"><a href="#cb140-381" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-020}</span></span>
<span id="cb140-382"><a href="#cb140-382" aria-hidden="true" tabindex="-1"></a><span class="co">#| eval: true</span></span>
<span id="cb140-383"><a href="#cb140-383" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> rr<span class="sc">$</span><span class="fu">prediction</span>()</span>
<span id="cb140-384"><a href="#cb140-384" aria-hidden="true" tabindex="-1"></a>pred</span>
<span id="cb140-385"><a href="#cb140-385" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-386"><a href="#cb140-386" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-387"><a href="#cb140-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-388"><a href="#cb140-388" aria-hidden="true" tabindex="-1"></a>By default, the intermediate models produced at each resampling iteration are discarded after the prediction step to reduce memory consumption of the <span class="in">`r ref("ResampleResult")`</span> object and because only the predictions are required to calculate the performance measure.</span>
<span id="cb140-389"><a href="#cb140-389" aria-hidden="true" tabindex="-1"></a>However, it can sometimes be useful to inspect, compare, or extract information from these intermediate models.</span>
<span id="cb140-390"><a href="#cb140-390" aria-hidden="true" tabindex="-1"></a>To do so, we can configure the <span class="in">`r ref("resample()")`</span> function to keep the fitted intermediate models by setting the <span class="in">`store_models`</span> argument to <span class="in">`TRUE`</span>.</span>
<span id="cb140-391"><a href="#cb140-391" aria-hidden="true" tabindex="-1"></a>Each model trained in a specific resampling iteration is then stored in the resulting <span class="in">`r ref("ResampleResult")`</span> object and can be accessed via <span class="in">`$learners[[i]]$model`</span>, where <span class="in">`i`</span> refers to the <span class="in">`i`</span>-th resampling iteration:</span>
<span id="cb140-392"><a href="#cb140-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-393"><a href="#cb140-393" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-021}</span></span>
<span id="cb140-394"><a href="#cb140-394" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb140-395"><a href="#cb140-395" aria-hidden="true" tabindex="-1"></a>rr<span class="sc">$</span>learners[[<span class="dv">1</span>]]<span class="sc">$</span>model</span>
<span id="cb140-396"><a href="#cb140-396" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-397"><a href="#cb140-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-398"><a href="#cb140-398" aria-hidden="true" tabindex="-1"></a>Here, we see the model output of a decision tree fitted by the <span class="in">`r ref_pkg("rpart")`</span> package.</span>
<span id="cb140-399"><a href="#cb140-399" aria-hidden="true" tabindex="-1"></a>As models fitted by <span class="in">`r ref_pkg("rpart")`</span> provide information on how important features are, we can inspect if the importance varies across the resampling iterations:</span>
<span id="cb140-400"><a href="#cb140-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-401"><a href="#cb140-401" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-022}</span></span>
<span id="cb140-402"><a href="#cb140-402" aria-hidden="true" tabindex="-1"></a><span class="fu">lapply</span>(rr<span class="sc">$</span>learners, <span class="cf">function</span>(x) x<span class="sc">$</span>model<span class="sc">$</span>variable.importance)</span>
<span id="cb140-403"><a href="#cb140-403" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-404"><a href="#cb140-404" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-405"><a href="#cb140-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-406"><a href="#cb140-406" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- - Filter the result and keep only results of certain resampling iterations, e.g., use `$filter(c(1, 3))` to discard the results of the second resampling iteration. --&gt;</span></span>
<span id="cb140-407"><a href="#cb140-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-408"><a href="#cb140-408" aria-hidden="true" tabindex="-1"></a>Each resampling iteration involves a training step and a prediction step.</span>
<span id="cb140-409"><a href="#cb140-409" aria-hidden="true" tabindex="-1"></a>Learner-specific error or warning messages may occur at each of these two steps.</span>
<span id="cb140-410"><a href="#cb140-410" aria-hidden="true" tabindex="-1"></a>If the learner passed to the <span class="in">`r ref("resample()")`</span> function runs in an encapsulated framework that allows logging (see the <span class="in">`$encapsulate`</span> field of a <span class="in">`r ref("Learner")`</span> object), all potential warning or error messages will be stored in the <span class="in">`$warnings`</span> and <span class="in">`$errors`</span> fields of the <span class="in">`r ref("ResampleResult")`</span> object.</span>
<span id="cb140-411"><a href="#cb140-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-412"><a href="#cb140-412" aria-hidden="true" tabindex="-1"></a><span class="fu">### Custom Resampling {#sec-resamp-custom}</span></span>
<span id="cb140-413"><a href="#cb140-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-414"><a href="#cb140-414" aria-hidden="true" tabindex="-1"></a>{{&lt; include _optional.qmd &gt;}}</span>
<span id="cb140-415"><a href="#cb140-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-416"><a href="#cb140-416" aria-hidden="true" tabindex="-1"></a>Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study with pre-defined folds.</span>
<span id="cb140-417"><a href="#cb140-417" aria-hidden="true" tabindex="-1"></a>A custom resampling strategy can be constructed using <span class="in">`rsmp("custom")`</span>, where the row indices of the observations used for training and testing must be defined manually when instantiated in a task.</span>
<span id="cb140-418"><a href="#cb140-418" aria-hidden="true" tabindex="-1"></a>In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the <span class="in">`$train`</span> and <span class="in">`$test`</span> fields.</span>
<span id="cb140-419"><a href="#cb140-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-420"><a href="#cb140-420" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-023}</span></span>
<span id="cb140-421"><a href="#cb140-421" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom"</span>)</span>
<span id="cb140-422"><a href="#cb140-422" aria-hidden="true" tabindex="-1"></a>resampling<span class="sc">$</span><span class="fu">instantiate</span>(task,</span>
<span id="cb140-423"><a href="#cb140-423" aria-hidden="true" tabindex="-1"></a>  <span class="at">train =</span> <span class="fu">list</span>(<span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">50</span>, <span class="dv">151</span><span class="sc">:</span><span class="dv">333</span>)),</span>
<span id="cb140-424"><a href="#cb140-424" aria-hidden="true" tabindex="-1"></a>  <span class="at">test =</span> <span class="fu">list</span>(<span class="dv">51</span><span class="sc">:</span><span class="dv">150</span>)</span>
<span id="cb140-425"><a href="#cb140-425" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb140-426"><a href="#cb140-426" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-427"><a href="#cb140-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-428"><a href="#cb140-428" aria-hidden="true" tabindex="-1"></a>The resulting <span class="in">`r ref("Resampling")`</span> object can then be used like all other resampling strategies.</span>
<span id="cb140-429"><a href="#cb140-429" aria-hidden="true" tabindex="-1"></a>To show that both sets contain the row indices we have defined, we can inspect the instantiated <span class="in">`r ref("Resampling")`</span> object:</span>
<span id="cb140-430"><a href="#cb140-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-431"><a href="#cb140-431" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-024}</span></span>
<span id="cb140-432"><a href="#cb140-432" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(resampling<span class="sc">$</span><span class="fu">train_set</span>(<span class="dv">1</span>))</span>
<span id="cb140-433"><a href="#cb140-433" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(resampling<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>))</span>
<span id="cb140-434"><a href="#cb140-434" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-435"><a href="#cb140-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-436"><a href="#cb140-436" aria-hidden="true" tabindex="-1"></a>The above is equivalent to a single custom train-test split analogous to the holdout strategy.</span>
<span id="cb140-437"><a href="#cb140-437" aria-hidden="true" tabindex="-1"></a>A custom version of the cross-validation strategy can be constructed using <span class="in">`rsmp("custom_cv")`</span>.</span>
<span id="cb140-438"><a href="#cb140-438" aria-hidden="true" tabindex="-1"></a>The important difference is that we now have to specify either a custom <span class="in">`factor`</span> variable (using the <span class="in">`f`</span> argument of the <span class="in">`$instantiate()`</span> method) or a <span class="in">`factor`</span> column (using the <span class="in">`col`</span> argument of the <span class="in">`$instantiate()`</span> method) from the data to determine the folds.</span>
<span id="cb140-439"><a href="#cb140-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-440"><a href="#cb140-440" aria-hidden="true" tabindex="-1"></a>In the example below, we instantiate a custom 4-fold cross-validation strategy using a <span class="in">`factor`</span> variable called <span class="in">`folds`</span> that contains 4 equally sized levels to define the 4 folds, each with one quarter of the total size of the <span class="in">`"penguin"`</span> task:</span>
<span id="cb140-441"><a href="#cb140-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-442"><a href="#cb140-442" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-025}</span></span>
<span id="cb140-443"><a href="#cb140-443" aria-hidden="true" tabindex="-1"></a>custom_cv <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"custom_cv"</span>)</span>
<span id="cb140-444"><a href="#cb140-444" aria-hidden="true" tabindex="-1"></a>folds <span class="ot">=</span> <span class="fu">as.factor</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="at">each =</span> task<span class="sc">$</span>nrow<span class="sc">/</span><span class="dv">4</span>))</span>
<span id="cb140-445"><a href="#cb140-445" aria-hidden="true" tabindex="-1"></a>custom_cv<span class="sc">$</span><span class="fu">instantiate</span>(task, <span class="at">f =</span> folds)</span>
<span id="cb140-446"><a href="#cb140-446" aria-hidden="true" tabindex="-1"></a>custom_cv</span>
<span id="cb140-447"><a href="#cb140-447" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-448"><a href="#cb140-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-449"><a href="#cb140-449" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-450"><a href="#cb140-450" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resampling with Stratification and Grouping</span></span>
<span id="cb140-451"><a href="#cb140-451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-452"><a href="#cb140-452" aria-hidden="true" tabindex="-1"></a>{{&lt; include _complex.qmd &gt;}}</span>
<span id="cb140-453"><a href="#cb140-453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-454"><a href="#cb140-454" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3`</span>, we can assign a special role to a feature contained in the data by configuring the corresponding <span class="in">`$col_roles`</span> field of a <span class="in">`r ref("Task")`</span>.</span>
<span id="cb140-455"><a href="#cb140-455" aria-hidden="true" tabindex="-1"></a>The two relevant column roles that will affect behavior of a resampling strategy are <span class="in">`"group"`</span> or <span class="in">`"stratum"`</span>.</span>
<span id="cb140-456"><a href="#cb140-456" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-457"><a href="#cb140-457" aria-hidden="true" tabindex="-1"></a>In some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belong to a group, e.g., when the data contains repeated measurements of individuals (longitudinal studies) or when dealing with spatial or temporal data.</span>
<span id="cb140-458"><a href="#cb140-458" aria-hidden="true" tabindex="-1"></a>When observations belong to groups, we want to ensure that all observations of the same group belong to either the training set or the test set to prevent any potential leakage of information between training and testing sets.</span>
<span id="cb140-459"><a href="#cb140-459" aria-hidden="true" tabindex="-1"></a>For example, in a longitudinal study, measurements of a person are usually taken at multiple time points.</span>
<span id="cb140-460"><a href="#cb140-460" aria-hidden="true" tabindex="-1"></a>Grouping ensures that the model is tested on data from each person that it has not seen during training, while maintaining the integrity of the person's measurements across different time points.</span>
<span id="cb140-461"><a href="#cb140-461" aria-hidden="true" tabindex="-1"></a>In this context, the leave-one-out cross-validation strategy can be coarsened to the "leave-one-object-out" cross-validation strategy, where not only a single observation is left out, but all observations associated with a certain group (see @fig-group for an illustration).</span>
<span id="cb140-462"><a href="#cb140-462" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-463"><a href="#cb140-463" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- where it is not one measurement that is left out, but all measurements associated with one particular individual, institution, or broader object of measurement. --&gt;</span></span>
<span id="cb140-464"><a href="#cb140-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-465"><a href="#cb140-465" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-026, echo=FALSE}</span></span>
<span id="cb140-466"><a href="#cb140-466" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-group</span></span>
<span id="cb140-467"><a href="#cb140-467" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors)."</span></span>
<span id="cb140-468"><a href="#cb140-468" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-469"><a href="#cb140-469" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Illustration of the train-test splits of a leave-one-object-out cross-validation with 3 groups of observations (highlighted by different colors)."</span></span>
<span id="cb140-470"><a href="#cb140-470" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/loobject.svg"</span>)</span>
<span id="cb140-471"><a href="#cb140-471" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-472"><a href="#cb140-472" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-473"><a href="#cb140-473" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3`</span>, the column role <span class="in">`"group"`</span> allows to specify the column in the data that defines the group structure of the observations (see also the help page of <span class="in">`r ref("Resampling")`</span> for more information on the column role <span class="in">`"group"`</span>).</span>
<span id="cb140-474"><a href="#cb140-474" aria-hidden="true" tabindex="-1"></a>The column role can be specified by assigning a feature to the <span class="in">`$col_roles$group`</span> field which will then determine the group structure.</span>
<span id="cb140-475"><a href="#cb140-475" aria-hidden="true" tabindex="-1"></a>The following code performs leave-one-object-out cross-validation using the feature <span class="in">`year`</span> of the <span class="in">`r ref("mlr_tasks_penguins")`</span> task to determine the grouping.</span>
<span id="cb140-476"><a href="#cb140-476" aria-hidden="true" tabindex="-1"></a>Since the feature <span class="in">`year`</span> contains only three distinct values (i.e., <span class="in">`2007`</span>, <span class="in">`2008`</span>, and <span class="in">`2009`</span>), the corresponding test sets consist of observations from only one year:</span>
<span id="cb140-477"><a href="#cb140-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-478"><a href="#cb140-478" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In `r mlr3`, we can assign a special column role to a feature contained in the data either during task construction or afterwards by specifying the feature that defines the group in the `$col_roles$group` field.  --&gt;</span></span>
<span id="cb140-479"><a href="#cb140-479" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- For example, the column role `"group"` specifies which column in the data should be used to define the group structure of the observations (see also the help section on `r ref("Resampling")` for more information on the column role `"group"`). --&gt;</span></span>
<span id="cb140-480"><a href="#cb140-480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-481"><a href="#cb140-481" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- A possible use case for the need of grouping (or blocking) of observations during resampling is spatiotemporal modeling, where observations inherit a natural grouping, either in space or time or in both space and time that need to be considered during resampling. --&gt;</span></span>
<span id="cb140-482"><a href="#cb140-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-483"><a href="#cb140-483" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-027}</span></span>
<span id="cb140-484"><a href="#cb140-484" aria-hidden="true" tabindex="-1"></a>task_grp <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb140-485"><a href="#cb140-485" aria-hidden="true" tabindex="-1"></a>task_grp<span class="sc">$</span>col_roles<span class="sc">$</span>group <span class="ot">=</span> <span class="st">"year"</span></span>
<span id="cb140-486"><a href="#cb140-486" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"loo"</span>)</span>
<span id="cb140-487"><a href="#cb140-487" aria-hidden="true" tabindex="-1"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task_grp)</span>
<span id="cb140-488"><a href="#cb140-488" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-489"><a href="#cb140-489" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb140-490"><a href="#cb140-490" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb140-491"><a href="#cb140-491" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb140-492"><a href="#cb140-492" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task_grp<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"year"</span>))</span>
<span id="cb140-493"><a href="#cb140-493" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-494"><a href="#cb140-494" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-495"><a href="#cb140-495" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- </span><span class="al">TODO</span><span class="co">: Do we keep this note or relegate spatiotempcv to a "further reading" section at the end? When we mention spatiotemp in the text anyway, the note should maybe be just regular text as well? --&gt;</span></span>
<span id="cb140-496"><a href="#cb140-496" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-497"><a href="#cb140-497" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{.callout-tip appearance="simple"} --&gt;</span></span>
<span id="cb140-498"><a href="#cb140-498" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Dedicated spatiotemporal resampling methods are available in `r mlr3spatiotempcv` which implicitly take into account the spatiotemporal structure, see the [spatiotemporal resampling](special.html#spatiotemp-cv) section for more details. --&gt;</span></span>
<span id="cb140-499"><a href="#cb140-499" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb140-500"><a href="#cb140-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-501"><a href="#cb140-501" aria-hidden="true" tabindex="-1"></a>:::{.callout-tip}</span>
<span id="cb140-502"><a href="#cb140-502" aria-hidden="true" tabindex="-1"></a>If there are many groups, say 100 groups, we can limit the number of resampling iterations using k-fold cross-validation (or any other resampling strategy with a previously definable number of resampling iterations) instead of performing leave-one-object-out cross-validation.</span>
<span id="cb140-503"><a href="#cb140-503" aria-hidden="true" tabindex="-1"></a>In this case, each group is considered as a single observation, so that the division into training and test sets is done as determined by the resampling strategy</span>
<span id="cb140-504"><a href="#cb140-504" aria-hidden="true" tabindex="-1"></a>:::</span>
<span id="cb140-505"><a href="#cb140-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-506"><a href="#cb140-506" aria-hidden="true" tabindex="-1"></a>Another column role available in <span class="in">`r mlr3`</span> is <span class="in">`"stratum"`</span>, which implements stratified sampling.</span>
<span id="cb140-507"><a href="#cb140-507" aria-hidden="true" tabindex="-1"></a>Stratified sampling ensures that one or more discrete features within the training and test sets will have a similar distribution as in the original task containing all observations.</span>
<span id="cb140-508"><a href="#cb140-508" aria-hidden="true" tabindex="-1"></a>This is especially useful when a discrete feature is highly imbalanced and we want to make sure that the distribution of that feature is similar in each resampling iteration.</span>
<span id="cb140-509"><a href="#cb140-509" aria-hidden="true" tabindex="-1"></a>Stratification is commonly used for imbalanced classification tasks where the classes of the target feature are imbalanced (see @fig-stratification for an illustration).</span>
<span id="cb140-510"><a href="#cb140-510" aria-hidden="true" tabindex="-1"></a>Stratification by the target feature ensures that each intermediate model is fit on training data where the class distribution of the target is representative of the actual task.</span>
<span id="cb140-511"><a href="#cb140-511" aria-hidden="true" tabindex="-1"></a>Otherwise it could happen that target classes are severely under- or over represented in individual resampling iterations, skewing the estimation of the generalization performance.</span>
<span id="cb140-512"><a href="#cb140-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-513"><a href="#cb140-513" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-028, echo=FALSE}</span></span>
<span id="cb140-514"><a href="#cb140-514" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-stratification</span></span>
<span id="cb140-515"><a href="#cb140-515" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification)."</span></span>
<span id="cb140-516"><a href="#cb140-516" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-517"><a href="#cb140-517" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Illustration of a 3-fold cross-validation with stratification for an imbalanced binary classification task with a majority class that is about twice as large as the minority class. In each resampling iteration, the class distribution from the available data is preserved (which is not necessarily the case for cross-validation without stratification)."</span></span>
<span id="cb140-518"><a href="#cb140-518" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/stratification.svg"</span>)</span>
<span id="cb140-519"><a href="#cb140-519" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-520"><a href="#cb140-520" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-521"><a href="#cb140-521" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Stratified sampling ensures that the training and test sets will have similar distribution regarding one or more discrete features as in the original task containing all observations. --&gt;</span></span>
<span id="cb140-522"><a href="#cb140-522" aria-hidden="true" tabindex="-1"></a>The <span class="in">`$col_roles$stratum`</span> field of a <span class="in">`r ref("Task")`</span> can be set to one or multiple features (including the target in case of classification tasks).</span>
<span id="cb140-523"><a href="#cb140-523" aria-hidden="true" tabindex="-1"></a>In case of multiple features, each combination of the values of all stratification features will form a strata.</span>
<span id="cb140-524"><a href="#cb140-524" aria-hidden="true" tabindex="-1"></a>For example, the target column <span class="in">`species`</span> of the <span class="in">`r ref("mlr_tasks_penguins")`</span> task is imbalanced:</span>
<span id="cb140-525"><a href="#cb140-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-526"><a href="#cb140-526" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-029}</span></span>
<span id="cb140-527"><a href="#cb140-527" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-528"><a href="#cb140-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-529"><a href="#cb140-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-530"><a href="#cb140-530" aria-hidden="true" tabindex="-1"></a>Without specifying a <span class="in">`"stratum"`</span> column role, the <span class="in">`species`</span> column may have quite different class distributions across the training and test sets of a 3-fold cross-validation strategy:</span>
<span id="cb140-531"><a href="#cb140-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-532"><a href="#cb140-532" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-030}</span></span>
<span id="cb140-533"><a href="#cb140-533" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb140-534"><a href="#cb140-534" aria-hidden="true" tabindex="-1"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task)</span>
<span id="cb140-535"><a href="#cb140-535" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-536"><a href="#cb140-536" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-537"><a href="#cb140-537" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-538"><a href="#cb140-538" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-539"><a href="#cb140-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-540"><a href="#cb140-540" aria-hidden="true" tabindex="-1"></a>In the worst case, and especially for highly imbalanced classes, the minority class might be entirely left out of the training set in one or more resampling iterations.</span>
<span id="cb140-541"><a href="#cb140-541" aria-hidden="true" tabindex="-1"></a>Consequently, the intermediate models within these resampling iterations will never predict the minority class, resulting in a misleading performance estimate for any resampling strategy without stratification.</span>
<span id="cb140-542"><a href="#cb140-542" aria-hidden="true" tabindex="-1"></a>Relying on such a misleading performance estimate can have severe consequences for a deployed model, as it will perform poorly on the minority class in real-world scenarios.</span>
<span id="cb140-543"><a href="#cb140-543" aria-hidden="true" tabindex="-1"></a>For example, misclassification of the minority class can have serious consequences in certain applications such as in medical diagnosis or fraud detection, where failing to identify the minority class may result in serious harm or financial losses.</span>
<span id="cb140-544"><a href="#cb140-544" aria-hidden="true" tabindex="-1"></a>Therefore, it is important to be aware of the potential consequences of imbalanced class distributions in resampling and use stratification to mitigate highly unreliable performance estimates.</span>
<span id="cb140-545"><a href="#cb140-545" aria-hidden="true" tabindex="-1"></a>The code below uses <span class="in">`species`</span> as <span class="in">`"stratum"`</span> column role to illustrate that the distribution of <span class="in">`species`</span> in each test set will closely match the original distribution:</span>
<span id="cb140-546"><a href="#cb140-546" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-547"><a href="#cb140-547" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-031}</span></span>
<span id="cb140-548"><a href="#cb140-548" aria-hidden="true" tabindex="-1"></a>task_str <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"penguins"</span>)</span>
<span id="cb140-549"><a href="#cb140-549" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum <span class="ot">=</span> <span class="st">"species"</span></span>
<span id="cb140-550"><a href="#cb140-550" aria-hidden="true" tabindex="-1"></a>r <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">3</span>)</span>
<span id="cb140-551"><a href="#cb140-551" aria-hidden="true" tabindex="-1"></a>r<span class="sc">$</span><span class="fu">instantiate</span>(task_str)</span>
<span id="cb140-552"><a href="#cb140-552" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-553"><a href="#cb140-553" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">1</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-554"><a href="#cb140-554" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">2</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-555"><a href="#cb140-555" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.table</span>(<span class="fu">table</span>(task_str<span class="sc">$</span><span class="fu">data</span>(<span class="at">rows =</span> r<span class="sc">$</span><span class="fu">test_set</span>(<span class="dv">3</span>), <span class="at">cols =</span> <span class="st">"species"</span>)))</span>
<span id="cb140-556"><a href="#cb140-556" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-557"><a href="#cb140-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-558"><a href="#cb140-558" aria-hidden="true" tabindex="-1"></a>Rather than assigning the <span class="in">`$col_roles$stratum`</span> directly, it is also possible to use the <span class="in">`$set_col_roles()`</span> method to add or remove columns to specific roles incrementally:</span>
<span id="cb140-559"><a href="#cb140-559" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-560"><a href="#cb140-560" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-032}</span></span>
<span id="cb140-561"><a href="#cb140-561" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"species"</span>, <span class="at">remove_from =</span> <span class="st">"stratum"</span>)</span>
<span id="cb140-562"><a href="#cb140-562" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum</span>
<span id="cb140-563"><a href="#cb140-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-564"><a href="#cb140-564" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"species"</span>, <span class="at">add_to =</span> <span class="st">"stratum"</span>)</span>
<span id="cb140-565"><a href="#cb140-565" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>col_roles<span class="sc">$</span>stratum</span>
<span id="cb140-566"><a href="#cb140-566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-567"><a href="#cb140-567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-568"><a href="#cb140-568" aria-hidden="true" tabindex="-1"></a>We can further inspect the current stratification via the <span class="in">`$strata`</span> field, which returns a <span class="in">`data.table`</span> of the number of observations (<span class="in">`N`</span>) and row indices (<span class="in">`row_id`</span>) of each stratum.</span>
<span id="cb140-569"><a href="#cb140-569" aria-hidden="true" tabindex="-1"></a>Since we stratified by the <span class="in">`species`</span> column, we expect to see the same class frequencies as when we tabulate the task by the <span class="in">`species`</span> column:</span>
<span id="cb140-570"><a href="#cb140-570" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-571"><a href="#cb140-571" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-033}</span></span>
<span id="cb140-572"><a href="#cb140-572" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>strata</span>
<span id="cb140-573"><a href="#cb140-573" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="st">"species"</span>))</span>
<span id="cb140-574"><a href="#cb140-574" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-575"><a href="#cb140-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-576"><a href="#cb140-576" aria-hidden="true" tabindex="-1"></a>Should we add another stratification column, the <span class="in">`$strata`</span> field will show the same values as when we cross-tabulate the two variables of the task:</span>
<span id="cb140-577"><a href="#cb140-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-578"><a href="#cb140-578" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-034}</span></span>
<span id="cb140-579"><a href="#cb140-579" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span><span class="fu">set_col_roles</span>(<span class="st">"year"</span>, <span class="at">add_to =</span> <span class="st">"stratum"</span>)</span>
<span id="cb140-580"><a href="#cb140-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-581"><a href="#cb140-581" aria-hidden="true" tabindex="-1"></a>task_str<span class="sc">$</span>strata</span>
<span id="cb140-582"><a href="#cb140-582" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(task<span class="sc">$</span><span class="fu">data</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="st">"species"</span>, <span class="st">"year"</span>)))</span>
<span id="cb140-583"><a href="#cb140-583" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-584"><a href="#cb140-584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-585"><a href="#cb140-585" aria-hidden="true" tabindex="-1"></a><span class="fu">### Plotting Resample Results {#sec-autoplot-resampleresult}</span></span>
<span id="cb140-586"><a href="#cb140-586" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-587"><a href="#cb140-587" aria-hidden="true" tabindex="-1"></a><span class="in">`r mlr3viz`</span> provides a <span class="in">`r ref("autoplot()")`</span> method to automatically visualize the resampling results either in a boxplot or histogram:</span>
<span id="cb140-588"><a href="#cb140-588" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-589"><a href="#cb140-589" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-035}</span></span>
<span id="cb140-590"><a href="#cb140-590" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb140-591"><a href="#cb140-591" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"bootstrap"</span>)</span>
<span id="cb140-592"><a href="#cb140-592" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling)</span>
<span id="cb140-593"><a href="#cb140-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-594"><a href="#cb140-594" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlr3viz)</span>
<span id="cb140-595"><a href="#cb140-595" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"boxplot"</span>)</span>
<span id="cb140-596"><a href="#cb140-596" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>), <span class="at">type =</span> <span class="st">"histogram"</span>)</span>
<span id="cb140-597"><a href="#cb140-597" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-598"><a href="#cb140-598" aria-hidden="true" tabindex="-1"></a>The histogram is useful to visually gauge the variance of the performance results across resampling iterations, whereas the boxplot is often used when multiple learners are compared side-by-side.</span>
<span id="cb140-599"><a href="#cb140-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-600"><a href="#cb140-600" aria-hidden="true" tabindex="-1"></a>We can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:</span>
<span id="cb140-601"><a href="#cb140-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-602"><a href="#cb140-602" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-036}</span></span>
<span id="cb140-603"><a href="#cb140-603" aria-hidden="true" tabindex="-1"></a>task<span class="sc">$</span><span class="fu">select</span>(<span class="fu">c</span>(<span class="st">"bill_length"</span>, <span class="st">"flipper_length"</span>))</span>
<span id="cb140-604"><a href="#cb140-604" aria-hidden="true" tabindex="-1"></a>resampling <span class="ot">=</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">4</span>)</span>
<span id="cb140-605"><a href="#cb140-605" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(task, learner, resampling, <span class="at">store_models =</span> <span class="cn">TRUE</span>)</span>
<span id="cb140-606"><a href="#cb140-606" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"prediction"</span>)</span>
<span id="cb140-607"><a href="#cb140-607" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-608"><a href="#cb140-608" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-609"><a href="#cb140-609" aria-hidden="true" tabindex="-1"></a>Prediction surfaces like this are a useful tool for model inspection, as they can help to identify the cause of unexpected performance result.</span>
<span id="cb140-610"><a href="#cb140-610" aria-hidden="true" tabindex="-1"></a>Naturally, they are also popular for didactical purposes to illustrate the prediction behaviour of different learning algorithms, such as the classification tree in the example above with its characteristic orthogonal lines.</span>
<span id="cb140-611"><a href="#cb140-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-612"><a href="#cb140-612" aria-hidden="true" tabindex="-1"></a><span class="fu">## Benchmarking {#sec-benchmarking}</span></span>
<span id="cb140-613"><a href="#cb140-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-614"><a href="#cb140-614" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- introduction from chapter intro moved to section intro --&gt;</span></span>
<span id="cb140-615"><a href="#cb140-615" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("Benchmarking")`</span> in supervised machine learning refers to the comparison of different learners on a single task or multiple tasks.</span>
<span id="cb140-616"><a href="#cb140-616" aria-hidden="true" tabindex="-1"></a>When comparing learners on a single task or on a domain consisting of multiple similar tasks, the main aim is often to rank the learners according to a pre-defined performance measure and to identify the best-performing learner for the considered task or domain.</span>
<span id="cb140-617"><a href="#cb140-617" aria-hidden="true" tabindex="-1"></a>In an applied setting, benchmarking may be used to evaluate whether a deployed model used for a given task or domain can be replaced by a better alternative solution.</span>
<span id="cb140-618"><a href="#cb140-618" aria-hidden="true" tabindex="-1"></a>When comparing multiple learners on multiple tasks, the main aim is often more of a scientific nature, e.g., to gain insights into how different learners perform in different data situations or whether there are certain data properties that heavily affect the performance of certain learners (or certain hyperparameter of learners).</span>
<span id="cb140-619"><a href="#cb140-619" aria-hidden="true" tabindex="-1"></a>For example, it is common practice for algorithm designers to analyze the generalization performance or runtime of a newly proposed learning algorithm in a benchmark study where it has been compared with existing learners.</span>
<span id="cb140-620"><a href="#cb140-620" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In an applied setting, benchmarking is used to compare a production model to e.g. a novel method or re-trained model based on the original learner to evaluate whether the model is still suitable. --&gt;</span></span>
<span id="cb140-621"><a href="#cb140-621" aria-hidden="true" tabindex="-1"></a>In @sec-benchmarking, we provide code examples for conducting benchmark studies and performing statistical analysis of benchmark results using the <span class="in">`r mlr3`</span> package.</span>
<span id="cb140-622"><a href="#cb140-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-623"><a href="#cb140-623" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- previous subsection intro</span></span>
<span id="cb140-624"><a href="#cb140-624" aria-hidden="true" tabindex="-1"></a><span class="co">`r index("Benchmarking")` is used to compare the performance of different learning algorithms applied on one or more tasks using (potentially different) resampling strategies.</span></span>
<span id="cb140-625"><a href="#cb140-625" aria-hidden="true" tabindex="-1"></a><span class="co">The purpose is to rank the learning algorithms regarding a performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks.</span></span>
<span id="cb140-626"><a href="#cb140-626" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb140-627"><a href="#cb140-627" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r mlr3`</span> package offers the convenience function <span class="in">`r ref("benchmark()")`</span> to conduct a <span class="in">`r index("benchmark experiment")`</span>.</span>
<span id="cb140-628"><a href="#cb140-628" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- and repeatedly train and evaluate multiple learners under the same conditions. --&gt;</span></span>
<span id="cb140-629"><a href="#cb140-629" aria-hidden="true" tabindex="-1"></a>The function internally runs the <span class="in">`r ref("resample()")`</span> function on each task separately.</span>
<span id="cb140-630"><a href="#cb140-630" aria-hidden="true" tabindex="-1"></a>The provided resampling strategy is instantiated on each task to ensure a fair comparison by training and evaluating multiple learners under the same conditions.</span>
<span id="cb140-631"><a href="#cb140-631" aria-hidden="true" tabindex="-1"></a>This means that all provided learners use the same train-test splits for each task.</span>
<span id="cb140-632"><a href="#cb140-632" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Specifically, it repeatedly trains and evaluates all provided learners under the same conditions by the specified resampling strategy which is instantiated on each task to ensure a fair comparison. --&gt;</span></span>
<span id="cb140-633"><a href="#cb140-633" aria-hidden="true" tabindex="-1"></a>In this section, we cover how to</span>
<span id="cb140-634"><a href="#cb140-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-635"><a href="#cb140-635" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>construct a benchmark design (@sec-bm-design) to define the benchmark experiments to be performed,</span>
<span id="cb140-636"><a href="#cb140-636" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>run the benchmark experiments (@sec-bm-exec) and aggregate their results, and</span>
<span id="cb140-637"><a href="#cb140-637" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>convert benchmark objects (@sec-bm-resamp) to other types of objects that can be used for different purposes.</span>
<span id="cb140-638"><a href="#cb140-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-639"><a href="#cb140-639" aria-hidden="true" tabindex="-1"></a><span class="fu">### Constructing Benchmarking Designs {#sec-bm-design}</span></span>
<span id="cb140-640"><a href="#cb140-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-641"><a href="#cb140-641" aria-hidden="true" tabindex="-1"></a>In <span class="in">`r mlr3`</span>, we can define a design to perform benchmark experiments via the <span class="in">`r ref("benchmark_grid()")`</span> convenience function.</span>
<span id="cb140-642"><a href="#cb140-642" aria-hidden="true" tabindex="-1"></a>The design is essentially a table of scenarios to be evaluated and usually consists of unique combinations of <span class="in">`r ref("Task")`</span>, <span class="in">`r ref("Learner")`</span> and <span class="in">`r ref("Resampling")`</span> triplets.</span>
<span id="cb140-643"><a href="#cb140-643" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-644"><a href="#cb140-644" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref("benchmark_grid()")`</span> function constructs an exhaustive design to describe which combinations of learner, task and resampling should be used in a benchmark experiment.</span>
<span id="cb140-645"><a href="#cb140-645" aria-hidden="true" tabindex="-1"></a>It properly instantiates the used resampling strategies so that all learners are evaluated on the same train-test splits for each task, ensuring a fair comparison.</span>
<span id="cb140-646"><a href="#cb140-646" aria-hidden="true" tabindex="-1"></a>To construct a list of <span class="in">`r ref("Task")`</span>, <span class="in">`r ref("Learner")`</span> and <span class="in">`r ref("Resampling")`</span> objects, we can use the convenience functions <span class="in">`r ref("tsks()")`</span>, <span class="in">`r ref("lrns()")`</span>, and <span class="in">`r ref("rsmps()")`</span>.</span>
<span id="cb140-647"><a href="#cb140-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-648"><a href="#cb140-648" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure. --&gt;</span></span>
<span id="cb140-649"><a href="#cb140-649" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-650"><a href="#cb140-650" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data. --&gt;</span></span>
<span id="cb140-651"><a href="#cb140-651" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-652"><a href="#cb140-652" aria-hidden="true" tabindex="-1"></a>We design an exemplary benchmark experiment and train a classification tree from the <span class="in">`r ref_pkg("rpart")`</span> package, a random forest from the <span class="in">`r ref_pkg("ranger")`</span> package and a featureless learner serving as a baseline on four different binary classification tasks.</span>
<span id="cb140-653"><a href="#cb140-653" aria-hidden="true" tabindex="-1"></a>The constructed <span class="in">`r index("benchmark design")`</span> is a <span class="in">`data.table`</span> containing the task, learner, and resampling combinations in each row that should be performed:</span>
<span id="cb140-654"><a href="#cb140-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-655"><a href="#cb140-655" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-037}</span></span>
<span id="cb140-656"><a href="#cb140-656" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3verse"</span>)</span>
<span id="cb140-657"><a href="#cb140-657" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-658"><a href="#cb140-658" aria-hidden="true" tabindex="-1"></a>tsks <span class="ot">=</span> <span class="fu">tsks</span>(<span class="fu">c</span>(<span class="st">"german_credit"</span>, <span class="st">"sonar"</span>, <span class="st">"breast_cancer"</span>))</span>
<span id="cb140-659"><a href="#cb140-659" aria-hidden="true" tabindex="-1"></a>lrns <span class="ot">=</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.ranger"</span>, <span class="st">"classif.rpart"</span>, <span class="st">"classif.featureless"</span>),</span>
<span id="cb140-660"><a href="#cb140-660" aria-hidden="true" tabindex="-1"></a>  <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb140-661"><a href="#cb140-661" aria-hidden="true" tabindex="-1"></a>rsmp <span class="ot">=</span> <span class="fu">rsmps</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb140-662"><a href="#cb140-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-663"><a href="#cb140-663" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(tsks, lrns, rsmp)</span>
<span id="cb140-664"><a href="#cb140-664" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(design)</span>
<span id="cb140-665"><a href="#cb140-665" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-666"><a href="#cb140-666" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Note: Raphael's chapter comments were against printing the design grid, whereas Bernd in a later meeting was explicitly for printing it. --&gt;</span></span>
<span id="cb140-667"><a href="#cb140-667" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-668"><a href="#cb140-668" aria-hidden="true" tabindex="-1"></a>Since the <span class="in">`r ref("data.table")`</span> contains R6 columns within list-columns, we unfortunately can not infer too much about <span class="in">`task`</span> column, but the <span class="in">`r ref("ids")`</span> utility function can be used for quick inspection or subsetting:</span>
<span id="cb140-669"><a href="#cb140-669" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-670"><a href="#cb140-670" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-038}</span></span>
<span id="cb140-671"><a href="#cb140-671" aria-hidden="true" tabindex="-1"></a>mlr3misc<span class="sc">::</span><span class="fu">ids</span>(design<span class="sc">$</span>task)</span>
<span id="cb140-672"><a href="#cb140-672" aria-hidden="true" tabindex="-1"></a>design[mlr3misc<span class="sc">::</span><span class="fu">ids</span>(task) <span class="sc">==</span> <span class="st">"sonar"</span>, ]</span>
<span id="cb140-673"><a href="#cb140-673" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-674"><a href="#cb140-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-675"><a href="#cb140-675" aria-hidden="true" tabindex="-1"></a>It is also possible to subset the design, e.g., to exclude a specific task-learner combination by manually removing a certain row from the design which is a <span class="in">`data.table`</span>.</span>
<span id="cb140-676"><a href="#cb140-676" aria-hidden="true" tabindex="-1"></a>Alternatively, we can also construct a custom benchmark design by manually defining a <span class="in">`data.table`</span> containing task, learner, and resampling objects (see also the examples section in the help page of <span class="in">`r ref("benchmark_grid()")`</span>).</span>
<span id="cb140-677"><a href="#cb140-677" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-678"><a href="#cb140-678" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- :::{.callout-tip} --&gt;</span></span>
<span id="cb140-679"><a href="#cb140-679" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Note that if you construct a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design. --&gt;</span></span>
<span id="cb140-680"><a href="#cb140-680" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ::: --&gt;</span></span>
<span id="cb140-681"><a href="#cb140-681" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-682"><a href="#cb140-682" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r performance-021} --&gt;</span></span>
<span id="cb140-683"><a href="#cb140-683" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- #| echo: false --&gt;</span></span>
<span id="cb140-684"><a href="#cb140-684" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # Creating a grid using a cross join --&gt;</span></span>
<span id="cb140-685"><a href="#cb140-685" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- design_manual = data.table::CJ( --&gt;</span></span>
<span id="cb140-686"><a href="#cb140-686" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   task = tsks(c("german_credit", "sonar")), --&gt;</span></span>
<span id="cb140-687"><a href="#cb140-687" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"), --&gt;</span></span>
<span id="cb140-688"><a href="#cb140-688" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--                  predict_type = "prob", predict_sets = c("train", "test")), --&gt;</span></span>
<span id="cb140-689"><a href="#cb140-689" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   resampling = rsmps("cv", folds = 3), --&gt;</span></span>
<span id="cb140-690"><a href="#cb140-690" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   sorted = FALSE --&gt;</span></span>
<span id="cb140-691"><a href="#cb140-691" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ) --&gt;</span></span>
<span id="cb140-692"><a href="#cb140-692" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-693"><a href="#cb140-693" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # Manually remove e.g. the third combination from the grid --&gt;</span></span>
<span id="cb140-694"><a href="#cb140-694" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- design_manual = design_manual[-3] --&gt;</span></span>
<span id="cb140-695"><a href="#cb140-695" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-696"><a href="#cb140-696" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # Manually instantiate the resamplings --&gt;</span></span>
<span id="cb140-697"><a href="#cb140-697" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Map(function(task, resampling) { --&gt;</span></span>
<span id="cb140-698"><a href="#cb140-698" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   resampling$instantiate(task) --&gt;</span></span>
<span id="cb140-699"><a href="#cb140-699" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- }, task = design_manual$task, resampling = design_manual$resampling) --&gt;</span></span>
<span id="cb140-700"><a href="#cb140-700" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb140-701"><a href="#cb140-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-702"><a href="#cb140-702" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-703"><a href="#cb140-703" aria-hidden="true" tabindex="-1"></a><span class="fu">### Execution of Benchmark Experiments {#sec-bm-exec}</span></span>
<span id="cb140-704"><a href="#cb140-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-705"><a href="#cb140-705" aria-hidden="true" tabindex="-1"></a>To run the benchmark experiment, we can pass the constructed benchmark design to the <span class="in">`r ref("benchmark()")`</span> function, which will internally call <span class="in">`r ref("resample()")`</span> for all the combinations of task, learner, and resampling strategy in our benchmark design:</span>
<span id="cb140-706"><a href="#cb140-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-707"><a href="#cb140-707" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-039}</span></span>
<span id="cb140-708"><a href="#cb140-708" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb140-709"><a href="#cb140-709" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(bmr)</span>
<span id="cb140-710"><a href="#cb140-710" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-711"><a href="#cb140-711" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-712"><a href="#cb140-712" aria-hidden="true" tabindex="-1"></a>Once the benchmarking is finished (this can take some time, depending on the size of your design), we can aggregate the performance results with the <span class="in">`$aggregate()`</span> method of the returned <span class="in">`r ref("BenchmarkResult")`</span>:</span>
<span id="cb140-713"><a href="#cb140-713" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-714"><a href="#cb140-714" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-040}</span></span>
<span id="cb140-715"><a href="#cb140-715" aria-hidden="true" tabindex="-1"></a>acc <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-716"><a href="#cb140-716" aria-hidden="true" tabindex="-1"></a>acc[, .(task_id, learner_id, classif.acc)]</span>
<span id="cb140-717"><a href="#cb140-717" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-718"><a href="#cb140-718" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-719"><a href="#cb140-719" aria-hidden="true" tabindex="-1"></a>As the results are shown in a <span class="in">`r ref("data.table")`</span>, we can easily aggregate the results even further.</span>
<span id="cb140-720"><a href="#cb140-720" aria-hidden="true" tabindex="-1"></a>For example, if we are interested in the learner that performed best across all tasks, we could average the performance of each individual learner across all tasks.</span>
<span id="cb140-721"><a href="#cb140-721" aria-hidden="true" tabindex="-1"></a>Please note that averaging accuracy scores across multiple tasks as in this example is not always appropriate for comparison purposes.</span>
<span id="cb140-722"><a href="#cb140-722" aria-hidden="true" tabindex="-1"></a>A more common alternative to compare the overall algorithm performance across multiple tasks is to first compute the ranks of each learner on each task separately and then compute the average ranks.</span>
<span id="cb140-723"><a href="#cb140-723" aria-hidden="true" tabindex="-1"></a>For illustration purposes, we show how to average the performance of each individual learner across all tasks:</span>
<span id="cb140-724"><a href="#cb140-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-725"><a href="#cb140-725" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-041}</span></span>
<span id="cb140-726"><a href="#cb140-726" aria-hidden="true" tabindex="-1"></a>acc[, <span class="fu">list</span>(<span class="at">mean_accuracy =</span> <span class="fu">mean</span>(classif.acc)), by <span class="ot">=</span> <span class="st">"learner_id"</span>]</span>
<span id="cb140-727"><a href="#cb140-727" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-728"><a href="#cb140-728" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-729"><a href="#cb140-729" aria-hidden="true" tabindex="-1"></a>Ranking the performance scores can either be done via standard <span class="in">`r ref("data.table")`</span> syntax, or more conveniently with the <span class="in">`r ref("mlr3benchmark")`</span> package.</span>
<span id="cb140-730"><a href="#cb140-730" aria-hidden="true" tabindex="-1"></a>We first use <span class="in">`r ref("as.BenchmarkAggr")`</span> to aggregate the <span class="in">`r ref("BenchmarkResult")`</span> using our measure, after which we use the <span class="in">`$rank_data()`</span> method to convert the performance scores to ranks.</span>
<span id="cb140-731"><a href="#cb140-731" aria-hidden="true" tabindex="-1"></a>The <span class="in">`minimize`</span> argument is used to indicate that the classification accuracy should not be minimized, i.e. a higher score is better.</span>
<span id="cb140-732"><a href="#cb140-732" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-733"><a href="#cb140-733" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-042}</span></span>
<span id="cb140-734"><a href="#cb140-734" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">"mlr3benchmark"</span>)</span>
<span id="cb140-735"><a href="#cb140-735" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-736"><a href="#cb140-736" aria-hidden="true" tabindex="-1"></a>bma <span class="ot">=</span> <span class="fu">as.BenchmarkAggr</span>(bmr, <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-737"><a href="#cb140-737" aria-hidden="true" tabindex="-1"></a>bma<span class="sc">$</span><span class="fu">rank_data</span>(<span class="at">minimize =</span> <span class="cn">FALSE</span>)</span>
<span id="cb140-738"><a href="#cb140-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-739"><a href="#cb140-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-740"><a href="#cb140-740" aria-hidden="true" tabindex="-1"></a>This results in per-task rankings of the three learners.</span>
<span id="cb140-741"><a href="#cb140-741" aria-hidden="true" tabindex="-1"></a>Unsurprisingly, the featureless learner ranks last, as it always predicts the majority class.</span>
<span id="cb140-742"><a href="#cb140-742" aria-hidden="true" tabindex="-1"></a>However, it is common practice to include it as a baseline in benchmarking experiments to easily gauge the relative performance of other algorithms.</span>
<span id="cb140-743"><a href="#cb140-743" aria-hidden="true" tabindex="-1"></a>In this simple benchmark experiment, the random forest ranked first, outperforming a single classification tree as one would expect.</span>
<span id="cb140-744"><a href="#cb140-744" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-745"><a href="#cb140-745" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-746"><a href="#cb140-746" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- We construct two measures to calculate the area under the curve (AUC) for the training and the test set: --&gt;</span></span>
<span id="cb140-747"><a href="#cb140-747" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-748"><a href="#cb140-748" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r performance-023} --&gt;</span></span>
<span id="cb140-749"><a href="#cb140-749" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- measures = list( --&gt;</span></span>
<span id="cb140-750"><a href="#cb140-750" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   msr("classif.auc", predict_sets = "train", id = "auc_train"), --&gt;</span></span>
<span id="cb140-751"><a href="#cb140-751" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!--   msr("classif.auc", id = "auc_test") --&gt;</span></span>
<span id="cb140-752"><a href="#cb140-752" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ) --&gt;</span></span>
<span id="cb140-753"><a href="#cb140-753" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-754"><a href="#cb140-754" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- tab = bmr$aggregate(measures) --&gt;</span></span>
<span id="cb140-755"><a href="#cb140-755" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- print(tab[, .(task_id, learner_id, auc_train, auc_test)]) --&gt;</span></span>
<span id="cb140-756"><a href="#cb140-756" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb140-757"><a href="#cb140-757" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-758"><a href="#cb140-758" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Simply aggregating the performances with the mean is usually not statistically sound. --&gt;</span></span>
<span id="cb140-759"><a href="#cb140-759" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Instead, we calculate the rank statistic for each learner, grouped by task. --&gt;</span></span>
<span id="cb140-760"><a href="#cb140-760" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Then the calculated ranks, grouped by the learner, are aggregated with the `r ref_pkg("data.table")` package. --&gt;</span></span>
<span id="cb140-761"><a href="#cb140-761" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$. --&gt;</span></span>
<span id="cb140-762"><a href="#cb140-762" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-763"><a href="#cb140-763" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ```{r performance-024} --&gt;</span></span>
<span id="cb140-764"><a href="#cb140-764" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- library("data.table") --&gt;</span></span>
<span id="cb140-765"><a href="#cb140-765" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # group by levels of task_id, return columns: --&gt;</span></span>
<span id="cb140-766"><a href="#cb140-766" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # - learner_id --&gt;</span></span>
<span id="cb140-767"><a href="#cb140-767" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # - rank of col '-auc_train' (per level of learner_id) --&gt;</span></span>
<span id="cb140-768"><a href="#cb140-768" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # - rank of col '-auc_test' (per level of learner_id) --&gt;</span></span>
<span id="cb140-769"><a href="#cb140-769" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id] --&gt;</span></span>
<span id="cb140-770"><a href="#cb140-770" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- print(ranks) --&gt;</span></span>
<span id="cb140-771"><a href="#cb140-771" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-772"><a href="#cb140-772" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # group by levels of learner_id, return columns: --&gt;</span></span>
<span id="cb140-773"><a href="#cb140-773" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # - mean rank of col 'rank_train' (per level of learner_id) --&gt;</span></span>
<span id="cb140-774"><a href="#cb140-774" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # - mean rank of col 'rank_test' (per level of learner_id) --&gt;</span></span>
<span id="cb140-775"><a href="#cb140-775" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id] --&gt;</span></span>
<span id="cb140-776"><a href="#cb140-776" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-777"><a href="#cb140-777" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- # print the final table, ordered by mean rank of AUC test --&gt;</span></span>
<span id="cb140-778"><a href="#cb140-778" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ranks[order(mrank_test)] --&gt;</span></span>
<span id="cb140-779"><a href="#cb140-779" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ``` --&gt;</span></span>
<span id="cb140-780"><a href="#cb140-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-781"><a href="#cb140-781" aria-hidden="true" tabindex="-1"></a><span class="fu">### Inspect BenchmarkResult Objects {#sec-bm-resamp}</span></span>
<span id="cb140-782"><a href="#cb140-782" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-783"><a href="#cb140-783" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- {{&lt; include _optional.qmd &gt;}} --&gt;</span></span>
<span id="cb140-784"><a href="#cb140-784" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-785"><a href="#cb140-785" aria-hidden="true" tabindex="-1"></a>A <span class="in">`r ref("BenchmarkResult")`</span> object is a collection of multiple <span class="in">`r ref("ResampleResult")`</span> objects.</span>
<span id="cb140-786"><a href="#cb140-786" aria-hidden="true" tabindex="-1"></a>We can analogously use <span class="in">`r ref("as.data.table")`</span> to take a look at the contents and compare them to the <span class="in">`r ref("data.table")`</span> of the <span class="in">`r ref("ResampleResult")`</span> from the previous section (<span class="in">`rrdt`</span>):</span>
<span id="cb140-787"><a href="#cb140-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-788"><a href="#cb140-788" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-043}</span></span>
<span id="cb140-789"><a href="#cb140-789" aria-hidden="true" tabindex="-1"></a>bmrdt <span class="ot">=</span> <span class="fu">as.data.table</span>(bmr)</span>
<span id="cb140-790"><a href="#cb140-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-791"><a href="#cb140-791" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(bmrdt)</span>
<span id="cb140-792"><a href="#cb140-792" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(rrdt)</span>
<span id="cb140-793"><a href="#cb140-793" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-794"><a href="#cb140-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-795"><a href="#cb140-795" aria-hidden="true" tabindex="-1"></a>By the column names alone, we see that the general contents of a <span class="in">`r ref("BenchmarkResult")`</span> and <span class="in">`r ref("ResampleResult")`</span> which we specified in @sec-resampling-inspect is very similar, with the additional unique identification column <span class="in">`"uhash"`</span> in the former being the only difference.</span>
<span id="cb140-796"><a href="#cb140-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-797"><a href="#cb140-797" aria-hidden="true" tabindex="-1"></a>The stored <span class="in">`r ref("ResampleResult")`</span>s can be extracted via the <span class="in">`$resample_result(i)`</span> method, where <span class="in">`i`</span> is the index of the performed benchmark experiment.</span>
<span id="cb140-798"><a href="#cb140-798" aria-hidden="true" tabindex="-1"></a>This allows us to investigate the extracted <span class="in">`r ref("ResampleResult")`</span> or individual resampling iterations as shown previously (see @sec-resampling).</span>
<span id="cb140-799"><a href="#cb140-799" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-800"><a href="#cb140-800" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-044}</span></span>
<span id="cb140-801"><a href="#cb140-801" aria-hidden="true" tabindex="-1"></a>rr1 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">1</span>)</span>
<span id="cb140-802"><a href="#cb140-802" aria-hidden="true" tabindex="-1"></a>rr2 <span class="ot">=</span> bmr<span class="sc">$</span><span class="fu">resample_result</span>(<span class="dv">2</span>)</span>
<span id="cb140-803"><a href="#cb140-803" aria-hidden="true" tabindex="-1"></a>rr1</span>
<span id="cb140-804"><a href="#cb140-804" aria-hidden="true" tabindex="-1"></a>rr2</span>
<span id="cb140-805"><a href="#cb140-805" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-806"><a href="#cb140-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-807"><a href="#cb140-807" aria-hidden="true" tabindex="-1"></a>Multiple <span class="in">`r ref("ResampleResult")`</span> can be again converted to a <span class="in">`r ref("BenchmarkResult")`</span> with the function <span class="in">`r ref("as_benchmark_result()")`</span> and combined with <span class="in">`c()`</span>:</span>
<span id="cb140-808"><a href="#cb140-808" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-809"><a href="#cb140-809" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-045}</span></span>
<span id="cb140-810"><a href="#cb140-810" aria-hidden="true" tabindex="-1"></a>bmr1 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr1)</span>
<span id="cb140-811"><a href="#cb140-811" aria-hidden="true" tabindex="-1"></a>bmr2 <span class="ot">=</span> <span class="fu">as_benchmark_result</span>(rr2)</span>
<span id="cb140-812"><a href="#cb140-812" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-813"><a href="#cb140-813" aria-hidden="true" tabindex="-1"></a>bmr_combined <span class="ot">=</span> <span class="fu">c</span>(bmr1, bmr2)</span>
<span id="cb140-814"><a href="#cb140-814" aria-hidden="true" tabindex="-1"></a>bmr_combined<span class="sc">$</span><span class="fu">aggregate</span>(<span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-815"><a href="#cb140-815" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-816"><a href="#cb140-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-817"><a href="#cb140-817" aria-hidden="true" tabindex="-1"></a>Combining multiple <span class="in">`r ref("BenchmarkResult")`</span>s into a larger result object can be useful if related benchmarks where computed on different machines.</span>
<span id="cb140-818"><a href="#cb140-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-819"><a href="#cb140-819" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Note: Removing ROC stuff from this section made it too small to justify a heading I think.</span></span>
<span id="cb140-820"><a href="#cb140-820" aria-hidden="true" tabindex="-1"></a><span class="al">###</span><span class="co"> Plotting Benchmark Results {#autoplot-benchmarkresult}</span></span>
<span id="cb140-821"><a href="#cb140-821" aria-hidden="true" tabindex="-1"></a><span class="co">--&gt;</span></span>
<span id="cb140-822"><a href="#cb140-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-823"><a href="#cb140-823" aria-hidden="true" tabindex="-1"></a>Similar to creating automated visualizations for tasks, <span class="co">[</span><span class="ot">predictions</span><span class="co">](#autoplot-prediction)</span>, or <span class="co">[</span><span class="ot">resample results</span><span class="co">](#autoplot-resampleresult)</span>, the <span class="in">`r mlr3viz`</span> package also provides a <span class="in">`r ref("autoplot()")`</span> method to visualize benchmark results, by default as a boxplot:</span>
<span id="cb140-824"><a href="#cb140-824" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-825"><a href="#cb140-825" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-046}</span></span>
<span id="cb140-826"><a href="#cb140-826" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-height: 8</span></span>
<span id="cb140-827"><a href="#cb140-827" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-width: 6</span></span>
<span id="cb140-828"><a href="#cb140-828" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-829"><a href="#cb140-829" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-830"><a href="#cb140-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-831"><a href="#cb140-831" aria-hidden="true" tabindex="-1"></a>Such a plot summarizes the benchmark experiment across all tasks and learners.</span>
<span id="cb140-832"><a href="#cb140-832" aria-hidden="true" tabindex="-1"></a>Visualizing performance scores across all learners and tasks in a benchmark helps identifying potentially unexpected behavior, such as a learner performing reasonably well for most tasks, but yielding noticeably worse scores in one task.</span>
<span id="cb140-833"><a href="#cb140-833" aria-hidden="true" tabindex="-1"></a>In the case of our example above, the three learners show consistent relative performance to each other, in the order we would expect.</span>
<span id="cb140-834"><a href="#cb140-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-835"><a href="#cb140-835" aria-hidden="true" tabindex="-1"></a><span class="fu">### Statistical Tests</span></span>
<span id="cb140-836"><a href="#cb140-836" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-837"><a href="#cb140-837" aria-hidden="true" tabindex="-1"></a>{{&lt; include _optional.qmd &gt;}}</span>
<span id="cb140-838"><a href="#cb140-838" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-839"><a href="#cb140-839" aria-hidden="true" tabindex="-1"></a>The package <span class="in">`r ref("mlr3benchmark")`</span> we previously used for ranking also provides infrastructure for applying statistical significance tests on <span class="in">`r ref("BenchmarkResult")`</span> objects.</span>
<span id="cb140-840"><a href="#cb140-840" aria-hidden="true" tabindex="-1"></a>Currently, Friedman tests and pairwise Friedman-Nemenyi tests <span class="co">[</span><span class="ot">@demsar2006</span><span class="co">]</span> are supported to analyze benchmark experiments with at least two independent tasks and at least two learners.</span>
<span id="cb140-841"><a href="#cb140-841" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-842"><a href="#cb140-842" aria-hidden="true" tabindex="-1"></a><span class="in">`$friedman_posthoc()`</span> can be used for a pairwise comparison:</span>
<span id="cb140-843"><a href="#cb140-843" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-844"><a href="#cb140-844" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-047}</span></span>
<span id="cb140-845"><a href="#cb140-845" aria-hidden="true" tabindex="-1"></a>bma <span class="ot">=</span> <span class="fu">as.BenchmarkAggr</span>(bmr, <span class="at">measures =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-846"><a href="#cb140-846" aria-hidden="true" tabindex="-1"></a>bma<span class="sc">$</span><span class="fu">friedman_posthoc</span>()</span>
<span id="cb140-847"><a href="#cb140-847" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-848"><a href="#cb140-848" aria-hidden="true" tabindex="-1"></a>These results would indicate a statistically significant difference between the <span class="in">`"featureless"`</span> learner and <span class="in">`"ranger"`</span>, assuming a 95% confidence level.</span>
<span id="cb140-849"><a href="#cb140-849" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-850"><a href="#cb140-850" aria-hidden="true" tabindex="-1"></a>The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:</span>
<span id="cb140-851"><a href="#cb140-851" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-852"><a href="#cb140-852" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-048}</span></span>
<span id="cb140-853"><a href="#cb140-853" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bma, <span class="at">type =</span> <span class="st">"cd"</span>)</span>
<span id="cb140-854"><a href="#cb140-854" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-855"><a href="#cb140-855" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-856"><a href="#cb140-856" aria-hidden="true" tabindex="-1"></a>Similar to the test output before, this visualization leads to the conclusion that the <span class="in">`"featureless"`</span> learner and <span class="in">`"ranger"`</span> are significantly different, whereas the critical rank difference of 1.66 is not exceed for the comparison of the <span class="in">`"featureless"`</span> learner, <span class="in">`"rpart"`</span> and <span class="in">`"ranger"`</span>, respectively.</span>
<span id="cb140-857"><a href="#cb140-857" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-858"><a href="#cb140-858" aria-hidden="true" tabindex="-1"></a><span class="fu">## ROC Analysis {#sec-roc}</span></span>
<span id="cb140-859"><a href="#cb140-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-860"><a href="#cb140-860" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- So far we have focused on methods applicable to general classification and regression tasks. --&gt;</span></span>
<span id="cb140-861"><a href="#cb140-861" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- For the special case of binary classification, there are specialized performance measures and methods to analyze and compare the performance. --&gt;</span></span>
<span id="cb140-862"><a href="#cb140-862" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Binary classification is unique because of the presence of a positive and negative class and a threshold probability to distinguish between the two. --&gt;</span></span>
<span id="cb140-863"><a href="#cb140-863" aria-hidden="true" tabindex="-1"></a><span class="in">`r index("ROC")`</span> (Receiver Operating Characteristic) analysis is widely used to evaluate binary classifiers.</span>
<span id="cb140-864"><a href="#cb140-864" aria-hidden="true" tabindex="-1"></a>Although extensions for multiclass classifiers exist (see e.g., @hand2001simple), we will only cover the much easier binary classification case here.</span>
<span id="cb140-865"><a href="#cb140-865" aria-hidden="true" tabindex="-1"></a>For binary classifiers that predict discrete classes, we can compute a <span class="in">`r index("confusion matrix")`</span> which computes the following quantities (see also @fig-confusion):</span>
<span id="cb140-866"><a href="#cb140-866" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-867"><a href="#cb140-867" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True positives (TP)**: Instances that are actually positive and correctly classified as positive.</span>
<span id="cb140-868"><a href="#cb140-868" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True negatives (TN)**: Instances that are actually negative and correctly classified as negative.</span>
<span id="cb140-869"><a href="#cb140-869" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False positives (FP)**: Instances that are actually negative but incorrectly classified as positive.</span>
<span id="cb140-870"><a href="#cb140-870" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False negatives (FN)**: Instances that are actually positive but incorrectly classified as negative.</span>
<span id="cb140-871"><a href="#cb140-871" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-872"><a href="#cb140-872" aria-hidden="true" tabindex="-1"></a>There are a multitude of performance measures that can be derived from a confusion matrix.</span>
<span id="cb140-873"><a href="#cb140-873" aria-hidden="true" tabindex="-1"></a>Unfortunately, many of them have different names for historical reasons, originating from different fields.</span>
<span id="cb140-874"><a href="#cb140-874" aria-hidden="true" tabindex="-1"></a>For a good overview of common confusion matrix-based measures, see the comprehensive table on <span class="in">`r link("https://en.wikipedia.org/wiki/Confusion_matrix#Table_of_confusion", "Wikipedia")`</span> which also provides many common aliases for each measure.</span>
<span id="cb140-875"><a href="#cb140-875" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-876"><a href="#cb140-876" aria-hidden="true" tabindex="-1"></a><span class="fu">### Confusion Matrix-based Measures</span></span>
<span id="cb140-877"><a href="#cb140-877" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-878"><a href="#cb140-878" aria-hidden="true" tabindex="-1"></a>Some common performance measures that are based on the confusion matrix and measure the ability of a classifier to separate the two classes (i.e., discrimination performance) include (see also @fig-confusion for their definition based on TP, FP, TN and FN):</span>
<span id="cb140-879"><a href="#cb140-879" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-880"><a href="#cb140-880" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True Positive Rate (TPR)**, **Sensitivity** or **Recall**: How many of the true positives did we predict as positive?</span>
<span id="cb140-881"><a href="#cb140-881" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**True Negative Rate (TNR)** or **Specificity**: How many of the true negatives did we predict as negative?</span>
<span id="cb140-882"><a href="#cb140-882" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**False Positive Rate (FPR)**, or 1 - **Specificity**: How many of the true negatives did we predict as positive?</span>
<span id="cb140-883"><a href="#cb140-883" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Positive Predictive Value (PPV)** or **Precision**: If we predict positive how likely is it a true positive?</span>
<span id="cb140-884"><a href="#cb140-884" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Negative Predictive Value (NPV)**: If we predict negative how likely is it a true negative?</span>
<span id="cb140-885"><a href="#cb140-885" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**Accuracy (ACC)**: The proportion of correctly classified instances out of the total number of instances.</span>
<span id="cb140-886"><a href="#cb140-886" aria-hidden="true" tabindex="-1"></a><span class="ss">* </span>**F1-score**: The harmonic mean of precision and recall, which balances the trade-off between precision and recall. It is calculated as $2 \times \frac{Precision \times Recall}{Precision + Recall}$.</span>
<span id="cb140-887"><a href="#cb140-887" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-888"><a href="#cb140-888" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-049}</span></span>
<span id="cb140-889"><a href="#cb140-889" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb140-890"><a href="#cb140-890" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-confusion</span></span>
<span id="cb140-891"><a href="#cb140-891" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Binary confusion matrix of ground truth class vs. predicted class."</span></span>
<span id="cb140-892"><a href="#cb140-892" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-893"><a href="#cb140-893" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Binary confusion matrix of ground truth class vs. predicted class."</span></span>
<span id="cb140-894"><a href="#cb140-894" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">"Figures/confusion_matrix.svg"</span>)</span>
<span id="cb140-895"><a href="#cb140-895" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-896"><a href="#cb140-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-897"><a href="#cb140-897" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-898"><a href="#cb140-898" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- |       |     | True                                              | Class                                                   |                                                          | --&gt;</span></span>
<span id="cb140-899"><a href="#cb140-899" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- |:-----:|:---:|:------------------------------------------------------:|:------------------------------------------------------:|:--------------------------------------------------------:| --&gt;</span></span>
<span id="cb140-900"><a href="#cb140-900" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- |       |     | $+$                                                    | $-$                                                    |                                                          | --&gt;</span></span>
<span id="cb140-901"><a href="#cb140-901" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- | Pred. | $+$ | TP                                                     | FP                                                     | $\text{PPV} = \frac{ \text{TP}}{\text{TP} + \text{FP}}$    | --&gt;</span></span>
<span id="cb140-902"><a href="#cb140-902" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- | $\yh$ | $-$ | FN                                                     | TN                                                     | $\text{NPV} = \frac{\text{TN}}{\text{FN} + \text{TN}}$   | --&gt;</span></span>
<span id="cb140-903"><a href="#cb140-903" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- |       |     | $\text{TPR} = \frac{\text{TP}}{\text{TP} + \text{FN}}$ | $\text{TNR} = \frac{\text{TN}}{\text{FP} + \text{TN}}$ | $\text{ACC} = \frac{\text{TP}+ \text{TN}}{\text{TP+FP+FN+TN}}$ | --&gt;</span></span>
<span id="cb140-904"><a href="#cb140-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-905"><a href="#cb140-905" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-906"><a href="#cb140-906" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- In Chapter @sec-basics, we have already seen how we can obtain the confusion matrix of a `r ref("Prediction")` by accessing the `$confusion` field. --&gt;</span></span>
<span id="cb140-907"><a href="#cb140-907" aria-hidden="true" tabindex="-1"></a>In the code example below, we first retrieve the <span class="in">`r ref("mlr_tasks_german_credit")`</span> task which is a binary classification task and construct a random forest learner using <span class="in">`classif.ranger`</span> that predicts probabilities using the <span class="in">`predict_type = "prob"`</span> option.</span>
<span id="cb140-908"><a href="#cb140-908" aria-hidden="true" tabindex="-1"></a>Next, we use the <span class="in">`r ref("partition()")`</span> helper function which acts as a convenience shortcut function to the <span class="in">`"holdout"`</span> resampling strategy to randomly partition the contained data into two disjoint set.</span>
<span id="cb140-909"><a href="#cb140-909" aria-hidden="true" tabindex="-1"></a>We train the learner on the training set and use the trained model to generate predictions on the test set.</span>
<span id="cb140-910"><a href="#cb140-910" aria-hidden="true" tabindex="-1"></a>Finally, we retrieve the confusion matrix from the resulting <span class="in">`r ref("Prediction")`</span> object by accessing the <span class="in">`$confusion`</span> field (see also @sec-classif-eval):</span>
<span id="cb140-911"><a href="#cb140-911" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-912"><a href="#cb140-912" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-050}</span></span>
<span id="cb140-913"><a href="#cb140-913" aria-hidden="true" tabindex="-1"></a>task <span class="ot">=</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>)</span>
<span id="cb140-914"><a href="#cb140-914" aria-hidden="true" tabindex="-1"></a>learner <span class="ot">=</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>)</span>
<span id="cb140-915"><a href="#cb140-915" aria-hidden="true" tabindex="-1"></a>splits <span class="ot">=</span> <span class="fu">partition</span>(task, <span class="at">ratio =</span> <span class="fl">0.8</span>)</span>
<span id="cb140-916"><a href="#cb140-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-917"><a href="#cb140-917" aria-hidden="true" tabindex="-1"></a>learner<span class="sc">$</span><span class="fu">train</span>(task, splits<span class="sc">$</span>train)</span>
<span id="cb140-918"><a href="#cb140-918" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">=</span> learner<span class="sc">$</span><span class="fu">predict</span>(task, splits<span class="sc">$</span>test)</span>
<span id="cb140-919"><a href="#cb140-919" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span>confusion</span>
<span id="cb140-920"><a href="#cb140-920" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-921"><a href="#cb140-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-922"><a href="#cb140-922" aria-hidden="true" tabindex="-1"></a>The <span class="in">`r ref_pkg("mlr3measures")`</span> package allows to additionally compute several common confusion matrix-based measures using the <span class="in">`r ref("mlr3measures::confusion_matrix", "confusion_matrix")`</span> function:</span>
<span id="cb140-923"><a href="#cb140-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-924"><a href="#cb140-924" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-051}</span></span>
<span id="cb140-925"><a href="#cb140-925" aria-hidden="true" tabindex="-1"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(<span class="at">truth =</span> pred<span class="sc">$</span>truth,</span>
<span id="cb140-926"><a href="#cb140-926" aria-hidden="true" tabindex="-1"></a>  <span class="at">response =</span> pred<span class="sc">$</span>response, <span class="at">positive =</span> task<span class="sc">$</span>positive)</span>
<span id="cb140-927"><a href="#cb140-927" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-928"><a href="#cb140-928" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-929"><a href="#cb140-929" aria-hidden="true" tabindex="-1"></a>If a binary classifier predicts probabilities instead of discrete classes, we could arbitrarily set a threshold to cut-off the probabilities and assign them to the positive and negative class.</span>
<span id="cb140-930"><a href="#cb140-930" aria-hidden="true" tabindex="-1"></a>When it comes to classification performance, it is generally difficult to achieve a high TPR and low FPR simultaneously because there is often a trade-off between the two rates.</span>
<span id="cb140-931"><a href="#cb140-931" aria-hidden="true" tabindex="-1"></a>Increasing the threshold for identifying the positive cases, leads to a higher number of negative predictions and fewer positive predictions.</span>
<span id="cb140-932"><a href="#cb140-932" aria-hidden="true" tabindex="-1"></a>As a consequence, the FPR is usually better (lower), but at the cost of a worse (lower) TPR.</span>
<span id="cb140-933"><a href="#cb140-933" aria-hidden="true" tabindex="-1"></a>For example, in the special case where the threshold is set too high and no instance is predicted as positive, the confusion matrix shows zero true positives (no instances that are actually positive and correctly classified as positive) and zero false positives (no instances that are actually negative but incorrectly classified as positive).</span>
<span id="cb140-934"><a href="#cb140-934" aria-hidden="true" tabindex="-1"></a>Therefore, the FPR and TPR are also zero since there are zero false positives and zero true positives.</span>
<span id="cb140-935"><a href="#cb140-935" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- However, the TPR is also zero since there are no true positives, which means that the model fails to identify any positive cases, even if there are positive cases in the dataset.  --&gt;</span></span>
<span id="cb140-936"><a href="#cb140-936" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Consider a binary classifier that predicts whether an email is spam (positive class) or not (negative class). --&gt;</span></span>
<span id="cb140-937"><a href="#cb140-937" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- Increasing the threshold for identifying positive cases means that the model will require more evidence (i.e., a higher predicted probability) before classifying an email as spam.  --&gt;</span></span>
<span id="cb140-938"><a href="#cb140-938" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- This can result in fewer emails being classified as spam (fewer positive predictions) and more emails being classified as not spam (more negative predictions). --&gt;</span></span>
<span id="cb140-939"><a href="#cb140-939" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- This can increase the model's specificity (the proportion of true negatives among all negative predictions) which leads to a better (lower) FPR. --&gt;</span></span>
<span id="cb140-940"><a href="#cb140-940" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- However, it may also reduce the model's sensitivity and TPR (i.e., the proportion of true positives among all positive predictions). --&gt;</span></span>
<span id="cb140-941"><a href="#cb140-941" aria-hidden="true" tabindex="-1"></a>Conversely, lowering the threshold for identifying positive cases may never predict the negative class and can increase (improve) TPR, but at the cost of a worse (higher) FPR.</span>
<span id="cb140-942"><a href="#cb140-942" aria-hidden="true" tabindex="-1"></a>For example, below we set the threshold to <span class="in">`0.99`</span> and <span class="in">`0.01`</span> for the <span class="in">`r ref("mlr_tasks_german_credit")`</span> task to illustrate the two special cases explained above where zero positives and where zero negatives are predicted and inspect the resulting confusion matrix-based measures (some measures can not be computed due to division by 0 and therefore will produce <span class="in">`NaN`</span> values):</span>
<span id="cb140-943"><a href="#cb140-943" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-944"><a href="#cb140-944" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-052}</span></span>
<span id="cb140-945"><a href="#cb140-945" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="fl">0.99</span>)</span>
<span id="cb140-946"><a href="#cb140-946" aria-hidden="true" tabindex="-1"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(pred<span class="sc">$</span>truth, pred<span class="sc">$</span>response, task<span class="sc">$</span>positive)</span>
<span id="cb140-947"><a href="#cb140-947" aria-hidden="true" tabindex="-1"></a>pred<span class="sc">$</span><span class="fu">set_threshold</span>(<span class="fl">0.01</span>)</span>
<span id="cb140-948"><a href="#cb140-948" aria-hidden="true" tabindex="-1"></a>mlr3measures<span class="sc">::</span><span class="fu">confusion_matrix</span>(pred<span class="sc">$</span>truth, pred<span class="sc">$</span>response, task<span class="sc">$</span>positive)</span>
<span id="cb140-949"><a href="#cb140-949" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-950"><a href="#cb140-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-951"><a href="#cb140-951" aria-hidden="true" tabindex="-1"></a><span class="fu">### ROC Space</span></span>
<span id="cb140-952"><a href="#cb140-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-953"><a href="#cb140-953" aria-hidden="true" tabindex="-1"></a>ROC analysis aims at evaluating the performance of classifiers by visualizing the trade-off between the TPR and the FPR which can be obtained from a confusion matrix.</span>
<span id="cb140-954"><a href="#cb140-954" aria-hidden="true" tabindex="-1"></a>Each classifier that predicts discrete classes, will be a single point in the ROC space (see @fig-roc, panel (a)).</span>
<span id="cb140-955"><a href="#cb140-955" aria-hidden="true" tabindex="-1"></a>The best classifier lies on the top-left corner where the TPR is 1 and the FPR is 0.</span>
<span id="cb140-956"><a href="#cb140-956" aria-hidden="true" tabindex="-1"></a>Classifiers on the diagonal predict class labels randomly (possibly with different class proportions).</span>
<span id="cb140-957"><a href="#cb140-957" aria-hidden="true" tabindex="-1"></a>For example, if each positive instance will be randomly classified with 25% as to the positive class, we get a TPR of 0.25.</span>
<span id="cb140-958"><a href="#cb140-958" aria-hidden="true" tabindex="-1"></a>If we assign each negative instance randomly to the positive class, we get a FPR of 0.25.</span>
<span id="cb140-959"><a href="#cb140-959" aria-hidden="true" tabindex="-1"></a>In practice, we should never obtain a classifier clearly below the diagonal.</span>
<span id="cb140-960"><a href="#cb140-960" aria-hidden="true" tabindex="-1"></a>Swapping the predicted classes of a classifier would results in points in the ROC space being mirrored at the diagonal baseline.</span>
<span id="cb140-961"><a href="#cb140-961" aria-hidden="true" tabindex="-1"></a>A point in the ROC space below the diagonal might indicate that the positive and negative class labels have been switched by the classifier.</span>
<span id="cb140-962"><a href="#cb140-962" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-963"><a href="#cb140-963" aria-hidden="true" tabindex="-1"></a>Using different thresholds to cut-off predicted probabilities and assign them to the positive and negative class may lead to different confusion matrices.</span>
<span id="cb140-964"><a href="#cb140-964" aria-hidden="true" tabindex="-1"></a>In this case, we can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values --- this is the ROC curve.</span>
<span id="cb140-965"><a href="#cb140-965" aria-hidden="true" tabindex="-1"></a>For example, we can use the previous <span class="in">`r ref("Prediction")`</span> object, compute all possible TPR and FPR combinations if we use all predicted probabilities as possible threshold, and visualize them to manually create a ROC curve:</span>
<span id="cb140-966"><a href="#cb140-966" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-967"><a href="#cb140-967" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-053}</span></span>
<span id="cb140-968"><a href="#cb140-968" aria-hidden="true" tabindex="-1"></a>thresholds <span class="ot">=</span> <span class="fu">sort</span>(pred<span class="sc">$</span>prob[,<span class="dv">1</span>])</span>
<span id="cb140-969"><a href="#cb140-969" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-970"><a href="#cb140-970" aria-hidden="true" tabindex="-1"></a>rocvals <span class="ot">=</span> data.table<span class="sc">::</span><span class="fu">rbindlist</span>(<span class="fu">lapply</span>(thresholds, <span class="cf">function</span>(t) {</span>
<span id="cb140-971"><a href="#cb140-971" aria-hidden="true" tabindex="-1"></a>  pred<span class="sc">$</span><span class="fu">set_threshold</span>(t)</span>
<span id="cb140-972"><a href="#cb140-972" aria-hidden="true" tabindex="-1"></a>  <span class="fu">data.frame</span>(</span>
<span id="cb140-973"><a href="#cb140-973" aria-hidden="true" tabindex="-1"></a>    <span class="at">threshold =</span> t,</span>
<span id="cb140-974"><a href="#cb140-974" aria-hidden="true" tabindex="-1"></a>    <span class="at">FPR =</span> pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.fpr"</span>)),</span>
<span id="cb140-975"><a href="#cb140-975" aria-hidden="true" tabindex="-1"></a>    <span class="at">TPR =</span> pred<span class="sc">$</span><span class="fu">score</span>(<span class="fu">msr</span>(<span class="st">"classif.tpr"</span>))</span>
<span id="cb140-976"><a href="#cb140-976" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb140-977"><a href="#cb140-977" aria-hidden="true" tabindex="-1"></a>}))</span>
<span id="cb140-978"><a href="#cb140-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-979"><a href="#cb140-979" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(rocvals)</span>
<span id="cb140-980"><a href="#cb140-980" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-981"><a href="#cb140-981" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb140-982"><a href="#cb140-982" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(rocvals, <span class="fu">aes</span>(FPR, TPR)) <span class="sc">+</span></span>
<span id="cb140-983"><a href="#cb140-983" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb140-984"><a href="#cb140-984" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_path</span>(<span class="at">color =</span> <span class="st">"darkred"</span>) <span class="sc">+</span></span>
<span id="cb140-985"><a href="#cb140-985" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb140-986"><a href="#cb140-986" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb140-987"><a href="#cb140-987" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb140-988"><a href="#cb140-988" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Manually constructed ROC curve"</span>,</span>
<span id="cb140-989"><a href="#cb140-989" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"1 - Specificity (FPR)"</span>,</span>
<span id="cb140-990"><a href="#cb140-990" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Sensitivity (TPR)"</span></span>
<span id="cb140-991"><a href="#cb140-991" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb140-992"><a href="#cb140-992" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb140-993"><a href="#cb140-993" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-994"><a href="#cb140-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-995"><a href="#cb140-995" aria-hidden="true" tabindex="-1"></a>A natural performance measure that can be derived from the ROC curve is the area under the curve (AUC).</span>
<span id="cb140-996"><a href="#cb140-996" aria-hidden="true" tabindex="-1"></a>The higher the AUC value, the better the performance, whereas a random classifier would result in an AUC of 0.5  (see @fig-roc, panel (b) for an illustration).</span>
<span id="cb140-997"><a href="#cb140-997" aria-hidden="true" tabindex="-1"></a>The AUC can be interpreted as the probability that a randomly chosen positive instance is ranked higher (in the sense that it gets a higher predicted probability of belonging to the positive class) by the classification model than a randomly chosen negative instance.</span>
<span id="cb140-998"><a href="#cb140-998" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ROC curves for different labels are symmetric with respect to the diagonal, so  --&gt;</span></span>
<span id="cb140-999"><a href="#cb140-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1000"><a href="#cb140-1000" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-054, echo = FALSE}</span></span>
<span id="cb140-1001"><a href="#cb140-1001" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: fig-roc</span></span>
<span id="cb140-1002"><a href="#cb140-1002" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-cap: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."</span></span>
<span id="cb140-1003"><a href="#cb140-1003" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-align: "center"</span></span>
<span id="cb140-1004"><a href="#cb140-1004" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.height: 3.5</span></span>
<span id="cb140-1005"><a href="#cb140-1005" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig.width: 8</span></span>
<span id="cb140-1006"><a href="#cb140-1006" aria-hidden="true" tabindex="-1"></a><span class="co">#| fig-alt: "Panel (a): ROC space with best discrete classifier, two random guessing classifiers lying on the diagonal line (baseline), one that always predicts the positive class and one that never predicts the positive class, and three classifiers C1, C2, C3. We cannot say if C1 or C3 is better as both lie on a parallel line to the baseline. C2 is clearly dominated by C1, C3 as it is further away from the best classifier at (TPR = 1, FPR = 0). Panel (b): ROC curves of the best classifier (AUC = 1), of a random guessing classifier (AUC = 0.5), and the classifiers C1, C3, and C2."</span></span>
<span id="cb140-1007"><a href="#cb140-1007" aria-hidden="true" tabindex="-1"></a><span class="co">#library(gridExtra)</span></span>
<span id="cb140-1008"><a href="#cb140-1008" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb140-1009"><a href="#cb140-1009" aria-hidden="true" tabindex="-1"></a><span class="co"># devtools::install_github("thomasp85/patchwork")</span></span>
<span id="cb140-1010"><a href="#cb140-1010" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(patchwork)</span>
<span id="cb140-1011"><a href="#cb140-1011" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1012"><a href="#cb140-1012" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb140-1013"><a href="#cb140-1013" aria-hidden="true" tabindex="-1"></a>fun <span class="ot">=</span> <span class="cf">function</span>(x, lambda) <span class="dv">1</span> <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>lambda<span class="sc">*</span>x) <span class="co">#ecdf(rexp(1000000, rate = 5))</span></span>
<span id="cb140-1014"><a href="#cb140-1014" aria-hidden="true" tabindex="-1"></a>funinv <span class="ot">=</span> <span class="cf">function</span>(x, lambda) <span class="dv">1</span> <span class="sc">+</span> <span class="fu">log</span>(x)<span class="sc">/</span>lambda</span>
<span id="cb140-1015"><a href="#cb140-1015" aria-hidden="true" tabindex="-1"></a>x <span class="ot">=</span> <span class="fu">c</span>(<span class="fu">seq</span>(<span class="fl">2e-5</span>, <span class="dv">1</span>, <span class="at">length =</span> <span class="dv">1000</span>))</span>
<span id="cb140-1016"><a href="#cb140-1016" aria-hidden="true" tabindex="-1"></a>lambda1 <span class="ot">=</span>  <span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.75</span>)<span class="sc">/</span><span class="fl">0.125</span></span>
<span id="cb140-1017"><a href="#cb140-1017" aria-hidden="true" tabindex="-1"></a>lambda2 <span class="ot">=</span>  <span class="sc">-</span><span class="dv">1</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> <span class="fl">0.625</span>)<span class="sc">/</span><span class="fl">0.25</span></span>
<span id="cb140-1018"><a href="#cb140-1018" aria-hidden="true" tabindex="-1"></a><span class="co">#lambda3 =  -1*log(1 - 0.875)/0.25</span></span>
<span id="cb140-1019"><a href="#cb140-1019" aria-hidden="true" tabindex="-1"></a>d1 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">fun</span>(x, <span class="at">lambda =</span> lambda1))</span>
<span id="cb140-1020"><a href="#cb140-1020" aria-hidden="true" tabindex="-1"></a>d2 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">fun</span>(x, <span class="at">lambda =</span> lambda2))</span>
<span id="cb140-1021"><a href="#cb140-1021" aria-hidden="true" tabindex="-1"></a>d3 <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x, <span class="at">y =</span> <span class="fu">funinv</span>(x, <span class="at">lambda =</span> lambda1))<span class="co">#fun(x, lambda = lambda3))</span></span>
<span id="cb140-1022"><a href="#cb140-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1023"><a href="#cb140-1023" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d1$y)</span></span>
<span id="cb140-1024"><a href="#cb140-1024" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d2$y)</span></span>
<span id="cb140-1025"><a href="#cb140-1025" aria-hidden="true" tabindex="-1"></a><span class="co"># mean(d3$y)</span></span>
<span id="cb140-1026"><a href="#cb140-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1027"><a href="#cb140-1027" aria-hidden="true" tabindex="-1"></a>rd <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb140-1028"><a href="#cb140-1028" aria-hidden="true" tabindex="-1"></a>classif <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.125</span>, <span class="fl">0.25</span>, <span class="fl">0.25</span>), <span class="at">y =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="fl">0.75</span>, <span class="fl">0.625</span>, <span class="fl">0.875</span>),</span>
<span id="cb140-1029"><a href="#cb140-1029" aria-hidden="true" tabindex="-1"></a>  <span class="at">classifier =</span> <span class="fu">c</span>(<span class="st">"best"</span>, <span class="st">"worst"</span>, <span class="st">"random"</span>, <span class="st">"random"</span>, <span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>))</span>
<span id="cb140-1030"><a href="#cb140-1030" aria-hidden="true" tabindex="-1"></a>classif <span class="ot">=</span> <span class="fu">droplevels</span>(classif[<span class="sc">-</span><span class="dv">2</span>, ])</span>
<span id="cb140-1031"><a href="#cb140-1031" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1032"><a href="#cb140-1032" aria-hidden="true" tabindex="-1"></a>p <span class="ot">=</span> <span class="fu">ggplot</span>(rd, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb140-1033"><a href="#cb140-1033" aria-hidden="true" tabindex="-1"></a>  <span class="co"># geom_area(mapping = aes(x = x, y = y), fill = "red", alpha = 0.5) +</span></span>
<span id="cb140-1034"><a href="#cb140-1034" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_fixed</span>(<span class="at">ratio =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb140-1035"><a href="#cb140-1035" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="fu">expression</span>(TPR)) <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">expression</span>(FPR)) <span class="sc">+</span></span>
<span id="cb140-1036"><a href="#cb140-1036" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb140-1037"><a href="#cb140-1037" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1038"><a href="#cb140-1038" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">=</span> p <span class="sc">+</span></span>
<span id="cb140-1039"><a href="#cb140-1039" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">colour =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">linewidth =</span> <span class="fl">0.75</span>) <span class="sc">+</span></span>
<span id="cb140-1040"><a href="#cb140-1040" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fl">0.5</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">label =</span> <span class="st">"baseline (random classifiers)"</span>), <span class="at">colour =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">45</span>) <span class="sc">+</span></span>
<span id="cb140-1041"><a href="#cb140-1041" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> classif, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">colour =</span> classifier, <span class="at">shape =</span> classifier), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb140-1042"><a href="#cb140-1042" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[classif<span class="sc">$</span>classifier <span class="sc">==</span> <span class="st">"random"</span>,],</span>
<span id="cb140-1043"><a href="#cb140-1043" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">1.1</span>, <span class="sc">-</span><span class="fl">0.1</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)),</span>
<span id="cb140-1044"><a href="#cb140-1044" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"always predict positive class"</span>, <span class="st">"never predict positive class"</span>),</span>
<span id="cb140-1045"><a href="#cb140-1045" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb140-1046"><a href="#cb140-1046" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ],</span>
<span id="cb140-1047"><a href="#cb140-1047" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb140-1048"><a href="#cb140-1048" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>),</span>
<span id="cb140-1049"><a href="#cb140-1049" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">c</span>(<span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span>), <span class="co">#c("C1" = "gray70", "C2" = "gray50", "C3" = "gray30"),</span></span>
<span id="cb140-1050"><a href="#cb140-1050" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb140-1051"><a href="#cb140-1051" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">"(a)"</span>) <span class="sc">+</span></span>
<span id="cb140-1052"><a href="#cb140-1052" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">"classifier"</span>,</span>
<span id="cb140-1053"><a href="#cb140-1053" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb140-1054"><a href="#cb140-1054" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>,  <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span></span>
<span id="cb140-1055"><a href="#cb140-1055" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb140-1056"><a href="#cb140-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1057"><a href="#cb140-1057" aria-hidden="true" tabindex="-1"></a>dall <span class="ot">=</span> <span class="fu">rbind</span>(</span>
<span id="cb140-1058"><a href="#cb140-1058" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d1, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d1<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C1"</span>),</span>
<span id="cb140-1059"><a href="#cb140-1059" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d2, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d2<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C2"</span>),</span>
<span id="cb140-1060"><a href="#cb140-1060" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(d3, <span class="at">AUC =</span> <span class="fu">round</span>(<span class="fu">mean</span>(d3<span class="sc">$</span>y), <span class="dv">2</span>), <span class="at">classifier =</span> <span class="st">"C3"</span>),</span>
<span id="cb140-1061"><a href="#cb140-1061" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(classif[<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>], <span class="at">AUC =</span> <span class="dv">1</span>, <span class="at">classifier =</span> <span class="st">"best"</span>),</span>
<span id="cb140-1062"><a href="#cb140-1062" aria-hidden="true" tabindex="-1"></a>  <span class="fu">cbind</span>(rd, <span class="at">AUC =</span> <span class="fl">0.5</span>, <span class="at">classifier =</span> <span class="st">"random"</span>)</span>
<span id="cb140-1063"><a href="#cb140-1063" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb140-1064"><a href="#cb140-1064" aria-hidden="true" tabindex="-1"></a>dall<span class="sc">$</span>AUC <span class="ot">=</span> <span class="fu">factor</span>(dall<span class="sc">$</span>classifier, <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"best"</span>, <span class="st">"random"</span>, <span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>))</span>
<span id="cb140-1065"><a href="#cb140-1065" aria-hidden="true" tabindex="-1"></a><span class="co">#dall$AUC = factor(dall$AUC, levels = sort(unique(dall$AUC), decreasing = TRUE))</span></span>
<span id="cb140-1066"><a href="#cb140-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1067"><a href="#cb140-1067" aria-hidden="true" tabindex="-1"></a>lab <span class="ot">=</span> <span class="fu">c</span>(<span class="st">"best </span><span class="sc">\n</span><span class="st">(AUC = 1)"</span>, <span class="st">"random </span><span class="sc">\n</span><span class="st">(AUC = 0.5)"</span>, <span class="st">"C1 (AUC = 0.9)"</span>, <span class="st">"C2 (AUC = 0.75)"</span>, <span class="st">"C3 (AUC = 0.9)"</span>)</span>
<span id="cb140-1068"><a href="#cb140-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1069"><a href="#cb140-1069" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">=</span> p <span class="sc">+</span></span>
<span id="cb140-1070"><a href="#cb140-1070" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="fl">0.5</span>, <span class="at">y =</span> <span class="fl">0.5</span>, <span class="at">hjust =</span> <span class="fl">0.5</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>, <span class="at">label =</span> <span class="st">"baseline"</span>), <span class="at">colour =</span> <span class="dv">2</span>, <span class="at">size =</span> <span class="dv">3</span>, <span class="at">angle =</span> <span class="dv">45</span>) <span class="sc">+</span></span>
<span id="cb140-1071"><a href="#cb140-1071" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> dall, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">lty =</span> AUC, <span class="at">col =</span> AUC), <span class="at">linewidth =</span> <span class="fl">0.75</span>) <span class="sc">+</span> <span class="fu">ggtitle</span>(<span class="st">"(b)"</span>) <span class="sc">+</span></span>
<span id="cb140-1072"><a href="#cb140-1072" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ], <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">shape =</span> classifier), <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb140-1073"><a href="#cb140-1073" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">data =</span> classif[<span class="fu">grepl</span>(<span class="st">"^C"</span>, classif<span class="sc">$</span>classifier), ],</span>
<span id="cb140-1074"><a href="#cb140-1074" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">hjust =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>), <span class="at">vjust =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>)),</span>
<span id="cb140-1075"><a href="#cb140-1075" aria-hidden="true" tabindex="-1"></a>    <span class="at">label =</span> <span class="fu">c</span>(<span class="st">"C1"</span>, <span class="st">"C2"</span>, <span class="st">"C3"</span>),</span>
<span id="cb140-1076"><a href="#cb140-1076" aria-hidden="true" tabindex="-1"></a>    <span class="at">colour =</span> <span class="fu">c</span>(<span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"black"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"black"</span>),</span>
<span id="cb140-1077"><a href="#cb140-1077" aria-hidden="true" tabindex="-1"></a>    <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span></span>
<span id="cb140-1078"><a href="#cb140-1078" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylim</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb140-1079"><a href="#cb140-1079" aria-hidden="true" tabindex="-1"></a>  <span class="fu">guides</span>(<span class="at">shape =</span> <span class="st">"none"</span>) <span class="sc">+</span></span>
<span id="cb140-1080"><a href="#cb140-1080" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="st">"ROC curve"</span>,</span>
<span id="cb140-1081"><a href="#cb140-1081" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb140-1082"><a href="#cb140-1082" aria-hidden="true" tabindex="-1"></a>      <span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>,</span>
<span id="cb140-1083"><a href="#cb140-1083" aria-hidden="true" tabindex="-1"></a>      <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb140-1084"><a href="#cb140-1084" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="st">"gray70"</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="st">"gray70"</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="st">"gray70"</span>),</span>
<span id="cb140-1085"><a href="#cb140-1085" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> lab) <span class="sc">+</span></span>
<span id="cb140-1086"><a href="#cb140-1086" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(<span class="st">"ROC curve"</span>,</span>
<span id="cb140-1087"><a href="#cb140-1087" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb140-1088"><a href="#cb140-1088" aria-hidden="true" tabindex="-1"></a>      <span class="st">"best"</span> <span class="ot">=</span> <span class="dv">3</span>,</span>
<span id="cb140-1089"><a href="#cb140-1089" aria-hidden="true" tabindex="-1"></a>      <span class="st">"random"</span> <span class="ot">=</span> <span class="dv">2</span>,</span>
<span id="cb140-1090"><a href="#cb140-1090" aria-hidden="true" tabindex="-1"></a>      <span class="st">"C1"</span> <span class="ot">=</span> <span class="dv">3</span>, <span class="st">"C2"</span> <span class="ot">=</span> <span class="dv">4</span>, <span class="st">"C3"</span> <span class="ot">=</span> <span class="dv">5</span>),</span>
<span id="cb140-1091"><a href="#cb140-1091" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> lab) <span class="sc">+</span></span>
<span id="cb140-1092"><a href="#cb140-1092" aria-hidden="true" tabindex="-1"></a>  <span class="cn">NULL</span></span>
<span id="cb140-1093"><a href="#cb140-1093" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1094"><a href="#cb140-1094" aria-hidden="true" tabindex="-1"></a><span class="co">#ggarrange(p1, p2, nrow = 1, ncol = 2)</span></span>
<span id="cb140-1095"><a href="#cb140-1095" aria-hidden="true" tabindex="-1"></a><span class="co"># p1 + geom_function(fun = function(x) fun(x, lambda = lambda1), mapping = aes(col = "0.91")) +</span></span>
<span id="cb140-1096"><a href="#cb140-1096" aria-hidden="true" tabindex="-1"></a><span class="co">#   geom_function(fun = function(x) fun(x, lambda = lambda2)) +</span></span>
<span id="cb140-1097"><a href="#cb140-1097" aria-hidden="true" tabindex="-1"></a><span class="co">#   geom_function(fun = function(x) funinv(x, lambda = lambda1))</span></span>
<span id="cb140-1098"><a href="#cb140-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1099"><a href="#cb140-1099" aria-hidden="true" tabindex="-1"></a>p1 <span class="sc">+</span> p2 <span class="sc">&amp;</span> <span class="fu">theme</span>(<span class="at">plot.margin =</span> grid<span class="sc">::</span><span class="fu">unit</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="st">"mm"</span>))</span>
<span id="cb140-1100"><a href="#cb140-1100" aria-hidden="true" tabindex="-1"></a><span class="co">#p1 + p2 &amp; theme(legend.position = "bottom")</span></span>
<span id="cb140-1101"><a href="#cb140-1101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1102"><a href="#cb140-1102" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1103"><a href="#cb140-1103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1104"><a href="#cb140-1104" aria-hidden="true" tabindex="-1"></a>For <span class="in">`r mlr3`</span> prediction objects, the ROC curve can be constructed with the previously seen <span class="in">`r ref("autoplot.PredictionClassif")`</span> from <span class="in">`r mlr3viz`</span>.</span>
<span id="cb140-1105"><a href="#cb140-1105" aria-hidden="true" tabindex="-1"></a>The x-axis showing the FPR is labelled "1 - Specificity" by convention, whereas the y-axis shows "Sensitivity" for the TPR.</span>
<span id="cb140-1106"><a href="#cb140-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1107"><a href="#cb140-1107" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-055}</span></span>
<span id="cb140-1108"><a href="#cb140-1108" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb140-1109"><a href="#cb140-1109" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1110"><a href="#cb140-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1111"><a href="#cb140-1111" aria-hidden="true" tabindex="-1"></a>We can also plot the precision-recall (PR) curve which visualize the PPV vs. TPR.</span>
<span id="cb140-1112"><a href="#cb140-1112" aria-hidden="true" tabindex="-1"></a>The main difference between ROC curves and PR curves is that the number of true-negatives are not used to produce a PR curve.</span>
<span id="cb140-1113"><a href="#cb140-1113" aria-hidden="true" tabindex="-1"></a>PR curves are preferred over ROC curves for imbalanced populations.</span>
<span id="cb140-1114"><a href="#cb140-1114" aria-hidden="true" tabindex="-1"></a>This is because the positive class is usually rare in imbalanced classification tasks.</span>
<span id="cb140-1115"><a href="#cb140-1115" aria-hidden="true" tabindex="-1"></a>Hence, the FPR is often low even for a random classifier.</span>
<span id="cb140-1116"><a href="#cb140-1116" aria-hidden="true" tabindex="-1"></a>As a result, the ROC curve may not provide a good assessment of the classifier's performance, because it does not capture the high rate of false negatives (i.e., misclassified positive observations).</span>
<span id="cb140-1117"><a href="#cb140-1117" aria-hidden="true" tabindex="-1"></a>See also @davis2006relationship for a detailed discussion about the relationship between the PRC and ROC curves.</span>
<span id="cb140-1118"><a href="#cb140-1118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1119"><a href="#cb140-1119" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-056}</span></span>
<span id="cb140-1120"><a href="#cb140-1120" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb140-1121"><a href="#cb140-1121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1122"><a href="#cb140-1122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1123"><a href="#cb140-1123" aria-hidden="true" tabindex="-1"></a>Another useful way to think about the performance of a classifier is to visualize the relationship of the set threshold with the performance metric at the given threshold.</span>
<span id="cb140-1124"><a href="#cb140-1124" aria-hidden="true" tabindex="-1"></a>For example, if we want to see the FPR and accuracy across all possible thresholds:</span>
<span id="cb140-1125"><a href="#cb140-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1126"><a href="#cb140-1126" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-057}</span></span>
<span id="cb140-1127"><a href="#cb140-1127" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb140-1128"><a href="#cb140-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.fpr"</span>))</span>
<span id="cb140-1129"><a href="#cb140-1129" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(pred, <span class="at">type =</span> <span class="st">"threshold"</span>, <span class="at">measure =</span> <span class="fu">msr</span>(<span class="st">"classif.acc"</span>))</span>
<span id="cb140-1130"><a href="#cb140-1130" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1131"><a href="#cb140-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1132"><a href="#cb140-1132" aria-hidden="true" tabindex="-1"></a>This visualization would show us that it would not matter if we picked a threshold of 0.5 or 0.75, since neither FPR nor accuracy changes in that range.</span>
<span id="cb140-1133"><a href="#cb140-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1134"><a href="#cb140-1134" aria-hidden="true" tabindex="-1"></a>These visualizations are also available for <span class="in">`r ref("ResampleResult")`</span>.</span>
<span id="cb140-1135"><a href="#cb140-1135" aria-hidden="true" tabindex="-1"></a>Here, the predictions of individual resampling iterations are merged prior to calculating a ROC or PR curve (micro-averaged):</span>
<span id="cb140-1136"><a href="#cb140-1136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1137"><a href="#cb140-1137" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-058}</span></span>
<span id="cb140-1138"><a href="#cb140-1138" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb140-1139"><a href="#cb140-1139" aria-hidden="true" tabindex="-1"></a>rr <span class="ot">=</span> <span class="fu">resample</span>(</span>
<span id="cb140-1140"><a href="#cb140-1140" aria-hidden="true" tabindex="-1"></a>  <span class="at">task =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb140-1141"><a href="#cb140-1141" aria-hidden="true" tabindex="-1"></a>  <span class="at">learner =</span> <span class="fu">lrn</span>(<span class="st">"classif.ranger"</span>, <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb140-1142"><a href="#cb140-1142" aria-hidden="true" tabindex="-1"></a>  <span class="at">resampling =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb140-1143"><a href="#cb140-1143" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb140-1144"><a href="#cb140-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1145"><a href="#cb140-1145" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb140-1146"><a href="#cb140-1146" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(rr, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb140-1147"><a href="#cb140-1147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1148"><a href="#cb140-1148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1149"><a href="#cb140-1149" aria-hidden="true" tabindex="-1"></a>We can also visualize a <span class="in">`r ref("BenchmarkResult")`</span> to compare multiple learners on the same <span class="in">`r ref("Task")`</span>:</span>
<span id="cb140-1150"><a href="#cb140-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1151"><a href="#cb140-1151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r performance-059}</span></span>
<span id="cb140-1152"><a href="#cb140-1152" aria-hidden="true" tabindex="-1"></a><span class="co">#| layout-ncol: 2</span></span>
<span id="cb140-1153"><a href="#cb140-1153" aria-hidden="true" tabindex="-1"></a>design <span class="ot">=</span> <span class="fu">benchmark_grid</span>(</span>
<span id="cb140-1154"><a href="#cb140-1154" aria-hidden="true" tabindex="-1"></a>  <span class="at">tasks =</span> <span class="fu">tsk</span>(<span class="st">"german_credit"</span>),</span>
<span id="cb140-1155"><a href="#cb140-1155" aria-hidden="true" tabindex="-1"></a>  <span class="at">learners =</span> <span class="fu">lrns</span>(<span class="fu">c</span>(<span class="st">"classif.rpart"</span>, <span class="st">"classif.ranger"</span>), <span class="at">predict_type =</span> <span class="st">"prob"</span>),</span>
<span id="cb140-1156"><a href="#cb140-1156" aria-hidden="true" tabindex="-1"></a>  <span class="at">resamplings =</span> <span class="fu">rsmp</span>(<span class="st">"cv"</span>, <span class="at">folds =</span> <span class="dv">5</span>)</span>
<span id="cb140-1157"><a href="#cb140-1157" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb140-1158"><a href="#cb140-1158" aria-hidden="true" tabindex="-1"></a>bmr <span class="ot">=</span> <span class="fu">benchmark</span>(design)</span>
<span id="cb140-1159"><a href="#cb140-1159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1160"><a href="#cb140-1160" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"roc"</span>)</span>
<span id="cb140-1161"><a href="#cb140-1161" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(bmr, <span class="at">type =</span> <span class="st">"prc"</span>)</span>
<span id="cb140-1162"><a href="#cb140-1162" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb140-1163"><a href="#cb140-1163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1164"><a href="#cb140-1164" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb140-1165"><a href="#cb140-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1166"><a href="#cb140-1166" aria-hidden="true" tabindex="-1"></a>In this chapter, we learned how to estimate the generalization performance of a model via resampling.</span>
<span id="cb140-1167"><a href="#cb140-1167" aria-hidden="true" tabindex="-1"></a>We also learned about benchmarking to fairly compare the estimated generalization performance of different learners across multiple tasks.</span>
<span id="cb140-1168"><a href="#cb140-1168" aria-hidden="true" tabindex="-1"></a>Performance calculations underpin these concepts, and we have seen some of them applied to classification tasks, with a more in-depth look at the special case of binary classification and ROC analysis.</span>
<span id="cb140-1169"><a href="#cb140-1169" aria-hidden="true" tabindex="-1"></a>We also learned how to visualize confusion matrix-based performance measures with regards to different thresholds as well as resampling and benchmark results with <span class="in">`r mlr3viz`</span>.</span>
<span id="cb140-1170"><a href="#cb140-1170" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- that cut-off the predicted probabilities and assign the predictions to the positive and negative class  --&gt;</span></span>
<span id="cb140-1171"><a href="#cb140-1171" aria-hidden="true" tabindex="-1"></a>The discussed topics belong to the fundamental concepts of supervised machine learning.</span>
<span id="cb140-1172"><a href="#cb140-1172" aria-hidden="true" tabindex="-1"></a>@sec-optimization builds on these concepts and applies them for tuning (i.e., to automatically choose the optimal hyperparameters of a learner) through nested resampling (@sec-nested-resampling).</span>
<span id="cb140-1173"><a href="#cb140-1173" aria-hidden="true" tabindex="-1"></a>In @sec-special, we will also take a look at specialized tasks that require different resampling strategies.</span>
<span id="cb140-1174"><a href="#cb140-1174" aria-hidden="true" tabindex="-1"></a>Finally, @tbl-api-performance provides an overview of some important <span class="in">`r mlr3`</span> functions and the corresponding R6 classes that were most frequently used throughout this chapter.</span>
<span id="cb140-1175"><a href="#cb140-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1176"><a href="#cb140-1176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1177"><a href="#cb140-1177" aria-hidden="true" tabindex="-1"></a>| S3 function                 | R6 Class              | Summary                                      |</span>
<span id="cb140-1178"><a href="#cb140-1178" aria-hidden="true" tabindex="-1"></a>| --------------------------- | --------------------- | -------------------------------------------- |</span>
<span id="cb140-1179"><a href="#cb140-1179" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("rsmp()")`</span>           | <span class="in">`r ref("Resampling")`</span> | Assigns observations to train- and test sets |</span>
<span id="cb140-1180"><a href="#cb140-1180" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("resample()")`</span>       | <span class="in">`r ref("ResampleResult")`</span> | Evaluates learners on given tasks using a resampling strategy |</span>
<span id="cb140-1181"><a href="#cb140-1181" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("benchmark_grid()")`</span> | -                     | Constructs a design grid of learners, tasks, and resamplings  |</span>
<span id="cb140-1182"><a href="#cb140-1182" aria-hidden="true" tabindex="-1"></a>| <span class="in">`r ref("benchmark()")`</span>      | <span class="in">`r ref("BenchmarkResult")`</span> | Evaluates learners on a given design grid |</span>
<span id="cb140-1183"><a href="#cb140-1183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1184"><a href="#cb140-1184" aria-hidden="true" tabindex="-1"></a>:Core S3 'sugar' functions for resampling and benchmarking in mlr3 with the underlying R6 class that are constructed when these functions are called (if applicable) and a summary of the purpose of the functions. {#tbl-api-performance}</span>
<span id="cb140-1185"><a href="#cb140-1185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1186"><a href="#cb140-1186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1187"><a href="#cb140-1187" aria-hidden="true" tabindex="-1"></a><span class="fu">### Resources {.unnumbered .unlisted}</span></span>
<span id="cb140-1188"><a href="#cb140-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1189"><a href="#cb140-1189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn more about advanced resampling techniques in: <span class="in">`r link("https://mlr-org.com/gallery/basic/2020-03-30-stratification-blocking/", "Resampling - Stratified, Blocked and Predefined")`</span>.</span>
<span id="cb140-1190"><a href="#cb140-1190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Check out the blog post <span class="in">`r link("https://mlr-org.com/gallery/basic/2020-03-18-iris-mlr3-basics/", "mlr3 Basics on “Iris” - Hello World!")`</span> to see minimal examples on using  resampling and benchmarking on the iris dataset.</span>
<span id="cb140-1191"><a href="#cb140-1191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use resampling and benchmarking for the <span class="in">`r link("https://mlr-org.com/gallery/basic/2020-08-14-comparison-of-decision-boundaries/", "comparison of decision boundaries of classification learners")`</span>.</span>
<span id="cb140-1192"><a href="#cb140-1192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Learn how to effectively pick thresholds by applying tuning and pipelines (Chapters <span class="co">[</span><span class="ot">-@sec-optimization</span><span class="co">]</span> and <span class="co">[</span><span class="ot">-@sec-pipelines</span><span class="co">]</span>) in <span class="in">`r link("https://mlr-org.com/gallery/optimization/2020-10-14-threshold-tuning/index.html", "this post on threshold tuning")`</span>.</span>
<span id="cb140-1193"><a href="#cb140-1193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1194"><a href="#cb140-1194" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb140-1195"><a href="#cb140-1195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1196"><a href="#cb140-1196" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Use the <span class="in">`spam`</span> task and 5-fold cross-validation to benchmark Random Forest (<span class="in">`classif.ranger`</span>), Logistic Regression (<span class="in">`classif.log_reg`</span>), and XGBoost (<span class="in">`classif.xgboost`</span>) with regards to AUC.</span>
<span id="cb140-1197"><a href="#cb140-1197" aria-hidden="true" tabindex="-1"></a>Which learner appears to do best? How confident are you in your conclusion?</span>
<span id="cb140-1198"><a href="#cb140-1198" aria-hidden="true" tabindex="-1"></a>How would you improve upon this?</span>
<span id="cb140-1199"><a href="#cb140-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb140-1200"><a href="#cb140-1200" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>A colleague claims to have achieved a 93.1% classification accuracy using the <span class="in">`classif.rpart`</span> learner on the <span class="in">`penguins_simple`</span> task.</span>
<span id="cb140-1201"><a href="#cb140-1201" aria-hidden="true" tabindex="-1"></a>You want to reproduce their results and ask them about their resampling strategy.</span>
<span id="cb140-1202"><a href="#cb140-1202" aria-hidden="true" tabindex="-1"></a>They said they used 3-fold cross-validation, and they assigned rows using the task's <span class="in">`row_id`</span> modulo 3 to generate three evenly sized folds.</span>
<span id="cb140-1203"><a href="#cb140-1203" aria-hidden="true" tabindex="-1"></a>Reproduce their results using the custom CV strategy.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">All content licenced under <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a>.</div>   
      <div class="nav-footer-center"><a href="https://mlr-org.com">Website</a> | <a href="https://github.com/mlr-org/mlr3book">GitHub</a> | <a href="https://mlr-org.com/gallery">Gallery</a> | <a href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">Mattermost</a></div>
    <div class="nav-footer-right">Written with <i class="bi bi-heart-fill"></i> for #rstats, ML and FOSS by the mlr-org team.</div>
  </div>
</footer>


<script src="site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>