---
author:
  - name: Giuseppe Casalicchio
    orcid: 0000-0001-5324-5966
    email: giuseppe.casalicchio@stat.uni-muenchen.de
    affiliations:
      - name: LMU München
      - name: Munich Center for Machine Learning (MCML)
  - name: Lukas Burk
    orcid: 0000-0001-7528-3795
    email: Lukas.Burk@stat.uni-muenchen.de
    affiliations:
      - name: LMU München
      - name: Leibniz Institute for Prevention Research and Epidemiology - BIPS
abstract: TODO (150-200 WORDS)
---

# Resampling and Benchmarking {#sec-performance}

{{< include _setup.qmd >}}

<!-- TODO: check links e.g. autoplot etc. -->
<!-- TODO: Check crossrefs, e.g. (#measures) lead so chapter "Special Tasks" -->

<!-- why performance estimation, what is a performance measure -->

A deployed supervised machine learning model should generalize well to new, unseen data.
To assess the generalization performance of a model, we must first decide on a [performance measure](#measures) that is appropriate for our given task and evaluation goal.
<!-- The measure computes a numeric score indicating, e.g., how well the model predictions match the ground truth. -->
<!-- However, it may also reflect other qualities such as the time for training a model. -->

<!-- motivate the need for splitting the data -->
Once we have decided on a performance measure, we next need a strategy for how to use the available data to reliably estimate the generalization performance of the deployed model.
Unfortunately, using the same data to train and test a model would be a bad strategy as this would lead to an overly optimistic performance estimate.
For example, an overfitted model may perfectly fit the data it was trained on but would not generalize well to new data. 
Assessing the generalization performance using the same data a model was trained on would misleadingly suggest a well-performing model.
To avoid such an optimistically biased performance estimate, it is recommended to test a model on independent data not used for training.
However, we typically train a deployed model on all available data, which leaves no data to assess its generalization performance.
Existing strategies for estimating the generalization performance therefore partition all available data into training and test data in different ways, and use the test data to mimic the presence of unseen data.

<!-- describe performance estimation procedure and desiderata -->
The most straightforward strategy is the holdout method which partitions the data into a single training and test set.
We then train an intermediate model only on the training set and use the test set to assess its performance.
This performance estimate is then used as a proxy for the performance of the final model.
Ideally, we want the training set to be as close as possible to the size of all available data because the intermediate model should represent the final model well.
If the training data is too small, the performance estimate may be pessimistically biased. 
This is because learning algorithms often can not learn complex relationships in the data when given a small amount of training data.
On the other hand, we also want as much test data as possible to reliably estimate the generalization performance.
However, both goals are difficult to achieve by the holdout method if we have only access to a limited amount of data.

<!-- explain resampling -->
To address this issue, [resampling strategies](#resampling) repeatedly split all available data into multiple training and test sets and average the performance across multiple resampling iterations (see @fig-ml-abstraction for an illustration).
Resampling allows using more data points for testing while keeping the training sets as large as possible. 
A higher number of resampling iterations can reduce the variance and result in a more reliable performance estimate.
For example, the $k$-fold cross-validation method randomly partitions the data into $k$ subsets, called folds (see illustration TODO).
Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations.
The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate.
A resampling strategy like $k$-fold cross-validation would be preferable over a single train-test split as it makes use of all available data and avoids the performance estimate being strongly biased by any particular split.
Resampling strategies evaluate the performance of the learning algorithm that induced the final model as a proxy for the performance of the final model. 
However, this is the best we can do if we only have access to a limited amount of data.

```{r performance-001, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "A general abstraction of the ML model evaluation process."
#| fig.align: "center"
knitr::include_graphics("Figures/ml_abstraction.svg")
```


<!-- Several variations of cross-validation exist, including repeated k-fold cross-validation (`repeated_cv`) and leave-one-out cross-validation (`loo`).  -->
<!-- The specific variation used depends on properties of the task at hand and the goals of the performance assessment. -->
<!-- Selecting a resampling strategy will typically be motivated by the context defined by the task(s) and learner(s) to be evaluated. -->

TODO: add fig to illustrate CV

TODO: should we add a fig for an overview of other resampling strategies and some remarks on practical guidelines (e.g. take the one from i2ml lecture)?  
+1 for >1 illustrations for resampling strategies, e.g. CV + bootstrapping or subsampling?  
Also maybe mention LOO with CV as it's got it's own name.

<!-- chapter outline -->

In the previous chapter, we have already applied the holdout method and manually partitioned the data contained in a `r ref("Task")` object into a single training set (to train the model) and a single test set (to estimate the generalization performance).
In this chapter, we will learn how to estimate the generalization performance of a `r ref("Learner")` using resampling strategies implemented in the `r mlr3` package. 
Additionally, we introduce convenient functions to conduct benchmark experiments and compare the generalization performance of multiple learners across various tasks.

<!-- #### Further Reading -->
<!-- I assume this should go to the end of the chapter? -->
<!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. -->
<!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). -->
<!-- * [@bischl2012resampling](https://direct.mit.edu/evco/article-abstract/20/2/249/925/Resampling-Methods-for-Meta-Model-Validation-with?redirectedFrom=fulltext) provide an overview of resampling methods. -->

<!-- * Japkowicz, N. and M. Shah (2011). Evaluating Learning Algorithms: A Classification Perspective. Cambridge: Cambridge University Press -->

## Resampling Strategies {#sec-resampling}

<!-- [Resampling](#resampling) is a strategy that defines how to partition the available data into training and test data. -->
In this section, we cover how to use `r mlr3` to

* [query](#resampling-strategies) implemented resampling strategies,
* [construct](#resampling-construct) resampling objects for a selected resampling strategy,
* [instantiate](#resampling-inst) the train-test splits of a resampling object on a given task, and
* [execute](#resampling-exec) the selected resampling strategy on a learning algorithm to obtain resampling results.

There are many different resampling strategies for partitioning a dataset into training and test.
`r mlr3` already provides many resampling strategies, so users do not have to implement them from scratch, which can be tedious and error-prone.
The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment.

TODO: refer to literature?

The following sections provide guidance with concrete code examples on how to select a resampling strategy and then apply it on the `r ref("mlr_tasks_penguins", text = "penguins")` task to estimate the performance of a simple classification tree from the `r ref_pkg("rpart")` package. 
We therefore define the corresponding `r ref("Task")` and `r ref("Learner")` object used througout the chapter as follows:

```{r performance-002}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
```

### Query {#resampling-strategies}

<!-- We first need to define which resampling strategy we want to use. -->
All implemented resampling strategies can be queried by looking at the `r ref("mlr_resamplings")` dictionary.
The dictionary contains several common resampling strategies, including holdout, (repeated) cross-validation, bootstrap, and subsampling.
Passing the dictionary to the `as.data.table` function provides a more structured output with additional information:

```{r performance-003}
as.data.table(mlr_resamplings)
```

For example, the column `params` shows the parameters of each resampling strategy (e.g., the train-test splitting `ratio` or the number of `repeats`) and the column `iters` shows the default value for the number of performed resampling iterations (i.e., the number of model fits).

### Construction {#resampling-construct}

Once we have decided on a resampling strategy, we have to construct a `r ref("Resampling")` object via the function `r ref("rsmp()")` which will define the resampling strategy we want to employ.
For example, to construct a `r ref("Resampling")` object for holdout, we use the value of the `key` column from the `r ref("mlr_resamplings")` dictionary and pass it to the convenience function `r ref("rsmp()")`:

```{r performance-004}
resampling = rsmp("holdout")
print(resampling)
```

By default, holdout resampling will use 2/3 of the data as training set and 1/3 as test set.
However, we can change this by specifying the `ratio` parameter for holdout either during construction or by updating the `ratio` parameter afterwards.
For example, we construct a `r ref("Resampling")` object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):

```{r performance-005}
resampling = rsmp("holdout", ratio = 0.8)
resampling$param_set$values = list(ratio = 0.5)
```

Holdout only estimates the generalization performance using a single test set.
To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies.
For example, we could also set up a 10-fold cross-validation via

```{r}
resampling = rsmp("cv", folds = 10)
```

By default, the `$is_instantiated` field of a `r ref("Resampling")` object constructed as shown above is set to `FALSE`.
This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the `r ref("Resampling")` object. 
<!-- In the next section, we  -->
<!-- imo these sections are short enough that "in the next/previous section..." sentences aren't needed -->

<!-- When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy. -->

<!-- In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue. -->
<!-- If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly. -->


### Instantiation {#resampling-inst}

<!-- In this section, we show how to instantiate a resampling strategy (i.e., how to generate the train-test splits) by applying it to a task. -->
<!-- To obtain the row indices for the training and the test splits, we need to call the `instantiate()` method on a `r ref("Task")` object. -->
<!-- The resulting train-test indices are then stored in the `r ref("Resampling")` object: -->

To generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the `instantiate()` method of the previously constructed `r ref("Resampling")` object on a `r ref("Task")`.
This will manifest a fixed partition and store the row indices for the training and test sets directly in the `r ref("Resampling")` object. 
We can access these rows via the `$train_set()` and `$test_set()` methods:

```{r performance-006}
resampling = rsmp("holdout", ratio = 0.8)
resampling$instantiate(task)
train_ids = resampling$train_set(1)
test_ids = resampling$test_set(1)
str(train_ids)
str(test_ids)
```

Note that for a fair comparison of multiple [learners](#learners), it is important to use the same instantiated `r ref("Resampling")` object for each learner. 
This will ensure that each learner uses the same training data to build a model and that the performance of the trained model is then assessed using the same test set. 
<!-- In @sec-benchmarking, we will explore this process in detail. -->


### Execution {#resampling-exec}

<!-- With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations. -->
<!-- For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object. -->

Calling the function `r ref("resample()")` on a task, learner and the constructed resampling object will return a `r ref("ResampleResult")` object which contains all information needed to estimate the generalization performance.
Specifically, the function will internally use the learner to train a model for each training set determined by the resampling strategy and store the model predictions of each test set.
By default, these models are discarded after the prediction step to reduce memory consumption of the resulting `r ref("ResampleResult")` object and because we usually only need the stored predictions to calculate the performance measure.
However, we can configure the `r ref("resample()")` function to keep the fitted models (e.g. if we want to use or inspect the intermediate models later) by setting the `store_models` argument of the `r ref("resample()")` function to `TRUE`:

```{r performance-007}
resampling = rsmp("cv", folds = 5)
rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

Here, we used 5-fold cross-validation as resampling strategy.
The resulting `r ref("ResampleResult")` object (stored as `rr`) provides various methods to access and aggregate the stored information.
The two most relevant methods are `$score()` and `$aggregate()`:

The `$score()` method uses a `r ref("Measure")` to calculate the performance measure for each resampling iteration. 
By default, the classification error (`classif.ce`) is used for classification and the mean squared error (`regr.mse`) for regression tasks. 
In the code example below, we explicitly use the classification accuracy (`classif.acc`) as performance measure and pass it to the `$score()` method:

```{r}
acc = rr$score(msr("classif.acc"))
acc[, .(iteration, classif.acc)]
```

Similarly, we can pass a performance measure to the `$aggregate()` method to calculate the average performance across all resampling iterations:

```{r performance-008}
#| eval: true
rr$aggregate(msr("classif.acc"))
```

This aggregate score is the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the `r ref("Resampling")` object.
The performance results in each resampling iteration returned by the `$score()` method may be useful to inspect if one (or more) of the iterations lead to very different performance results from the average.

### Inspect ResampleResult Objects {#resampling-inspect}

{{< include _optional.qmd >}}

TODO: Do we need all of the bulletpoints below? It's basically stuff from the docu... I would remove the whole section  
+1 for removing the bullet points, but showcasing `$errors`, `$model`, `$train_set()` is useful imo, so I wouldn't trim the whole section I think. Maybe we de-bulletpointify this section while showing the relevant elements?



In this section, we show how to inspect important fields and methods of `r ref("ResampleResult")` object:

- Check for warnings or errors by looking at the `$warnings` and `$errors` fields.

- Access the `r ref("Resampling")` object by the `$resampling` field which allows to extract information on the train-test splits of the employed resampling strategy. 
For example, we can extract the total number of performed resampling iterations via `$resampling$iters` or the row indices used for training and testing in each resampling iteration via `$resampling$train_set(i)` and `$resampling$test_set(i)` methods, where `i` refers to the `i`-th resampling iteration:

```{r performance-010}
rr$resampling$iters
str(rr$resampling$train_set(1))
str(rr$resampling$test_set(1))
```

- Retrieve and inspect the model trained in a specific resampling iteration (if we have set the argument `store_models = TRUE`) via `$learners[[i]]$model` where `i` refers to the `i`-th resampling iteration:

```{r performance-011}
rr$learners[[1]]$model
```

- Extract a list of prediction objects of each model trained in each iteration via `$predictions()` (e.g. to assess the performance on all prediction objects individually and then average them to obtain a macro averaged performance estimate) or to extract a single prediction object that combines all predictions via `$prediction()` (e.g. to assess the performance on the combined prediction object to obtain a micro averaged performance estimate).

- Filter the result and keep only results of certain resampling iterations, e.g., use `$filter(c(1, 3))` to discard the results of the second resampling iteration.


### Custom Resampling {#resamp-custom}

TODO: Should this be marked optional? It's not required to get started / understand how to resample and benchmark, so it feels more like an advanced topic that's useful for people specifically interested in it.

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study.
A custom resampling strategy can be constructed using `rsmp("custom")`, which involves defining the row indices used for training and testing manually to instantiate it on a task.
In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the `$train` and `$test` fields.

```{r performance-014}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:50, 151:333)),
  test = list(51:150)
)
```

The resulting `r ref("Resampling")` object can then be used like all other resampling strategies.
To show that both sets contain the row indices we have defined, we can inspect the instantiated `r ref("Resampling")` object:

```{r performance-015}
str(resampling$train_set(1))
str(resampling$test_set(1))
```

The above is equivalent to a custom train-test split analogous to the holdout strategy.  
A custom version of the cross-validation strategy can be constructed using `rsmp("custom_cv")`.
The important difference is that we have to specify either a custom `factor` variable (using the `f` argument of the `$instantiate()` method) or a `factor` column (using the `col` argument of the `$instantiate()` method) from the data to determine the folds.

<!-- We can then assign custom folds either by providing either a `factor` variable as `f`, or a task column to be used for splitting as `col`. -->

In the example below, we instantiate a custom 4-fold cross-validation strategy using a `factor` variable called `folds` that contains 4 equally sized levels to define the 4 folds, each with one quarter of the total size of the `"penguin"` task:

```{r performance-017}
custom_cv = rsmp("custom_cv")
folds = as.factor(rep(1:4, each = task$nrow/4))
custom_cv$instantiate(task, f = folds)
custom_cv
```


### Resampling with Stratification and Grouping

In some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belong to a group (e.g., when the data contains repeated measurements of individuals). 
In such settings, observations usually belong to groups, and we either want to ensure that all observations of the same group belong to the training set or to the test set.
In `r mlr3`, we can assign a special column role to a feature contained in the data already during task construction. 
For example, the column role `"group"` specifies which column in the data should be used to define the group structure of the observations (see also the help section on `r ref("Resampling")` for more information on the column role `"group"`).

A possible use case for the need of grouping (or blocking) of observations during resampling is spatiotemporal modeling, where observations inherit a natural grouping, either in space or time or in both space and time that need to be considered during resampling.

TODO: Do we keep this note or relegate spatiotempcv to a "further reading" section at the end? When we mention spatiotemp in the text anyway, the note should maybe be just regular text as well?

:::{.callout-tip appearance="simple"}
Dedicated spatiotemporal resampling methods are available in `r mlr3spatiotempcv` which implicitly take into account the spatiotemporal structure, see the [spatiotemporal resampling](special.html#spatiotemp-cv) section for more details.
:::

Another column role available in `r mlr3` is `"stratum"` which implements stratified sampling and ensures that the observations in the training and test set are similarly distributed as in the original task containing all observations.
One or multiple columns can be used for stratification via the column role `stratum`.
See also the [mlr3gallery post on this topic](https://mlr3gallery.mlr-org.com/posts/2020-03-30-stratification-blocking/) for a practical introduction.

TODO: iirc gallery posts should be grouped at the end of the chapter?

### Plotting Resample Results {#autoplot-resampleresult}

`r mlr3viz` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method to automatically visualize the resampling results either in a boxplot or histogram :

```{r performance-018}
#| layout-ncol: 2
resampling = rsmp("bootstrap")
rr = resample(task, learner, resampling, store_models = TRUE)

library(mlr3viz)
autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
autoplot(rr, measure = msr("classif.acc"), type = "histogram")
```

We can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:

```{r performance-019}
task$select(c("bill_length", "flipper_length"))
resampling = rsmp("cv", folds = 4)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

TODO: Plot legend "Learner" shows target classes, bug in mlr3viz?
TODO: We removed ROC, what do we do with the note below?

:::{.callout-tip}
In case of a binary classification task, you can also use `type = "roc"` or `type = "prc"` to obtain ROC or Precision-Recall curves, respectively. 
For more information see the help page of `r ref("autoplot.ResampleResult()")`.
:::

## Benchmarking {#sec-benchmarking}

<!-- Benchmarking in machine learning is the comparison of the performance of different learning algorithms on multiple tasks and/or different resampling strategies. -->
Benchmarking is used to compare the performance of different learning algorithms applied on one or more tasks using potentially different resampling strategies.
The purpose is to rank the learning algorithms regarding a performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks.
The `r mlr3` package offers the convenience function `r ref("benchmark()")` to conduct a benchmark experiment and repeatedly train and evaluate multiple learners under the same conditions.
In this section, we cover how to

* [construct a benchmark design](#bm-design) to define the benchmark experiments to be performed,
* [run the benchmark experiments](#bm-exec) and aggregate their results, and
* [convert benchmark objects](#bm-resamp) to other types of objects that can be used for different purposes.

### Construction of Benchmarking Design {#bm-design}

In `r mlr3`, we can define a design to perform benchmark experiments via the `r ref("benchmark_grid()")` convenience function.
The design is essentially a table of scenarios to be evaluated and usually consists of unique combinations of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` triplets.

The `r ref("benchmark_grid()")` function constructs an exhaustive design to describe which combinations of learner, task and resampling should be used in a benchmark experiment.
It properly instantiates the used resampling strategies so that all learners are evaluated on the same train-test splits for each task, ensuring a fair comparison.
To construct a list of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` objects, we can use the convenience functions `r ref("tsks()")`, `r ref("lrns()")`, and `r ref("rsmps()")`.

<!-- To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure. -->

<!-- We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data. -->

We design an examplary benchmark experiment and train a classification tree from the `r ref_pkg("rpart")` package, a random forest from the `r ref_pkg("ranger")` package and a featureless learner serving as a baseline on four different binary classification tasks.

```{r performance-020}
library("mlr3verse")

tsks = tsks(c("spam", "german_credit", "sonar", "breast_cancer"))
lrns = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"), 
  predict_type = "prob")
rsmp = rsmps("cv", folds = 5)

design = benchmark_grid(tsks, lrns, rsmp)
```
<!-- as.data.table(lapply(design, function(col) vapply(col, function(obj) obj$id, FUN.VALUE = NA_character_))) -->

The constructed benchmark design is a `data.table` containing the task, learner, and resampling combinations in each row that should be performed.
It is also possible to subset the design, e.g. to exclude a specific task-learner combination by manually removing a certain row from the design table.
Alternatively, we can also construct a custom benchmark design by manually defining a `data.table` containing task, learner, and resampling objects (see also the examples section in the help page of `r ref("benchmark_grid()")`).

TODO: Note is somewhat redundant with the previous lines, but since proper instantiation is crucial, this bit of information should not be overlooked.

:::{.callout-tip}
Note that if you construct a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design.
:::

<!-- ```{r performance-021} -->
<!-- #| echo: false -->
<!-- # Creating a grid using a cross join -->
<!-- design_manual = data.table::CJ( -->
<!--   task = tsks(c("spam", "german_credit", "sonar")), -->
<!--   learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"), -->
<!--                  predict_type = "prob", predict_sets = c("train", "test")), -->
<!--   resampling = rsmps("cv", folds = 3), -->
<!--   sorted = FALSE -->
<!-- ) -->

<!-- # Manually remove e.g. the third combination from the grid -->
<!-- design_manual = design_manual[-3] -->

<!-- # Manually instantiate the resamplings -->
<!-- Map(function(task, resampling) { -->
<!--   resampling$instantiate(task) -->
<!-- }, task = design_manual$task, resampling = design_manual$resampling) -->
<!-- ``` -->


### Execution of Benchmark Experiments {#bm-exec}

To run the benchmark experiment, we can pass the constructed [benchmark design](#bm-design) to the `r ref("benchmark()")` function:

```{r performance-022}
bmr = benchmark(design)
```

<!-- I think this part is clear from the previous section? -->
<!-- :::{.callout-tip} -->
<!-- Note that we did not have to instantiate the resampling manually. -->
<!-- A benchmark design created using `r ref("benchmark_grid()")` ensures that each resampling strategy is instantiated once for each task during the construction of the exhaustive grid. -->
<!-- This way, each learner operates on the same training and test sets and allows a fair comparison. -->
<!-- ::: -->

Once the benchmarking is finished (this can take some time, depending on the size of your design), we can aggregate the performance results with the `$aggregate()` method of the returned `r ref("BenchmarkResult")`:

```{r performance-023}
acc = bmr$aggregate(msr("classif.acc"))
acc[, .(task_id, learner_id, classif.acc)]
```

As the results are shown in a `r ref("data.table")`, we can easily aggregate the results even further.
For example, if we are interested in the learner that performed best across all tasks, we could average the performance of each individual learner across all tasks:

```{r performance-024}
acc[, mean(classif.acc), by = "learner_id"]
```

Unsurprisingly, the featureless learner has lowest performance overall, as it always predicts the majority class.
However, it is common practice to include it as a baseline in benchmarking experiments to easily gauge the relative performance of other algorithms.
In this simple benchmark experiment, the random forest performed best and outperformed a single classification tree.

<!-- We construct two measures to calculate the area under the curve (AUC) for the training and the test set: -->

<!-- ```{r performance-023} -->
<!-- measures = list( -->
<!--   msr("classif.auc", predict_sets = "train", id = "auc_train"), -->
<!--   msr("classif.auc", id = "auc_test") -->
<!-- ) -->

<!-- tab = bmr$aggregate(measures) -->
<!-- print(tab[, .(task_id, learner_id, auc_train, auc_test)]) -->
<!-- ``` -->

<!-- Simply aggregating the performances with the mean is usually not statistically sound. -->
<!-- Instead, we calculate the rank statistic for each learner, grouped by task. -->
<!-- Then the calculated ranks, grouped by the learner, are aggregated with the `r ref_pkg("data.table")` package. -->
<!-- As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$. -->

<!-- ```{r performance-024} -->
<!-- library("data.table") -->
<!-- # group by levels of task_id, return columns: -->
<!-- # - learner_id -->
<!-- # - rank of col '-auc_train' (per level of learner_id) -->
<!-- # - rank of col '-auc_test' (per level of learner_id) -->
<!-- ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id] -->
<!-- print(ranks) -->

<!-- # group by levels of learner_id, return columns: -->
<!-- # - mean rank of col 'rank_train' (per level of learner_id) -->
<!-- # - mean rank of col 'rank_test' (per level of learner_id) -->
<!-- ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id] -->

<!-- # print the final table, ordered by mean rank of AUC test -->
<!-- ranks[order(mrank_test)] -->
<!-- ``` -->


### Plotting Benchmark Results {#autoplot-benchmarkresult}

Similar to creating automated visualizations for [tasks](#autoplot-task), [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), the `r mlr3viz` package also provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method to visualize benchmark results:

```{r performance-025}
#| fig-height: 8
#| fig-width: 6
autoplot(bmr, measure = msr("classif.acc"))
```

Such a plot summarizes the overall performance of learners for each task in a boxplot.

TODO: We have removed most ROC-relatec content from this chapter, so this should probably also go in wherever the "ROC part" ends up going?

We can also plot the ROC (receiver operating characteristics) curves a specific binary classification tasks.
For this purpose, we filter the `r ref("BenchmarkResult")` to only contain a single `r ref("Task")` and then use `type = "roc"` in the `r ref("ggplot2::autoplot()", text = "autoplot()")` function to create ROC curves of each learner:

```{r performance-026}
bmr_spam = bmr$clone(deep = TRUE)$filter(task_id = "spam")
autoplot(bmr_spam, type = "roc")
```

All available plot types are listed in the help page of `r ref("autoplot.BenchmarkResult()")`.

### Statistical Tests

{{< include _optional.qmd >}}

The package `r mlr3benchmark` provides an infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")` objects.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported for benchmark experiments with at least two tasks and at least two learners.

```{r performance-027}
library("mlr3benchmark")

bma = as.BenchmarkAggr(bmr, measures = msr("classif.acc"))
bma$friedman_posthoc()
```

The results can be summarized in a critical difference plot which typically shows the mean rank of a learning algorithm on the x-axis along with a thick horizontal line that connects learners which are not significantly different:

```{r performance-0271}
autoplot(bma, type = "cd")
```


<!-- ### Extracting ResampleResult Objects {#bm-resamp} -->

<!-- ```{r performance-029} -->
<!-- measure = msr("classif.auc") -->
<!-- rr$aggregate(measure) -->

<!-- # get the iteration with worst AUC -->
<!-- perf = rr$score(measure) -->
<!-- i = which.min(perf$classif.auc) -->

<!-- # get the corresponding learner and training set -->
<!-- print(rr$learners[[i]]) -->
<!-- head(rr$resampling$train_set(i)) -->
<!-- ``` -->

### Extracting, Converting and Merging {#bm-resamp}

<!-- {{< include _optional.qmd >}} -->

A `r ref("BenchmarkResult")` object is a collection of multiple `r ref("ResampleResult")` objects.
These can be extracted via the `$resample_result(i)` method, where `i` is the index of the performed benchmark experiment.
This allows us to investigate the extracted `r ref("ResampleResult")` or individual resampling iterations as shown in [the previous section on resampling](#resampling):

```{r performance-028}
rr1 = bmr$resample_result(1)
rr2 = bmr$resample_result(2)
rr1
rr2
```

A `r ref("ResampleResult")` can be again converted to a `r ref("BenchmarkResult")` with the function `r ref("as_benchmark_result()")`:

```{r performance-029}
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)
bmr1$aggregate(msr("classif.acc"))
bmr2$aggregate(msr("classif.acc"))
```

We can also combine two `r ref("BenchmarkResult", text = "BenchmarkResults")` into a larger result object, for example, for two related benchmarks that are computed on different machines.

```{r performance-030}
bmr_combined = c(bmr1, bmr2)
bmr_combined$aggregate(msr("classif.acc"))
```

## Conclusion (TODO)

If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. 
In chapter [pipelines](pipelines.qmd), we introduce the `r mlr3pipelines` package that solves this issue by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline. 
As the pipeline itself behaves like a `r ref("Learner")`, we can use all functions introduced in this chapter to estimate its generalization performance.

See also the section about [nested resampling](#nested-resampling) in the chapter on [model optimization](#optimization) when a `r ref("Learner")` involves tuning of hyperparameters.

Furthermore, depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). 

TODO:

- mini-API table
- Gallery posts
- Exercises
