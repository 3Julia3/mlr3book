# Resampling and Benchmarking {#sec-performance}

{{< include _setup.qmd >}}

<!-- first concern: why performance estimation, what is a performance measure -->

In supervised machine learning, we often look for a learning algorithm that produces a well-performing model on a given task. 
We want the model to generalize well to new, unseen data.
For this purpose, we must first decide on a suitable [performance measure](#measures) to assess this so-called generalization performance.
This performance measure usually provides a *score* indicating, e.g., how well the model predictions match the ground truth values and may also reflect other qualities such as the time for training a model.

<!-- motivate the need for splitting the data -->

Using the same data to train and test a model usually leads to an overly optimistic performance estimate.
For example, an overfitted model may fit the data on which it was trained almost perfectly, but typically does not generalize well to new data. 
Assessing its performance only based on the training data would misleadingly suggest a good model.
Hence, to avoid such a optimistically biased performance estimate, we should test the model on an independent dataset not used for training.
However, we typically train the final model on all available data, which leaves no data to estimate its generalization performance.
To solve this problem and obtain a reliable performance estimate, we need to adopt a strategy that mimics the presence of new, unseen data.

<!-- second concern: describe performance estimation procedure and desiderata -->

The easiest strategy is to partition all available data into training and test data.
Then train an intermediate model only on the training data and finally assess its performance on the test data.
Ideally, we want the size of the training data to be close to the size of all available data, as the intermediate model should represent the final model well.
This is because if the training data is too small, the performance estimate may be pessimistically biased, since a learning algorithm usually learns complex relationships in the data worse with less data. 
On the other hand, we also need as much test data as possible to reliably estimate the generalization performance with a sufficiently high amount of data.
As a trade-off, we usually repeat the partitioning process and average the resulting performance estimates to reduce the variance of our estimate. 
This gives rise to [resampling strategies](#resampling) that (repeatedly) partition the available data in different ways (see @fig-ml-abstraction for an illustration).

```{r performance-001, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "A general abstraction of the ML model evaluation process."
#| fig.align: "center"
knitr::include_graphics("images/ml_abstraction.svg")
```

<!-- explain resampling -->
Resampling typically provides more reliable performance estimates by ensuring that the majority of all available data is used at least once for training and testing across all resampling iterations.
Repeatedly splitting the entire data in different ways can help ensure that the resulting performance estimate is not strongly biased by a "lucky" or "unlucky" split. 
For example, a strategy like $k$-fold cross-validation would be preferable over a single train-test split as it makes use of all available data.
Specifically, $k$-fold cross-validation randomly partitions the data into $k$ subsets, called folds.
Then $k$ models are trained on training data consisting of $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations.
The $k$ performance estimates resulting from each fold are then averaged to obtain a more reliable performance estimate.

TODO: add fig to illustrate CV?

TODO: add fig for an overview of other resampling strategies and some remarks on practical guidelines?


<!-- Quick chapter outline -->

In the previous chapter, we have already seen how to manually partition the data contained in a task into a single training set (to train the model) and a single test set (to estimate the generalization performance).
This simple partitioning approach is also known as *holdout*.
In this chapter, we will learn how to estimate the generalization performance of a `r ref("Learner")` using resampling strategies implemented in the `r mlr3` package. 
Additionally, we introduce convenient functions to conduct benchmark experiments and compare the generalization performance of multiple learners across various tasks.


<!-- Specifically, we will cover the following topics: -->

<!-- **Resampling** -->

<!-- [Resampling](#resampling) is a strategy that defines how to partition the available data into training and test data. -->
<!-- We cover how to -->

<!-- * access and select [resampling strategies](#resampling-strategies), -->
<!-- * instantiate the [train-test splits](#resampling-inst) to be performed, and -->
<!-- * run the selected resampling strategy to obtain [resampling results](#resampling-exec). -->

<!-- **Benchmarking** -->

<!-- [Benchmarking](#benchmarking) is used to compare the performance of different learning algorithms applied on various tasks using potentially different resampling strategies and performance measures. -->
<!-- The purpose is to ideally rank the learning algorithms regarding their predictive performance, runtime, or any other performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks. -->
<!-- We cover how to -->

<!-- * construct a [benchmarking design](#bm-design) using `benchmark_grid` to define the benchmark experiments to be performed, -->
<!-- * [run the benchmark experiments](#bm-exec) and aggregate their results, and -->
<!-- * [convert benchmarking objects](#bm-resamp) to other types of objects that can be used for different purposes. -->

<!-- #### Further Reading -->

<!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. -->
<!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). -->
<!-- * [@bischl2012resampling](https://direct.mit.edu/evco/article-abstract/20/2/249/925/Resampling-Methods-for-Meta-Model-Validation-with?redirectedFrom=fulltext) provide an overview of resampling methods. -->

<!-- * Japkowicz, N. and M. Shah (2011). Evaluating Learning Algorithms: A Classification Perspective. Cambridge: Cambridge University Press -->

## Resampling {#sec-resampling}

<!-- [Resampling](#resampling) is a strategy that defines how to partition the available data into training and test data. -->
In this section, we cover how to use `r mlr3` to

* [query](#resampling-strategies) implemented resampling strategies,
* [construct](#resampling-construct) resampling objects for a selected resampling strategy,
* [instantiate](#resampling-inst) the train-test splits of a resampling object on a given task, and
* [execute](#resampling-exec) the selected resampling strategy on a learning algorithm to obtain resampling results.

There are many different resampling strategies for partitioning a dataset into training and test.
`r mlr3` already provides many resampling strategies, so users do not have to implement them from scratch, which can be tedious and error-prone.
The choice of the resampling strategy usually depends on the specific task at hand and the goals of the performance assessment.

TODO: refer to literature?

The following sections provide guidance with concrete code examples on how to select a resampling strategy and then apply it on the `r ref("mlr_tasks_penguins", text = "penguins")` task to estimate the performance of a simple classification tree from the `r ref_pkg("rpart")` package. 
We therefore define the corresponding `r ref("Task")` and `r ref("Learner")` object used througout the chapter as follows:

```{r performance-002}
task = tsk("penguins")
learner = lrn("classif.rpart", predict_type = "prob")
```

### Query Strategies {#resampling-strategies}

<!-- We first need to define which resampling strategy we want to use. -->
All resampling strategies implemented in `r mlr3` can be queried by looking at the `r ref("mlr_resamplings")` dictionary.
The dictionary contains several common resampling strategies, including holdout, (repeated) cross-validation, bootstrap, and subsampling.
Passing the dictionary to the `as.data.table` function provides a more structured output with additional information:

```{r performance-003}
as.data.table(mlr_resamplings)
```

For example, the column `params` shows the parameters of each resampling strategy (e.g., the train-test splitting `ratio` or the number of `repeats`) and the column `iters` shows the default value for the number of performed resampling iterations (i.e., the number of model fits).

### Construction {#resampling-construct}

Once we have decided on a resampling strategy, we have to construct a `r ref("Resampling")` object via the function `r ref("rsmp()")` which will define the resampling strategy we want to employ.
For example, to construct a `r ref("Resampling")` object for holdout, we use the value of the `key` column from the `r ref("mlr_resamplings")` dictionary and pass it to the convenience function `r ref("rsmp()")`:

```{r performance-004}
resampling = rsmp("holdout")
print(resampling)
```

By default, holdout resampling will use 2/3 of the data as training set and 1/3 as test set.
However, we can change this by specifying the `ratio` parameter for holdout either during construction or by updating the `ratio` parameter afterwards.
For example, we construct a `r ref("Resampling")` object for holdout with a 80:20 split (see first line in the code below) then update to 50:50 (see second line in the code below):

```{r performance-005}
resampling = rsmp("holdout", ratio = 0.8)
resampling$param_set$values = list(ratio = 0.5)
```

<!-- Selecting a resampling strategy will typically be motivated by the context defined by the task(s) and learner(s) to be evaluated. -->

Holdout only estimates the generalization performance using a single test set.
To obtain a more reliable performance estimate by making use of all available data, we may use other resampling strategies.
<!-- For example, $k$-fold cross-validation randomly partitions the data into $k$ subsets, called folds. -->
<!-- In total, $k$ models are trained on $k-1$ of the folds, with the remaining fold being used as test data exactly once in each of the $k$ iterations. -->
<!-- The $k$ performance estimates from each fold are then averaged to obtain a more reliable performance estimate. -->
<!-- Several variations of cross-validation exist, including repeated k-fold cross-validation (`repeated_cv`) and leave-one-out cross-validation (`loo`).  -->
<!-- The specific variation used depends on properties of the task at hand and the goals of the performance assessment. -->
<!-- Cross-validation, for example, provides multiple folds test will result in more training and prediction steps, i.e. one for each `fold`, which provides more data points for performance estimation. -->
For example, we could also set up a 10-fold cross-validation via

```{r}
resampling = rsmp("cv", folds = 10)
```

By default, the `$is_instantiated` field of a `r ref("Resampling")` object constructed as shown above is set to `FALSE`.
This means that the resampling strategy is not yet applied to a task, i.e., the train-test splits are not contained in the `r ref("Resampling")` object. 
In the next section, we 

<!-- When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy. -->

<!-- In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue. -->
<!-- If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly. -->


### Instantiation {#resampling-inst}

<!-- In this section, we show how to instantiate a resampling strategy (i.e., how to generate the train-test splits) by applying it to a task. -->
<!-- To obtain the row indices for the training and the test splits, we need to call the `instantiate()` method on a `r ref("Task")` object. -->
<!-- The resulting train-test indices are then stored in the `r ref("Resampling")` object: -->

To generate the train-test splits for a given task, we need to instantiate a resampling strategy by calling the `instantiate()` method of the previously constructed `r ref("Resampling")` object on a `r ref("Task")`.
This will manifest a fixed partition and store the row indices for the training and test sets directly in the `r ref("Resampling")` object. 
We can access these rows via the `$train_set()` and `$test_set()` methods:

```{r performance-006}
resampling = rsmp("holdout", ratio = 0.8)
resampling$instantiate(task)
train_ids = resampling$train_set(1)
test_ids = resampling$test_set(1)
str(train_ids)
str(test_ids)
```

Note that for a fair comparison of multiple [learners](#learners), it is important to use the same instantiated `r ref("Resampling")` object for each learner. 
This will ensure that each learner uses the same training data to build a model and that the performance of the trained model is then assessed using the same test set. 
<!-- In @sec-benchmarking, we will explore this process in detail. -->


### Execution {#resampling-exec}

<!-- With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations. -->
<!-- For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object. -->

Calling the function `r ref("resample()")` on a `r ref("Task")`, `r ref("Learner")` and the constructed `r ref("Resampling")` object will return a `r ref("ResampleResult")` object which contains all information needed to estimate the generalization performance.
Specifically, the function will internally use the `r ref("Learner")` to train a model for each training set determined by the resampling strategy and stores the model predictions of each test set.
By default, these models are discarded after the prediction step to reduce memory consumption of the resulting `r ref("ResampleResult")` object and because we usually only need the stored predictions to calculate the performance measure.
However, we can configure the `r ref("resample()")` function to keep the fitted models (e.g. if we want to use or inspect the intermediate models later) by setting the `store_models` argument of the `r ref("resample()")` function to `TRUE`:

```{r performance-007}
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

Here, we used 3-fold cross-validation as resampling strategy.
The resulting `r ref("ResampleResult")` object (here stored as `rr` object) provides various methods to access and aggregate the stored information.
The two most relevant methods are `$score()` and `$aggregate()`:

The `$score()` method uses a `r ref("Measure")` to calculate the performance measure for each resampling iteration. 
By default, the classification error (`classif.ce`) is used in case of classification tasks and the mean squared error (`regr.mse``) in case of regression tasks. 
In the code example below, we explicitly use the classification accuracy as performance measure and pass it to the `$score()` method:

```{r}
acc = rr$score(msr("classif.acc"))
acc[, .(iteration, classif.acc)]
```

Similarly, we can pass a performance measure to the `$aggregate()` method to calculate the average performance across all resampling iterations:

```{r performance-008}
#| eval: true
rr$aggregate(msr("classif.acc"))
```

This aggregate score is the generalization performance of our selected learner on the given task estimated by the resampling strategy defined in the `r ref("Resampling")` object.
The performance results in each resampling iteration returned by the `$score()` method may be useful to inspect if one (or more) of the iterations lead to very different performance results from the average.

TODO: Do we need all of the "documentation" stuff below?

Below, we show some other fields and methods of the `r ref("ResampleResult")` object which may be useful:

- Check for warnings or errors by looking at the `$warnings` and `$errors` fields.

- Access the `r ref("Resampling")` object by the `$resampling` field which allows to extract information on the train-test splits of the employed resampling strategy. 
For example, we can extract the total number of performed resampling iterations via `$resampling$iters` or the row indices used for training and testing in each resampling iteration via `$resampling$train_set(i)` and `$resampling$test_set(i)` methods, where `i` refers to the `i`-th resampling iteration:

```{r performance-010}
rr$resampling$iters
str(rr$resampling$train_set(1))
str(rr$resampling$test_set(1))
```

- Retrieve and inspect the model trained in a specific resampling iteration (if we have set the argument `store_models = TRUE`) via `$learners[[i]]$model` where `i` refers to the `i`-th resampling iteration:

```{r performance-011}
rr$learners[[1]]$model
```

- Extract a list of prediction objects of each model trained in each iteration via `$predictions()` (e.g. to assess the performance on all prediction objects individually and then average them to obtain a macro averaged performance estimate) or to extract a single prediction object that combines all predictions via `$prediction()` (e.g. to assess the performance on the combined prediction object to obtain a micro averaged performance estimate).

- Filter the result and keep only results of certain resampling iterations, e.g., use `$filter(c(1, 3))` to discard the results of the second resampling iteration.


### Custom resampling {#resamp-custom}

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study.
A custom resampling strategy can be constructed using `rsmp("custom")`, which involves defining the row indices used for training and testing manually to instantiate it on a task.
In the example below, we construct a custom holdout resampling strategy by manually assigning row indices to the `$train` and `$test` fields.

```{r performance-014}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:50, 151:333)),
  test = list(51:150)
)
```

The resulting `r `ref("Resampling")` object can then be used like all other resampling strategies.
To show that both sets contain the row indices we have defined, we can inspect the instantiated `r ref("Resampling")` object:

```{r performance-015}
str(resampling$train_set(1))
str(resampling$test_set(1))
```

The above is equivalent to a custom train-test split analogous to the holdout strategy.
A custom version of the cross-validation strategy can be constructed using `rsmp("custom")`.
The important difference is that we have to specify either a custom `factor` variable (using the as `f` argument of the `$instantiate()` method) or a `factor` column (using the as `col` argument of the `$instantiate()` method) from the data to determine the folds.

<!-- We can then assign custom folds either by providing either a `factor` variable as `f`, or a task column to be used for splitting as `col`. -->

In the example below, we instantiate a custom 4-fold cross-validation strategy using a custom `factor` variable called `folds` that contains 4 equally sized factor levels (each with one quarter of the total size of the `"penguin"` task) to define the 4 folds:

```{r performance-017}
custom_cv = rsmp("custom_cv")
folds = as.factor(rep(1:4, each = task$nrow/4))
custom_cv$instantiate(task, f = folds)
custom_cv
```


### Resampling with (predefined) groups

In some cases, it is desirable to keep observations together when the data is split into corresponding training and test sets, especially when a set of observations naturally belongs together (e.g., when the data contains repeated measurements of individuals). 
In such settings, observations usually belong to groups, and we either want to ensure that all observations of the same group belong to the training set or to the test set.
In `r mlr3`, we can assign a special column role to a feature contained in the data already during task construction. 
For example, the column role `"group"` specifies which column in the data should be used to define the group structure of the observations (see also the help section on `r ref("Resampling")` for more information on the column role `"group"`).

A possible use case for the need of grouping (or blocking) of observations during resampling is spatiotemporal modeling, where observations inherit a natural grouping, either in space or time or in both space and time that need to be considered during resampling.

:::{.callout-tip appearance="simple"}
Besides the `"group"` column role, dedicated spatiotemporal resampling methods are available in `r mlr3spatiotempcv`.
More general information about this topic can be found in the [spatiotemporal resampling](special.html#spatiotemp-cv) section.
:::

Another column role available in `r mlr3` is `"stratum"` which ensures that the observations in the training and test set are similarly distributed as in the complete task.
One or multiple columns can be used for stratification via the column role `stratum`.
See also the [mlr3gallery post on this topic](https://mlr3gallery.mlr-org.com/posts/2020-03-30-stratification-blocking/) for a practical introduction.


### Plotting Resample Results {#autoplot-resampleresult}

`r mlr3viz` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method to automatically visualize the resampling results either in a boxplot or histogram :

```{r performance-018}
resampling = rsmp("bootstrap")
rr = resample(task, learner, resampling, store_models = TRUE)

library(mlr3viz)
autoplot(rr, measure = msr("classif.acc"), type = "boxplot")
autoplot(rr, measure = msr("classif.acc"), type = "histogram")
```

We can also visualize a 2-dimensional prediction surface of individual models in each resampling iteration if the task is restricted to two features:

```{r performance-019}
task$select(c("bill_length", "flipper_length"))
resampling = rsmp("cv", folds = 3)
rr = resample(task, learner, resampling, store_models = TRUE)
autoplot(rr, type = "prediction")
```

:::{.callout-tip}
In case of a binary classification task, you can also use `type = "roc"` or `type = "prc"` to obtain ROC or Precision-Recall curves, respectively. 
For more information see the help page of `r ref("autoplot.ResampleResult()")`.
:::

## Benchmarking {#sec-benchmarking}

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task.
This operation is usually referred to as "benchmarking" in the field of machine learning.
The `r mlr3` package offers the `r ref("benchmark()")` convenience function that takes care of most of the work of repeatedly training and evaluating models under the same conditions.

### Design Construction {#bm-design}

Benchmark experiments in `r mlr3` are specified through a design.
Such a design is essentially a table of scenarios to be evaluated; in particular unique combinations of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` triplets.

We use the `r ref("benchmark_grid()")` function to construct an exhaustive design (that evaluates each learner on each task with each resampling) and instantiate the resampling properly, so that all learners are executed on the same train/test split for each tasks.
We use `r ref("tsks()")`, `r ref("lrns()")`, and `r ref("rsmps()")` to retrieve lists of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")`.
To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure.
We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data.


```{r performance-020}
library("mlr3verse")

design = benchmark_grid(
  tasks = tsks(c("spam", "german_credit", "sonar")),
  learners = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
    predict_type = "prob", predict_sets = c("train", "test")),
  resamplings = rsmps("cv", folds = 3)
)
```

The constructed design table can be passed to `r ref("benchmark()")` to start the computation.
It is also possible to construct a custom design manually, for example, to exclude certain task-learner combinations.

Note that if you construct a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design.

```{r performance-021}
#| echo: false

# Creating a grid using a cross join
design_manual = data.table::CJ(
  task = tsks(c("spam", "german_credit", "sonar")),
  learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
                 predict_type = "prob", predict_sets = c("train", "test")),
  resampling = rsmps("cv", folds = 3),
  sorted = FALSE
)

# Manually remove e.g. the third combination from the grid
design_manual = design_manual[-3]

# Manually instantiate the resamplings
Map(function(task, resampling) {
  resampling$instantiate(task)
}, task = design_manual$task, resampling = design_manual$resampling)
```


### Execution Aggregation of Results {#bm-exec}

After the [benchmark design](#bm-design) is ready, we can call `r ref("benchmark()")` on it:

```{r performance-022}
bmr = benchmark(design)
```

:::{.callout-tip}
Note that we did not have to instantiate the resampling manually.
`r ref("benchmark_grid()")` took care of it for us: each resampling strategy is instantiated once for each task during the construction of the exhaustive grid.
This way, each learner operates on the same training and test sets which makes the results easier to compare.
:::

Once the benchmarking is finished (and, depending on the size of your design, this can take quite some time), we can aggregate the performance with the `$aggregate()` method of the returned `r ref("BenchmarkResult")`.
We construct two measures to calculate the area under the curve (AUC) for the training and the test set:

```{r performance-023}
measures = list(
  msr("classif.auc", predict_sets = "train", id = "auc_train"),
  msr("classif.auc", id = "auc_test")
)

tab = bmr$aggregate(measures)
print(tab[, .(task_id, learner_id, auc_train, auc_test)])
```

We can aggregate the results even further.
For example, we might be interested to know which learner performed best across all tasks.
Simply aggregating the performances with the mean is usually not statistically sound.
Instead, we calculate the rank statistic for each learner, grouped by task.
Then the calculated ranks, grouped by the learner, are aggregated with the `r ref_pkg("data.table")` package.
As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$.

```{r performance-024}
library("data.table")
# group by levels of task_id, return columns:
# - learner_id
# - rank of col '-auc_train' (per level of learner_id)
# - rank of col '-auc_test' (per level of learner_id)
ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]
print(ranks)

# group by levels of learner_id, return columns:
# - mean rank of col 'rank_train' (per level of learner_id)
# - mean rank of col 'rank_test' (per level of learner_id)
ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id]

# print the final table, ordered by mean rank of AUC test
ranks[order(mrank_test)]
```

Unsurprisingly, the featureless learner has lowest performance overall.
The winner is the classification forest, which outperforms a single classification tree.

### Plotting Benchmark Results {#autoplot-benchmarkresult}

Similar to [tasks](#autoplot-task), [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), `r mlr3viz` also provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method for benchmark results.

```{r performance-025}
autoplot(bmr) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

Such a plot gives a nice overview of the overall performance and how learners compare on different tasks in an intuitive way.

We can also plot ROC (receiver operating characteristics) curves.
We filter the `r ref("BenchmarkResult")` to only contain a single `r ref("Task")`, then we simply plot the result:

```{r performance-026}
bmr_small = bmr$clone(deep = TRUE)$filter(task_id = "german_credit")
autoplot(bmr_small, type = "roc")
```

All available plot types are listed on the manual page of `r ref("autoplot.BenchmarkResult()")`.

### Statistical Tests

The package `r mlr3benchmark` provides some infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")`.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported for benchmarks with at least two tasks and at least two learners.
The results can be summarized in Critical Difference Plots.

```{r performance-027}
library("mlr3benchmark")

bma = as.BenchmarkAggr(bmr, measures = msr("classif.auc"))
bma$friedman_posthoc()
autoplot(bma, type = "cd")
```

### Extracting ResampleResults {#bm-resamp}

A `r ref("BenchmarkResult")` object is essentially a collection of multiple `r ref("ResampleResult")` objects.
As these are stored in a column of the aggregated `r ref("data.table()")`, we can easily extract them:

```{r performance-028}
tab = bmr$aggregate(measures)
rr = tab[task_id == "german_credit" & learner_id == "classif.ranger"]$resample_result[[1]]
print(rr)
```

We can now investigate this resampling and even single resampling iterations using one of the approaches shown in [the previous section on resampling](#resampling):

```{r performance-029}
measure = msr("classif.auc")
rr$aggregate(measure)

# get the iteration with worst AUC
perf = rr$score(measure)
i = which.min(perf$classif.auc)

# get the corresponding learner and training set
print(rr$learners[[i]])
head(rr$resampling$train_set(i))
```

### Converting and Merging

A `r ref("ResampleResult")` can be converted to a `r ref("BenchmarkResult")` with the function `r ref("as_benchmark_result()")`.
We can also combine two `r ref("BenchmarkResult", text = "BenchmarkResults")` into a larger result object, for example, for two related benchmarks that are computed on different machines.

```{r performance-030}
task = tsk("iris")
resampling = rsmp("holdout")$instantiate(task)

rr1 = resample(task, lrn("classif.rpart"), resampling)
rr2 = resample(task, lrn("classif.featureless"), resampling)

# Cast both ResampleResults to BenchmarkResults
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

# Merge 2nd BMR into the first BMR
bmr = c(bmr1, bmr2)
print(bmr)
```

## Conclusion

If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. 
In chapter [pipelines](pipelines), we introduce the `r mlr3pipelines` package that solves this issue by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline. 
As the pipeline itself behaves like a `r ref("Learner")`, we can use all functions introduced in this chapter to estimate its generalization performance.

See also the section about [nested resampling](#nested-resampling) in the chapter on [model optimization](#optimization) when a `r ref("Learner")` involves tuning of hyperparameters.

Furthermore, depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). 

- mini-API table
- Gallery posts
- Exercises
