# Resampling and Benchmarking {#sec-performance}

{{< include _setup.qmd >}}

<!-- first concern: why performance estimation, what is a performance measure -->

In supervised machine learning, we often look for a learning algorithm that produces a well-performing model on a given task. 
We want the model to generalize well to new, unseen data and must first choose a suitable [performance measure](#measures) to assess this so-called generalization performance. 
This measure usually calculates a *score* indicating, e.g., how well the model predictions match the ground truth values and may also reflect other qualities such as its runtime.

<!-- motivate the need for splitting the data -->

Using the same data to train and test a model usually leads to an overly optimistic performance estimate, especially for an overfitted model that does not generalize well to new data. 
Hence, to avoid this bias, we need to test the model on an independent dataset that was not used for training.
However, the final model is typically trained on all available data, leaving no data to estimate its generalization performance.
To solve this problem, we need a strategy that mimics the presence of new, unseen data.

<!-- second concern: describe performance estimation procedure -->

The easiest strategy is to partition all available data into training and test data.
We then train an intermediate model only on the training data and assess its performance on the test data.
Ideally, the size of the training data should be similar to all available data, as we want the intermediate model to be a good representative of the final model.
If the training data is too small, the performance estimate may be pessimistically biased because the learning algorithm uses less data, making it difficult to learn complex relationships. 
On the other hand, we also need as much test data as possible to reliably estimate the generalization performance with a sufficient high amount of data.
We usually repeat the partitioning process and average the resulting performance estimates to reduce the variance of our estimate. 
This gives rise to [resampling strategies](#resampling) that (repeatedly) partition the available data in different ways (see @fig-ml-abstraction for an illustration).
Resampling typically provides more reliable performance estimates by ensuring that the majority of all available data is used at least once for training and testing across all repetitions.
Repeatedly splitting the entire data in different ways can help ensure that the resulting performance estimate is not biased by a "lucky" or "unlucky" split. 
For example, a strategy like 10-fold cross-validation would be preferrable over a single train-test split if the size of the data allows it.
<!-- TODO: add figure for an overview and practical guidelines? -->

```{r performance-001, echo=FALSE}
#| label: fig-ml-abstraction
#| fig-cap: "A general abstraction of the ML model evaluation process."
#| fig.align: "center"
knitr::include_graphics("images/ml_abstraction.svg")
```

<!-- Quick chapter outline -->

In the previous chapter, we have already seen how to manually partition the data contained in a task into a single training set (to train the model) and a single test set (to estimate the generalization performance).
This simple partitioning approach is also known as *holdout*.
In this chapter, we will learn how to estimate the generalization performance of a `r ref("Learner")` using resampling strategies implemented in the `r mlr3` package. 
Additionally, we introduce convenient functions to conduct benchmark experiments and compare the generalization performance of multiple learners across various tasks.


<!-- Specifically, we will cover the following topics: -->

<!-- **Resampling** -->

<!-- [Resampling](#resampling) is a strategy that defines how to partition the available data into training and test data. -->
<!-- We cover how to -->

<!-- * access and select [resampling strategies](#resampling-strategies), -->
<!-- * instantiate the [train-test splits](#resampling-inst) to be performed, and -->
<!-- * run the selected resampling strategy to obtain [resampling results](#resampling-exec). -->

<!-- **Benchmarking** -->

<!-- [Benchmarking](#benchmarking) is used to compare the performance of different learning algorithms applied on various tasks using potentially different resampling strategies and performance measures. -->
<!-- The purpose is to ideally rank the learning algorithms regarding their predictive performance, runtime, or any other performance measure of interest and to identify the best learning algorithms for a certain task or across various tasks. -->
<!-- We cover how to -->

<!-- * construct a [benchmarking design](#bm-design) using `benchmark_grid` to define the benchmark experiments to be performed, -->
<!-- * [run the benchmark experiments](#bm-exec) and aggregate their results, and -->
<!-- * [convert benchmarking objects](#bm-resamp) to other types of objects that can be used for different purposes. -->

<!-- #### Further Reading -->

<!-- * If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. [Pipelines](pipelines) solve this by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline that behaves like a learning algorithm. -->
<!-- * Depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). -->
<!-- * [@bischl2012resampling](https://direct.mit.edu/evco/article-abstract/20/2/249/925/Resampling-Methods-for-Meta-Model-Validation-with?redirectedFrom=fulltext) provide an overview of resampling methods. -->

<!-- * Japkowicz, N. and M. Shah (2011). Evaluating Learning Algorithms: A Classification Perspective. Cambridge: Cambridge University Press -->

## Resampling {#sec-resampling}

<!-- [Resampling](#resampling) is a strategy that defines how to partition the available data into training and test data. -->
In this section, we cover how to

* access and select [resampling strategies](#resampling-strategies),
* instantiate the [train-test splits](#resampling-inst) to be performed, and
* run the selected resampling strategy to obtain [resampling results](#resampling-exec).

We will use `r ref("Resampling")` objects that encode the resampling strategy we want to employ to estimate the generalization performance.
Calling the function `r ref("resample()")` on a `r ref("Task")`, `r ref("Learner")` and a `r ref("Resampling")` object will return a `r ref("ResampleResult")` object which contains the performance results.

<!-- When evaluating the performance of a model, we are interested in its generalization performance -- how well will it perform on new data that has not been seen during training? -->
<!-- We can estimate the generalization performance by evaluating a model on a test set, as we have done above, that was constructed to contain only observations that are not contained in the training set. -->

There are many different strategies for partitioning a data set into training and test.
In `r mlr3` we call these strategies "resampling".
Without the resampling strategies implemented in `r mlr3`, writing your own code to perform resampling would be a tedious and error-prone process.

The following sections provide guidance on how to select a resampling strategy and how to use it.

### Strategies {#resampling-strategies}

We will use the `r ref("mlr_tasks_penguins", text = "penguins")` task and a simple classification tree from the `r ref_pkg("rpart")` package as an example here.

```{r performance-002}
task = tsk("penguins")
learner = lrn("classif.rpart")
```

We first need to define which resampling strategy to use.
All resampling strategies implemented in `r mlr3` can be queried by looking at the `r ref("mlr_resamplings")` dictionary and pass it to `as.data.table` to obtain a structured output. This will also list the parameters that can be changed to affect the behavior of each resampling strategy (e.g., the number of repetitions):

```{r performance-003}
as.data.table(mlr_resamplings)
```

<!-- What we showed in the [train/predict/score](#train-predict) part is the equivalent of holdout resampling, done manually, so let's consider this one first. -->

We can retrieve resampling strategies from the dictionary `r ref("mlr_resamplings")` with the convenience function `r ref("rsmp()")` using their key:

```{r performance-004}
resampling = rsmp("holdout")
print(resampling)
```

Note that the `$is_instantiated` field is set to `FALSE`.
This means we did not apply the strategy to a dataset yet.

By default we get a .66/.33 split of the data into training and test.
We can specify parameters during construction or update the parameters afterwards, like in the example below where we construct the holdout with a 80:20 split (L1) then update to 50:50 (L2).

```{r performance-005}
rsmp("holdout", ratio = 0.8)
resampling$param_set$values = list(ratio = 0.5)
```

Selecting a resampling strategy will typically be motivated by the context defined by the task(s) and learner(s) to be evaluated.
The holdout strategy for example will result in only one training and one prediction step, resulting in only a single performance estimate and therefore not a good approximation of the generalization error.

Cross-validation on the other hand will result in more training and prediction steps, i.e. one for each `fold`, which provides more data points for performance estimation.

```{r}
rsmp("cv", folds = 10)
```

When the learner in question is very computationally intensive or the task contains large amounts of data, it may not be feasible to apply 10-fold cross-validation, whereas a faster learner likely could and should be evaluated with this strategy.

In our example we're using the quite fast `rpart` learner and the `"penguins"` task with less than 400 observations, where cross-validation should not be an issue.
If we intended to evaluate a large neural network on an image classification task however, we might not have the computational budget to re-train a learner repeatedly.


### Instantiation {#resampling-inst}

So far, we have only chosen a resampling strategy; we now need to instantiate it with data.

To perform the splitting and obtain indices for the training and the test split, the resampling needs a `r ref("Task")`.
By calling the method `instantiate()`, we split the row indices of the task into indices for training and test sets.
These resulting indices are stored in the `r ref("Resampling")` objects.

```{r performance-006}
resampling = rsmp("holdout", ratio = 0.8)

resampling$instantiate(task)
train = resampling$train_set(1)
test = resampling$test_set(1)
str(train)
str(test)

# are they disjoint?
intersect(train, test)

# are all row ids either in train or test?
setequal(c(train, test), task$row_ids)
```

Note that if you want to compare multiple [Learners](#learners) in a fair manner, using the same instantiated resampling for each learner is mandatory.
This ensures each learner gets exactly the same training data and the performance of the trained model is evaluated in exactly the same test set.
In @sec-benchmarking, we will explore this process in detail.


### Execution {#resampling-exec}

With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations.
For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object.
We can tell `r ref("resample()")` to keep the fitted models (for example for later inspection) by setting the `store_models` option to `TRUE`.

Per default, mlr3 discards fitted models after the prediction step to reduce memory consumption of the resulting `r ref("ResampleResult")` object.

```{r performance-007}
task = tsk("penguins")
learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling = rsmp("cv", folds = 3)

rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

Here we use a three-fold cross-validation resampling, which trains and evaluates on three different training and test sets.
The resulting `r ref("ResampleResult")`, stored as `rr`, provides various methods to access and aggregate the stored information.

The two most relevant methods are `$score()` and `$aggregate()`:

`$score()` calculates the performance score for each resampling iteration -- here in terms of classification accuracy:

```{r}
rr$score(msr("classif.acc"))[, .(iteration, classif.acc)]
```


`$aggregate()` calculates the average performance across all resampling iterations, using the same measure:

```{r performance-008}
#| eval: true
rr$aggregate(msr("classif.acc"))
```

This is useful to check if one (or more) of the iterations are very different from the average.
This aggregate score would also be our estimate for the generalization error of our selected learner and task.

Other methods can be useful in various scenarios, for example:

- Check for warnings or errors with `$warnings` and `$errors`

- Extract and inspect the resampling splits; this allows us to see in detail which observations were used for what purpose when:

```{r performance-010}
rr$resampling
rr$resampling$iters
str(rr$resampling$train_set(1))
str(rr$resampling$test_set(1))
```

- Retrieve the model trained in a specific iteration and inspect it, for example, to investigate why the performance in this iteration was very different from the average:

```{r performance-011}
lrn = rr$learners[[1]]
lrn$model
```

- Extract the individual predictions with `$prediction()` for all predictions, or e.g. `$prediction(2)` for only the second iteration

- Filter the result, keep only specified resampling iterations, e.g. `$filter(c(1, 3))` to discard the second iteration.


### Custom resampling {#resamp-custom}

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study.
A manual resampling instance can be constructed using the `"custom"` template.
In the following example, we construct a custom resampling strategy and manually assign row indices to the `$train` and `$test` fields.

```{r performance-014}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:50, 151:333)),
  test = list(51:150)
)
```

We can then check that both sets contain the indices we expect:

```{r performance-015}
str(resampling$train_set(1))
str(resampling$test_set(1))
```

The resulting resampling can be used just like any other strategy.
For example, here we apply it instead of the cross-validation resampling in the previous example:

```{r performance-016}
task = tsk("penguins")
learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")

rr = resample(task, learner, resampling)
```

The above is equivalent to a custom train-test split, analogous to the holdout strategy.
A custom version of the cross-validation strategy can be constructed using the `"custom_cv"` strategy.
The important difference is that we need to supply a task.

We can then assign custom folds either by providing either a `factor` variable as `f`, or a task column to be used for splitting as `col`.

In this example we create a factor `f` with 4 levels, each repeated 86 times to account for the 344 observations in the `"penguin"` task.
The result is a custom cross-validation strategy with 4 folds.

```{r performance-017}
task = tsk("penguins")

custom_cv = rsmp("custom_cv")
f = factor(rep(letters[1:4], each = 86))
custom_cv$instantiate(task, f = f)

custom_cv
```


### Resampling with (predefined) groups

In some cases, it is desirable to keep observations together, i.e., either have all observations of the same group in the training set or have all observations of the same group in the test set.
This can be defined through the column role `"group"` during Task construction, i.e., a column in the data specifies the groups (see also the help section on `r ref("Resampling")` on this column role).

A potential use case for this is spatial modeling, where one wants to account for groups of observations during resampling.

:::{.callout-tip appearance="simple"}
Besides the `"group"` column role, dedicated spatiotemporal resampling methods are available in `r mlr3spatiotempcv`.
More general information about this topic can be found in the [spatiotemporal resampling](special.html#spatiotemp-cv) section.
:::

If you want the ensure that the observations in the training and test set are similarly distributed as in the complete task, one or multiple columns can be used for stratification via the column role `stratum`.
See also the [mlr3gallery post on this topic](https://mlr3gallery.mlr-org.com/posts/2020-03-30-stratification-blocking/) for a practical introduction.


### Plotting Resample Results {#autoplot-resampleresult}

`r mlr3viz` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method for resampling results.
As an example, we construct a binary classification task with two features, perform a resampling with a 10-fold cross-validation and visualize the results:

```{r performance-018}
task = tsk("pima")
task$select(c("glucose", "mass"))
learner = lrn("classif.rpart", predict_type = "prob")
rr = resample(task, learner, rsmp("cv"), store_models = TRUE)
rr$score(msr("classif.auc"))[, .(iteration, classif.auc)]

library(mlr3viz)
# boxplot of AUC values across the 10 folds
autoplot(rr, measure = msr("classif.auc"))

# ROC curve, averaged over 10 folds
autoplot(rr, type = "roc")
```

We can also plot the predictions of individual models:

```{r performance-019}
# learner predictions for the first fold
rr1 = rr$clone()$filter(1)
autoplot(rr1, type = "prediction")
```

:::{.callout-tip}
All available plot types are listed on the manual page of `r ref("autoplot.ResampleResult()")`.
:::

## Benchmarking {#sec-benchmarking}

Comparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task.
This operation is usually referred to as "benchmarking" in the field of machine learning.
The `r mlr3` package offers the `r ref("benchmark()")` convenience function that takes care of most of the work of repeatedly training and evaluating models under the same conditions.

### Design Construction {#bm-design}

Benchmark experiments in `r mlr3` are specified through a design.
Such a design is essentially a table of scenarios to be evaluated; in particular unique combinations of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")` triplets.

We use the `r ref("benchmark_grid()")` function to construct an exhaustive design (that evaluates each learner on each task with each resampling) and instantiate the resampling properly, so that all learners are executed on the same train/test split for each tasks.
We use `r ref("tsks()")`, `r ref("lrns()")`, and `r ref("rsmps()")` to retrieve lists of `r ref("Task")`, `r ref("Learner")` and `r ref("Resampling")`.
To set up the learners, we deviate from the default behavior by setting them to predict probabilities rather than class labels (`predict_type = "prob"`) to allow scoring the results using the AUC measure.
We also set them up to predict for the observations of both the training and test set by (`predict_sets = c("train", "test")`), rather than only making predictions on the test data.


```{r performance-020}
library("mlr3verse")

design = benchmark_grid(
  tasks = tsks(c("spam", "german_credit", "sonar")),
  learners = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
    predict_type = "prob", predict_sets = c("train", "test")),
  resamplings = rsmps("cv", folds = 3)
)
```

The constructed design table can be passed to `r ref("benchmark()")` to start the computation.
It is also possible to construct a custom design manually, for example, to exclude certain task-learner combinations.

Note that if you construct a custom design with `r ref("data.table()")`, the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before constructing the design.

```{r performance-021}
#| echo: false

# Creating a grid using a cross join
design_manual = data.table::CJ(
  task = tsks(c("spam", "german_credit", "sonar")),
  learner = lrns(c("classif.ranger", "classif.rpart", "classif.featureless"),
                 predict_type = "prob", predict_sets = c("train", "test")),
  resampling = rsmps("cv", folds = 3),
  sorted = FALSE
)

# Manually remove e.g. the third combination from the grid
design_manual = design_manual[-3]

# Manually instantiate the resamplings
Map(function(task, resampling) {
  resampling$instantiate(task)
}, task = design_manual$task, resampling = design_manual$resampling)
```


### Execution Aggregation of Results {#bm-exec}

After the [benchmark design](#bm-design) is ready, we can call `r ref("benchmark()")` on it:

```{r performance-022}
bmr = benchmark(design)
```

:::{.callout-tip}
Note that we did not have to instantiate the resampling manually.
`r ref("benchmark_grid()")` took care of it for us: each resampling strategy is instantiated once for each task during the construction of the exhaustive grid.
This way, each learner operates on the same training and test sets which makes the results easier to compare.
:::

Once the benchmarking is finished (and, depending on the size of your design, this can take quite some time), we can aggregate the performance with the `$aggregate()` method of the returned `r ref("BenchmarkResult")`.
We construct two measures to calculate the area under the curve (AUC) for the training and the test set:

```{r performance-023}
measures = list(
  msr("classif.auc", predict_sets = "train", id = "auc_train"),
  msr("classif.auc", id = "auc_test")
)

tab = bmr$aggregate(measures)
print(tab[, .(task_id, learner_id, auc_train, auc_test)])
```

We can aggregate the results even further.
For example, we might be interested to know which learner performed best across all tasks.
Simply aggregating the performances with the mean is usually not statistically sound.
Instead, we calculate the rank statistic for each learner, grouped by task.
Then the calculated ranks, grouped by the learner, are aggregated with the `r ref_pkg("data.table")` package.
As larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$.

```{r performance-024}
library("data.table")
# group by levels of task_id, return columns:
# - learner_id
# - rank of col '-auc_train' (per level of learner_id)
# - rank of col '-auc_test' (per level of learner_id)
ranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]
print(ranks)

# group by levels of learner_id, return columns:
# - mean rank of col 'rank_train' (per level of learner_id)
# - mean rank of col 'rank_test' (per level of learner_id)
ranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id]

# print the final table, ordered by mean rank of AUC test
ranks[order(mrank_test)]
```

Unsurprisingly, the featureless learner has lowest performance overall.
The winner is the classification forest, which outperforms a single classification tree.

### Plotting Benchmark Results {#autoplot-benchmarkresult}

Similar to [tasks](#autoplot-task), [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), `r mlr3viz` also provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method for benchmark results.

```{r performance-025}
autoplot(bmr) +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

Such a plot gives a nice overview of the overall performance and how learners compare on different tasks in an intuitive way.

We can also plot ROC (receiver operating characteristics) curves.
We filter the `r ref("BenchmarkResult")` to only contain a single `r ref("Task")`, then we simply plot the result:

```{r performance-026}
bmr_small = bmr$clone(deep = TRUE)$filter(task_id = "german_credit")
autoplot(bmr_small, type = "roc")
```

All available plot types are listed on the manual page of `r ref("autoplot.BenchmarkResult()")`.

### Statistical Tests

The package `r mlr3benchmark` provides some infrastructure for applying statistical significance tests on `r ref("BenchmarkResult")`.
Currently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported for benchmarks with at least two tasks and at least two learners.
The results can be summarized in Critical Difference Plots.

```{r performance-027}
library("mlr3benchmark")

bma = as.BenchmarkAggr(bmr, measures = msr("classif.auc"))
bma$friedman_posthoc()
autoplot(bma, type = "cd")
```

### Extracting ResampleResults {#bm-resamp}

A `r ref("BenchmarkResult")` object is essentially a collection of multiple `r ref("ResampleResult")` objects.
As these are stored in a column of the aggregated `r ref("data.table()")`, we can easily extract them:

```{r performance-028}
tab = bmr$aggregate(measures)
rr = tab[task_id == "german_credit" & learner_id == "classif.ranger"]$resample_result[[1]]
print(rr)
```

We can now investigate this resampling and even single resampling iterations using one of the approaches shown in [the previous section on resampling](#resampling):

```{r performance-029}
measure = msr("classif.auc")
rr$aggregate(measure)

# get the iteration with worst AUC
perf = rr$score(measure)
i = which.min(perf$classif.auc)

# get the corresponding learner and training set
print(rr$learners[[i]])
head(rr$resampling$train_set(i))
```

### Converting and Merging

A `r ref("ResampleResult")` can be converted to a `r ref("BenchmarkResult")` with the function `r ref("as_benchmark_result()")`.
We can also combine two `r ref("BenchmarkResult", text = "BenchmarkResults")` into a larger result object, for example, for two related benchmarks that are computed on different machines.

```{r performance-030}
task = tsk("iris")
resampling = rsmp("holdout")$instantiate(task)

rr1 = resample(task, lrn("classif.rpart"), resampling)
rr2 = resample(task, lrn("classif.featureless"), resampling)

# Cast both ResampleResults to BenchmarkResults
bmr1 = as_benchmark_result(rr1)
bmr2 = as_benchmark_result(rr2)

# Merge 2nd BMR into the first BMR
bmr = c(bmr1, bmr2)
print(bmr)
```

## Conclusion

If data is pre-processed before feeding it into a learning algorithm, the train-test splits need to be taken into account, i.e., the pre-processing steps should be integrated into the model-building process. 
In chapter [pipelines](pipelines), we introduce the `r mlr3pipeline` package that solves this issue by combining a `r ref("Learner")` with a pre-processing step into a more general machine learning pipeline. 
As the pipeline itself behaves like a `r ref("Learner")`, we can use all functions introduced in this chapter to estimate its generalization performance.

See also the section about [nested resampling](#nested-resampling) in the chapter on [model optimization](#optimization) when a `r ref("Learner")` involves tuning of hyperparameters.

Furthermore, depending on the task at hand, more complex resampling strategies might be required, e.g., for [spatiotemporal data](#spatiotemporal). 

- mini-API table
- Gallery posts
- Exercises
