---
author:
  - name: Przemys≈Çaw Biecek
    orcid: 0000-0001-8423-1823 
    email: przemyslaw.biecek@gmail.com
    affiliations:
      - name: MI2.AI, Warsaw University of Technology
  - name: Author 2
    orcid:
    email:
    affiliations:
      - name: Affiliation 2
abstract: 
  The goal of this chapter is to present key methods that allow in-depth post-hoc analysis of an already trained model. 
  The methods presented are model-agnostic, i.e. they can be applied to models of different classes.
  When using predictive models in practice, it is often the case that a high performance on a validation set is not enough. 
  Users more and more often want to know which variables are important and how they influence the model's predictions. 
  For the end user, such knowledge allows better utilisation of models in the decision-making process, e.g. by analysing different possible decision options. 
  In addition, if the model's behaviour turns out to be in line with the domain knowledge or the user's intuition then the user's confidence in the prediction will increase. 
  For the modeller, an in-depth analysis of the model allows undesirable model behaviour to be detected and corrected.
---

# Model Interpretation {#sec-interpretation}

{{< include _setup.qmd >}}

In principle, all generic frameworks for model interpretation are applicable on the models fitted with `r mlr3` by just extracting the fitted models from the `r ref("Learner")` objects.

However, two of the most popular frameworks additionally come with some convenience for `r mlr3`, these are

* `r ref_pkg("iml")` presented in @sec-iml, and
* `r ref_pkg("DALEX")` presented in @sec-dalex.

Both these packages offer similar functionality, but they differ in design choices. `r ref_pkg("iml")` is based on the R6 class system and for this reason working with it is more similar in style to working with the `r ref_pkg("mlr3")` package. `r ref_pkg("DALEX")` is based on the S3 class system and is mainly focused on the ability to compare multiple different models on the same graph for comparison and on the explainable model analysis process.


## Penguin Task  {#sec-penguin-task}

To understand what model interpretation packages can offer, we start off with a thorough example.
The goal of this example is to figure out the species of penguins given a set of features.
The `r ref("palmerpenguins::penguins")` data set will be used which is an alternative to the `iris` data set.
The `penguins` data sets contains 8 variables of 344 penguins:

```{r interpretation-001, message=FALSE, warning=FALSE}
data("penguins", package = "palmerpenguins")
str(penguins)
```

To get started run:

```{r interpretation-002, message=FALSE, warning=FALSE}
library("mlr3")
library("mlr3learners")
set.seed(1)
```

```{r interpretation-003, message=FALSE, warning=FALSE}
penguins = na.omit(penguins)
task_peng = as_task_classif(penguins, target = "species")
```

`penguins = na.omit(penguins)` is to omit the 11 cases with missing values.
If not omitted, there will be an error when running the learner from the data points that have N/A for some features.

```{r interpretation-004, message=FALSE, warning=FALSE}
learner = lrn("classif.ranger")
learner$predict_type = "prob"
learner$train(task_peng)
learner$model
x = penguins[which(names(penguins) != "species")]
model = Predictor$new(learner, data = x, y = penguins$species)
```

As explained in Section [Learners](#learners), specific learners can be queried with `r ref("mlr_learners")`.
In Section [Train/Predict](#train-predict) it is recommended for some classifiers to use the `predict_type` as `prob` instead of directly predicting a label.
This is what is done in this example.
`penguins[which(names(penguins) != "species")]` is the data of all the features and `y` will be the penguins`species`.
`learner$train(task_peng)` trains the model and `learner$model` stores the model from the training command.
`Predictor` holds the machine learning model and the data.
All interpretation methods in `r ref_pkg("iml")` need the machine learning model and the data to be wrapped in the `Predictor` object.

## iml {#sec-iml}

Author: Shawn Storm

`r ref_pkg("iml")` is an R package that interprets the behavior and explains predictions of machine learning models.
The functions provided in the `r ref_pkg("iml")` package are model-agnostic which gives the flexibility to use any machine learning model.

This chapter provides examples of how to use `r ref_pkg("iml")` with `r mlr3`.
For more information refer to the  [IML github](https://github.com/christophM/iml) and the [IML book](https://christophm.github.io/interpretable-ml-book/)

Next is the core functionality of `r ref_pkg("iml")`.
In this example three separate interpretation methods will be used: [FeatureEffects](https://github.com/christophM/iml/blob/master/R/FeatureEffects.R), [FeatureImp](https://github.com/christophM/iml/blob/master/R/FeatureImp.R) and [Shapley](https://github.com/christophM/iml/blob/master/R/Shapley.R)

* `FeatureEffects` computes the effects for all given features on the model prediction.
  Different methods are implemented: [Accumulated Local Effect (ALE) plots](https://christophm.github.io/interpretable-ml-book/ale.html), [Partial Dependence Plots (PDPs)](https://christophm.github.io/interpretable-ml-book/pdp.html) and [Individual Conditional Expectation (ICE) curves](https://christophm.github.io/interpretable-ml-book/ice.html).

* `Shapley` computes feature contributions for single predictions with the Shapley value -- an approach from cooperative game theory ([Shapley Value](https://christophm.github.io/interpretable-ml-book/shapley.html)).

* `FeatureImp` computes the importance of features by calculating the increase in the model's prediction error after permuting the feature (more [here](https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance)).

### FeatureEffects

In addition to the commands above the following two need to be ran:

```{r interpretation-005, message=FALSE, warning=FALSE, fig.cap='Plot of the results from FeatureEffects. FeatureEffects computes and plots feature effects of prediction models',  fig.align='center'}
library("iml")

num_features = c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "year")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

`effect` stores the object from the `FeatureEffect` computation and the results can then be plotted.
In this example, all of the features provided by the `penguins` data set were used.

All features except for `year` provide meaningful interpretable information.
It should be clear why `year` doesn't provide anything of significance.
`bill_length_mm` shows for example that when the bill length is smaller than roughly 40mm, there is a high chance that the penguin is an Adelie.

### Shapley

```{r interpretation-006, message=FALSE, warning=FALSE, fig.cap='Plot of the results from Shapley. $\\phi$ gives the increase or decrease in probability given the values on the vertical axis',  fig.align='center'}
x = penguins[which(names(penguins) != "species")]
model = Predictor$new(learner, data = penguins, y = "species")
x.interest = data.frame(penguins[1, ])
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley)
```

The $\phi$ provides insight into the probability given the values on the vertical axis.
For example, a penguin is less likely to be Gentoo if the bill\_depth=18.7 is and much more likely to be Adelie than Chinstrap.

### FeatureImp

```{r interpretation-007, message=FALSE, warning=FALSE, fig.cap='Plot of the results from FeatureImp. FeatureImp visualizes the importance of features given the prediction model',  fig.align='center'}
effect = FeatureImp$new(model, loss = "ce")
effect$plot(features = num_features)
```

`FeatureImp` shows the level of importance of the features when classifying the penguins.
It is clear to see that the `bill_length_mm` is of high importance and one should concentrate on different boundaries of this feature when attempting to classify the three species.

### Independent Test Data

It is also interesting to see how well the model performs on a test data set.
For this section, exactly as was recommended in Section [Train/Predict](#train-predict), 80% of the penguin data set will be used for the training set and 20% for the test set:

```{r interpretation-008, message=FALSE, warning=FALSE}
train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)
test_set = setdiff(seq_len(task_peng$nrow), train_set)
learner$train(task_peng, row_ids = train_set)
prediction = learner$predict(task_peng, row_ids = test_set)
```

First, we compare the feature importance on training and test set

```{r interpretation-009, message=FALSE, warning=FALSE, fig.cap='FeatImp on train (left) and test (right)',  fig.align='center'}
# plot on training
model = Predictor$new(learner, data = penguins[train_set, ], y = "species")
effect = FeatureImp$new(model, loss = "ce")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = penguins[test_set, ], y = "species")
effect = FeatureImp$new(model, loss = "ce")
plot_test = plot(effect, features = num_features)

# combine into single plot
library("patchwork")
plot_train + plot_test
```

The results of the train set for `FeatureImp` are very similar, which is expected.
We follow a similar approach to compare the feature effects:

```{r interpretation-010, message=FALSE, warning=FALSE, fig.cap='FeatEffect train data set', fig.align='center'}
model = Predictor$new(learner, data = penguins[train_set, ], y = "species")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

```{r interpretation-011, message=FALSE, warning=FALSE, fig.cap='FeatEffect test data set',  fig.align='center'}
model = Predictor$new(learner, data = penguins[test_set, ], y = "species")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

As is the case with `FeatureImp`, the test data results show either an over- or underestimate of feature importance / feature effects compared to the results where the entire penguin data set was used.
This would be a good opportunity for the reader to attempt to resolve the estimation by playing with the amount of features and the amount of data used for both the test and train data sets of `FeatureImp` and `FeatureEffects`.
Be sure to not change the line `train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)` as it will randomly sample the data again.

## DALEX {#sec-dalex}

The `r ref_pkg("DALEX")` package implements the most common methods for explaining predictive models using post-hoc model agnostic techniques. You can use this package for any model built with the `r ref_pkg("mlr3")` package as well as with other frameworks in R or Python.

The philosophy of working with this package is based on the process of explanatory model analysis described in the [EMA book](https://ema.drwhy.ai/) [@biecek_burzykowski_2021]. In this chapter we present code snippets and general overview of this package based on a model built in the chapter @sec-penguin-task on `r ref("palmerpenguins::penguins")` data.

Once you become familiar with the philosophy of working with the `r ref_pkg("DALEX")` package, you can also use other packages from this family such as `r ref_pkg("fairmodels")` for the bias analysis with the use of different fairness measures, `r ref_pkg("modelStudio")` for interactive model exploration, `r ref_pkg("modelDown")` for the automatic generation of IML model documentation in the form of a report, `r ref_pkg("survex")` for the explanation of survival models, `r ref_pkg("treeshap")` for the analysis of tree based models.


### Explanatory Model Analysis {#sec-interpretability-architecture}

The analysis of a model is usually an interactive process starting with a shallow analysis -- usually a single-number summary. Subsequent steps allow for a systematic deepening of the amount of information about the model by exploring the importance of single variables or pairs of variables to an in-depth analysing of the relationship between selected variable to the model outcome. 

This process can take place for a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we speak of global data analysis. Below, we will present these two scenarios in separate subsections. See @fig-dalex-fig-plot-01 for an overview of key functions that will be discussed.


```{r interpretation-012, echo=FALSE, fig.cap='Taxonomy of methods for model exploration presented in this chapter. Left part overview methods for global level exploration while right part is related to local level model exploration.', out.width = '92%', fig.align='center'}
#| label: fig-dalex-fig-plot-01
knitr::include_graphics("Figures/DALEX_ema_process.png")
```




Working with explanations in the `r ref_pkg("DALEX")` package always consists of three steps schematically shown in the pipe below.

```r
model %>%
  explain_mlr3(data = ..., y = ..., label = ...) %>%
  model_parts() %>%
  plot()
```

1. All functions in the `r ref_pkg("DALEX")` package can work for models with any structure.
  It is possible because in the first step we create an adapter that allows the downstream functions to access the model in a consistent fashion.
  In general, such an adapter is created with `r ref("DALEX::explain.default()")` function, but for models created in the `r mlr3` package it is more convenient to use the `r ref("DALEXtra::explain_mlr3()")`.

1. Explanations are determined by the functions `r ref("DALEX::model_parts()")`, `r ref("DALEX::model_profile()")`, `r ref("DALEX::predict_parts()")` and `r ref("DALEX::predict_profile()")`.
  Each of these functions takes the model adapter as its first argument.
  The other arguments describe how the function works.
  We will present them in the following section.

1. Explanations can be visualized with the generic function `plot` or summarised with the generic function `"print()"`.
  Each explanation is a data frame with an additional class attribute.
  The `plot` function creates graphs using the `r ref_pkg("ggplot2")` package, so they can be easily modified with usual `r ref_pkg("ggplot2")` decorators.

We show this cascade of functions based on the FIFA example.

To get started with the exploration of the model behaviour we need to create an explainer.
`r ref("DALEX::explain.default")` function handles is for all types of predictive models.
In the `r ref_pkg("DALEXtra")` package there generic versions for the most common ML frameworks.
Among them the `r ref("DALEXtra::explain_mlr3()")` function works for `r mlr3` models.

This function performs a series of internal checks so the output is a bit verbose.
Turn the `verbose = FALSE` argument to make it less wordy.

```{r interpretation-019, message=FALSE, warning=FALSE}
library("DALEX")
library("DALEXtra")
library("ggplot2")
#theme_set(theme_ema())

ranger_exp = explain_mlr3(learner,
  data = penguins[test_set, ],
  y = penguins[test_set, "species"],
  label = "Ranger RF",
  colorize = FALSE)
```

### Global level exploration {#sec-interpretability-dataset-level}

```{r interpretation-020a, message=FALSE, warning=FALSE}
model_performance(ranger_exp)

table(apply(predict(ranger_exp, penguins[test_set, ]), 1, which.max), 
      penguins[["species"]][test_set])
```

#### Permutational Variable Importance

The `r ref("DALEX::model_parts()")` function calculates the importance of variables using the [permutations based importance](https://ema.drwhy.ai/featureImportance.html).

```{r interpretation-020, message=FALSE, warning=FALSE}
ranger_effect = model_parts(ranger_exp)
head(ranger_effect)
```

Results can be visualized with generic `plot()`.
The chart for all 38 variables would be unreadable, so with the `max_vars` argument, we limit the number of variables on the plot.

```{r interpretation-021, message=FALSE, warning=FALSE, fig.width=8, fig.height=8}
plot(ranger_effect, show_boxplots = FALSE) + DALEX::theme_ema_vertical()
```

```{r interpretation-022, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5}
plot(ranger_effect, max_vars = 5) + DALEX::theme_ema()
```

#### Partial Dependence Plots

Once we know which variables are most important, we can use [Partial Dependence Plots](https://ema.drwhy.ai/partialDependenceProfiles.html) to show how the model, on average, changes with changes in selected variables.
In this example, they show the average relation between the particular variables and players' value.

```{r interpretation-023, message=FALSE, warning=FALSE}
ranger_profiles = model_profile(ranger_exp)
ranger_profiles
```

Again, the result of the explanation can be presented with the generic function `plot()`.

```{r interpretation-024, message=FALSE, warning=FALSE, fig.width=8, fig.height=8}
plot(ranger_profiles) + DALEX::theme_ema()
```

The general trend for most player characteristics is the same.
The higher are the skills the higher is the player's worth.
With a single exception ‚Äì variable Age.

### Local level explanation {#sec-interpretability-instance-level}

Time to see how the model behaves for a single observation/player.
This can be done for any player, but this example we will use the Cristiano Ronaldo.

#### Break Down Plots

The function `r ref("DALEX::predict_parts()")` is an instance-level version of the `model_parts` function introduced in the previous section.
For the background behind that method see the [Introduction to Break Down](https://ema.drwhy.ai/breakDown.html).

```{r interpretation-025, message=FALSE, warning=FALSE}
steve = penguins[1,]
steve

predict(ranger_exp, steve)
```

The generic `plot()` function shows the estimated contribution of variables to the final prediction.

Cristiano is a striker, therefore characteristics that influence his worth are those related to attack, like `attacking_volleys` or `skill_dribbling`.
The only variable with negative attribution is `age`.

```{r interpretation-026, message=FALSE, warning=FALSE, echo=FALSE}
ranger_attributions = model_parts(ranger_exp)
```

```{r interpretation-027, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5}
plot(ranger_attributions, show_boxplots = FALSE) + DALEX::theme_ema_vertical()
```

#### Shapley Values

Another way to inspect the local behaviour of the model is to use [SHapley Additive exPlanations (SHAP)](https://ema.drwhy.ai/shapley.html).
It locally shows the contribution of variables to a single observation, just like Break Down.

```{r interpretation-028, message=FALSE, warning=FALSE, fig.width=8, fig.height=4.5}
ranger_shap = predict_parts(ranger_exp, 
             new_observation = steve, 
             type = "shap")

plot(ranger_shap) + DALEX::theme_ema_vertical()
```

#### Ceteris Paribus Plots

In the previous section, we've introduced a global explanation - Partial Dependence Plots.
[Ceteris Paribus](https://ema.drwhy.ai/ceterisParibus.html) is the instance level version of that plot.
It shows the response of the model for observation when we change only one variable while others stay unchanged.
Blue dot stands for the original value.

```{r interpretation-029, message=FALSE, warning=FALSE, fig.width=8, fig.height=8}
ranger_ceteris = predict_profile(ranger_exp, steve)
plot(ranger_ceteris) + DALEX::theme_ema()
```
