---
author:
  - name: Przemysław Biecek
    orcid: 0000-0001-8423-1823 
    email: przemyslaw.biecek@gmail.com
    affiliations:
      - name: MI2.AI, Warsaw University of Technology
  - name: Susanne Dandl
    orcid: 0000-0003-4324-4163
    email: dandls.datascience@gmail.com
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract: 
  The goal of this chapter is to present key methods that allow in-depth posthoc analysis of an already trained model. 
  The methods presented are model-agnostic, i.e. they can be applied to models of different classes.
  When using predictive models in practice, it is often the case that high performance on a validation set is not enough. 
  Users more and more often want to know which variables are important and how they influence the model's predictions. 
  For the end user, such knowledge allows better utilisation of models in the decision-making process, e.g. by analysing different possible decision options. 
  In addition, if the model's behaviour turns out to be in line with the domain knowledge or the user's intuition then the user's confidence in the prediction will increase. 
  For the modeller, an in-depth analysis of the model allows undesirable model behaviour to be detected and corrected.
---

# Model Interpretation {#sec-interpretation}

{{< include _setup.qmd >}}

Predictive models have numerous applications in virtually every area of life. The increasing availability of data and frameworks to create models has allowed the widespread adoption of these solutions. However, this does not always go together with enough testing of the models and the consequences of incorrect predictions can be severe. The bestseller book ,,Weapons of Math Destruction'' [@ONeil] discusses examples of deployed black-boxes that have led to wrong-headed decisions, sometimes on a massive scale. So what can we do to make our models more thoroughly tested? The answer is methods that allow deeper interpretation of predictive models. In this chapter, we will provide illustrations of how to perform the most popular of these methods [@Holzinger2022].

In principle, all generic frameworks for model interpretation apply to the models fitted with `r mlr3` by just extracting the fitted models from the `r ref("Learner")` objects.

However, two of the most popular frameworks additionally come with some convenience for `r mlr3`, these are

-   `r ref_pkg("iml")` presented in @sec-iml, and
-   `r ref_pkg("DALEX")` presented in @sec-dalex.

Both these packages offer similar functionality, but they differ in design choices. `r ref_pkg("iml")` is based on the R6 class system and for this reason working with it is more similar in style to working with the `r ref_pkg("mlr3")` package. `r ref_pkg("DALEX")` is based on the S3 class system and is mainly focused on the ability to compare multiple different models on the same graph for comparison and on the explainable model analysis process.

<!-- -   `FeatureEffects` computes the effects for all given features on the model prediction. Different methods are implemented: [Accumulated Local Effect (ALE) plots](https://christophm.github.io/interpretable-ml-book/ale.html), [Partial Dependence Plots (PDPs)](https://christophm.github.io/interpretable-ml-book/pdp.html) and [Individual Conditional Expectation (ICE) curves](https://christophm.github.io/interpretable-ml-book/ice.html). -->

<!-- -   `Shapley` computes feature contributions for single predictions with the Shapley value -- an approach from cooperative game theory ([Shapley Value](https://christophm.github.io/interpretable-ml-book/shapley.html)). -->

<!-- -   `FeatureImp` computes the importance of features by calculating the increase in the model's prediction error after permuting the feature (more [here](https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance)). -->


## Penguin Task {#sec-penguin-task}

To understand what model interpretation packages can offer, we start with a thorough example. The goal of this example is to figure out the species of penguins given a set of features. The `r ref("palmerpenguins::penguins")` [@palmerpenguins2020] data set will be used which is an alternative to the `iris` data set. The `penguins` data sets contain 8 variables of 344 penguins:

```{r interpretation-001, message=FALSE, warning=FALSE}
data("penguins", package = "palmerpenguins")
str(penguins)
```

To get started run:

```{r interpretation-002, message=FALSE, warning=FALSE}
library("mlr3")
library("mlr3learners")
set.seed(1)
```

```{r interpretation-003, message=FALSE, warning=FALSE}
penguins = na.omit(penguins)
task_peng = as_task_classif(penguins, target = "species")
```

`penguins = na.omit(penguins)` is to omit the 11 cases with missing values. If not omitted, there will be an error when running the learner from the data points that have N/A for some features.

```{r interpretation-004, message=FALSE, warning=FALSE}
learner = lrn("classif.ranger")
learner$predict_type = "prob"
learner$train(task_peng)
learner$model
x = penguins[which(names(penguins) != "species")]
```

As explained in Section [Learners](#learners), specific learners can be queried with `r ref("mlr_learners")`. In Section [Train/Predict](#train-predict) it is recommended for some classifiers to use the `predict_type` as `prob` instead of directly predicting a label. This is what is done in this example. `penguins[which(names(penguins) != "species")]` is the data of all the features and `y` will be the penguins`species`. `learner$train(task_peng)` trains the model and `learner$model` stores the model from the training command. 

## iml {#sec-iml}

Authors: Shawn Storm, Susanne Dandl

The `r ref_pkg("iml")` R package was written by the author of the [interpretable machine learning book](https://christophm.github.io/interpretable-ml-book/) and covers the most popular, post-hoc interpretation tools.
The methods are all model-agnostic which gives the flexibility to use any machine learning model.
Like the `r ref_pkg("mlr3")` package, the package is based on the R6 class system. 
The implemented methods internally originate from the same parent class and use the same processing framework. Thus, calls to each method follow the same syntax and also the output and functionalities are consistent (for example, all methods have a `$plot()` method). This makes it easy to analyse machine learning models using multiple interpretation tools. 

In the following, we provide examples on how to use the `r ref_pkg("iml")` package to give insights into the random forest model we fitted above with the `r ref_pkg("mlr3")` package.
Therefore, we use the methods from the introduction: permutation feature importance and partial dependence plots.
We also briefly discuss which additional methods are available in the `r ref_pkg("iml")` package, namely, Shapley values, and global as well as local surrogate models.

### The Predictor object
In order for the interpretation methods in the `r ref_pkg("iml")` package to process machine learning models (for classification or regression) fitted by *any* R package, 
fitted models need to be wrapped in a `r ref("iml::Predictor")` object.
A `r ref("iml::Predictor")` contains the prediction model and the data used for analyzing the model.

Depending on the insights we want to get from the interpretation method or the question we want to answer with the it, 
the data set can be the training data or hold-out/test data (see Section <FIXME: add section> for details).
For now, we just reuse the training data.

```{r iml-Predictor, message=FALSE, warning=FALSE, fig.cap='Plot of the results from FeatureEffects. FeatureEffects computes and plots feature effects of prediction models',  fig.align='center'}
library("iml")
model = Predictor$new(learner, data = x, y = penguins$species)
```

`r ref("iml::Predictor")` also has an (optional) input argument `predict.function` which requires a function that predicts on new data. 
This argument only needs to be specified if the model was *not* built with the `r ref_pkg("mlr3")`, `r ref_pkg("mlr")` or `r ref_pkg("caret")` package.
Since the random forest (`learner`) was fitted with `r ref_pkg("mlr3")`, `predict.function` is already implemented in the `r ref_pkg("iml")` package and does not need to be specified by us.

### Permutation Feature Importance

First, we analyse which features are the most important features to classify penguins according to the permutation feature importance.
The importance of a feature is measured as the factor by which the model's prediction error increases when the feature is shuffled.
We initialize a `r ref("iml::FeatureImp")` object with the `model` as an input and the loss 

```{r interpretation-007, message=FALSE, warning=FALSE, fig.cap='Feature importances computed with the PFI method implemented in `iml::FeatureImp` for the penguin classification task and random forest model.',  fig.align='center'}
effect = FeatureImp$new(model, loss = "ce")
effect$plot(features = num_features)
```

With the `$plot()` method, we can visualize the importances. 
We see that the bill length is the most important feature: if we permute the column bill length in the training data, 
the classification error of our model increases on average by a factor of around 90.
We say on average because `r ref("iml::FeatureImp")` repeats per default the shuffling process 5 times and each time the classification error is computed. The point on the plot for bill length displays the median values of the 5 resulting error values and the boundaries of the error bar are equal to the 5 \% and 95 \% quantiles of the error values.

### FeatureEffects

Next, we inspect *how* bill length influences the penguin classification.
Therefore, we compute feature effects using a partial dependence plot (PDP).

```{r interpretation-pdp, message=FALSE, warning=FALSE, fig.cap='Feature effect of bill length computed with the PDP method implemented in `iml::FeatureEffect` for the penguin classification task and random forest model.',  fig.align='center'}
effect = FeatureEffect$new(model, feature = "bill_length_mm", method = "pdp")
effect$plot()
```

Again, we used the `$plot()` method to visualize the results. We see that when the bill length is smaller than roughly 40 mm, there is a high chance that the penguin is an Adelie.

<!-- We can also compute and visualize the feature effects of all numeric features at once with `r ref("iml::FeatureEffects")`.  -->
<!-- ```{r interpretation-pdp2, message=FALSE, warning=FALSE, fig.cap='Feature effects of all numeric features computed with the PDP method implemented in `iml::FeatureEffect` for the penguin classification task and random forest model.',  fig.align='center'} -->
<!-- num_features = c("bill_length_mm", "bill_depth_mm", "flipper_length_mm", "body_mass_g", "year") -->
<!-- effect = FeatureEffects$new(model, features = num_features, method = "pdp") -->
<!-- effect$plot() -->
<!-- ``` -->

<!-- All numeric features except for study `year` (either 2007, 2008 or 2009) provide meaningful interpretable information. -->

### Global surrogate model 

### Local surrogate model

### Shapley

```{r interpretation-006, message=FALSE, warning=FALSE, fig.cap='Plot of the results from Shapley. $\\phi$ gives the increase or decrease in probability given the values on the vertical axis',  fig.align='center'}
x = penguins[which(names(penguins) != "species")]
model = Predictor$new(learner, data = penguins, y = "species")
x.interest = data.frame(penguins[1, ])
shapley = Shapley$new(model, x.interest = x.interest)
plot(shapley)
```

The $\phi$ provides insight into the probability given the values on the vertical axis. For example, a penguin is less likely to be Gentoo if the bill_depth=18.7 is and much more likely to be Adelie than Chinstrap.


### Independent Test Data

It is also interesting to see how well the model performs on a test data set. For this section, exactly as was recommended in Section [Train/Predict](#train-predict), 80% of the penguin data set will be used for the training set and 20% for the test set:

```{r interpretation-008, message=FALSE, warning=FALSE}
train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)
test_set = setdiff(seq_len(task_peng$nrow), train_set)
learner$train(task_peng, row_ids = train_set)
prediction = learner$predict(task_peng, row_ids = test_set)
```

First, we compare the feature importance on training and test set

```{r interpretation-009, message=FALSE, warning=FALSE, fig.cap='FeatImp on train (left) and test (right)',  fig.align='center'}
# plot on training
model = Predictor$new(learner, data = penguins[train_set, ], y = "species")
effect = FeatureImp$new(model, loss = "ce")
plot_train = plot(effect, features = num_features)

# plot on test data
model = Predictor$new(learner, data = penguins[test_set, ], y = "species")
effect = FeatureImp$new(model, loss = "ce")
plot_test = plot(effect, features = num_features)

# combine into single plot
library("patchwork")
plot_train + plot_test
```

The results of the train set for `FeatureImp` are very similar, which is expected. We follow a similar approach to compare the feature effects:

```{r interpretation-010, message=FALSE, warning=FALSE, fig.cap='FeatEffect train data set', fig.align='center'}
model = Predictor$new(learner, data = penguins[train_set, ], y = "species")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

```{r interpretation-011, message=FALSE, warning=FALSE, fig.cap='FeatEffect test data set',  fig.align='center'}
model = Predictor$new(learner, data = penguins[test_set, ], y = "species")
effect = FeatureEffects$new(model)
plot(effect, features = num_features)
```

As is the case with `FeatureImp`, the test data results show either an over- or underestimate of feature importance / feature effects compared to the results where the entire penguin data set was used. This would be a good opportunity for the reader to attempt to resolve the estimation by playing with the amount of features and the amount of data used for both the test and train data sets of `FeatureImp` and `FeatureEffects`. Be sure to not change the line `train_set = sample(task_peng$nrow, 0.8 * task_peng$nrow)` as it will randomly sample the data again.

## counterfactuals {#sec-counterfactuals}

The `r ref_pkg("counterfactuals")` package is a subpackage of the `r ref_pkg("iml")` package for methods that generate counterfactual explanations, or short counterfactuals. Counterfactual explanations explain the prediction of a data point by presenting 
which minimal changes in the point are sufficient to receive a different, desired outcome. 

An example for a counterfactual for the `r ref("palmerpenguins::penguins")` data would be: "If the penguin has a bill length of 45 mm instead of 35 mm and a flipper length of 210 mm instead of 180 mm, the penguin would have been classified as Gentoo with a probability of \> 50 %" (@fig-counterfactuals-penguins). 
By revealing the feature changes that alter a decision, the counterfactuals reveal which features are the key drivers for a decision.

```{r interpretation-counterfactuals-fig, echo=FALSE, out.width = '50%', fig.align='center'}
#| label: fig-counterfactuals-penguins
#| fig-cap: Illustration of a counterfactual explanation. The blue dot displays a counterfactual for a given point (brown dot) which proposed changes in bill and flipper length such that the prediction changes from Adelie to Gentoo with > 50 \% probability.
#| fig-alt: Illustration of counterfactual explanations. Two dots are shown one that is the point whose prediction we want to explain and the other is it's counterfactual. The counterfactual proposes to increase the bill and flip length such that the point is classified as a Gentoo instead of Adelie with a probability of more than 50 \%.
knitr::include_graphics("Figures/counterfactuals_penguins.png")
```


Many methods were proposed in previous years to generate counterfactual explanations.
These methods differ in what targeted properties their generated counterfactuals have (for example, are the feature changes actionable?) and 
with which method (for example, should a set of counterfactuals be returned in a single run?). 
Due to the variety of methods, counterfactual explanations were outsourced into a separate package instead of integrating these methods into the `r ref_pkg("iml")` package. Currently, three methods are implemented in the R package but the R6-based interface makes it easy to add other counterfactual explanation methods in the future.

### The What-If method

In the following, we focus on the simplest method among the three implemented ones: the What-If approach [@Wexler2019]. For a data point, whose prediction should be explained, the returned counterfactual is equal to the closest data point of a given data set (for example, the training data) with the desired prediction. 
To illustrate the overall workflow of the package, 
we generate counterfactuals for Steve, the first penguin in the dataset.

```{r interpretation-steve, message=FALSE, warning=FALSE}
steve = penguins[1,]
```

The `r ref_pkg("counterfactuals")` package relies on `r ref("iml::Predictor()")` as a model wrapper and can, therefore, (as the `r ref_pkg("iml")`) package) explain any prediction model fitted with the `r ref_pkg("mlr3")` package including the random forest `model` we trained above.

```{r interpretation-predictsteve, message=FALSE, warning=FALSE}
model$predict(steve)
```

The random forest predicts that Steve is with 99 % Adelie. The What-If method answers how the features need to be changed such that Steve is classified as Gentoo with a probability of more or equal to 80 %. Since we have a classification `model` we initialize a `r ref("counterfactuals::WhatIfClassif()")` object. By calling `$find_counterfactuals()`, we generate a counterfactual for Steve.

```{r interpretation-whatif, message=FALSE, warning=FALSE}
library("counterfactuals")
whatif = WhatIfClassif$new(model, n_counterfactuals = 1L)
ranger_cfexp = whatif$find_counterfactuals(steve, desired_class = "Gentoo", 
  desired_prob = c(0.5, 1))
```

### The Counterfactuals object

`ranger_cfexp`is a `r ref("counterfactuals::Counterfactuals")` object which offers many visualization and evaluation methods. For example, `$evaluate(show_diff = TRUE)` shows how the features need to be changed.

```{r interpretation-whatifevaluation, message=FALSE, warning=FALSE}
ranger_cfexp$evaluate(show_diff = TRUE)
```

To receive a probability of more than 80 \% for Gentoo the bill length, flipper length, and body mass are enlarged while the bill depth is shortened. Furthermore, Steve must come from Biscoe island instead of Torgersen. For year born and sex no changes are required. Additional columns reveal the quality of the counterfactuals, for example, the number of required features changes (`no_changed`) or the distance to the closest training data point (`dist_train`) which is 0 because the counterfactual \textit{is} a training point.

Instead of a single counterfactual, we can also generate multiple ones with `r ref("counterfactuals::WhatIfClassif()")` by setting the `n_counterfactuals` parameter to a number > 1, for example, 5.

```{r interpretation-whatifmulti, message=FALSE, warning=FALSE}
library("counterfactuals")
whatif_multi = WhatIfClassif$new(model, n_counterfactuals = 5L)
ranger_cfexp_multi = whatif_multi$find_counterfactuals(steve, 
  desired_class = "Gentoo", desired_prob = c(0.8, 1))
```
For a concise overview of the required feature changes of the five counterfactuals, we can use the `$parallel_plot()` method.
The blue line corresponds to the original feature values of Steve, while the gray line displays the counterfactual.

```{r interpretation-whatifparallel, message=FALSE, warning=FALSE, fig.height=4, out.width = '90%', fig.align='center'}
ranger_cfexp_multi$plot_parallel()
```

The agreement of all five counterfactuals that bill length, flipper length, and body mass need to be increased and bill depth decreased for a label flip from Adelie to Gentoo, assures that these features are indeed major drivers for the decision. 

## DALEX {#sec-dalex}

The `r ref_pkg("DALEX")` [@Biecek2018] package belongs to [DrWhy](https://www.drwhy.ai/) family of solutions created to support the responsible development of machine learning models. It implements the most common methods for explaining predictive models using posthoc model agnostic techniques. You can use it for any model built with the `r ref_pkg("mlr3")` package as well as with other frameworks in `R`. The counterpart in `Python` is the library `dalex` [@Baniecki2021].

The philosophy of working with `r ref_pkg("DALEX")` package is based on the process of explanatory model analysis described in the [EMA book](https://ema.drwhy.ai/) [@biecek_burzykowski_2021]. In this chapter, we present code snippets and a general overview of this package. For illustrative purposes, we reuse the `learner` model built in the @sec-penguin-task on `r ref("palmerpenguins::penguins")` data.

Once you become familiar with the philosophy of working with the `r ref_pkg("DALEX")` package, you can also use other packages from this family such as `r ref_pkg("fairmodels")` [@Wisniewski2022] for detection and mitigation of biases, `r ref_pkg("modelStudio")` [@Baniecki2019] for interactive model exploration, `r ref_pkg("modelDown")` [@Romaszko2019] for the automatic generation of IML model documentation in the form of a report, `r ref_pkg("survex")` [@Krzyzinski2023] for the explanation of survival models, or `r ref_pkg("treeshap")` for the analysis of tree-based models.

### Explanatory model analysis {#sec-interpretability-architecture}

The analysis of a model is usually an interactive process starting with a shallow analysis -- usually a single-number summary. Then in a series of subsequent steps, one can systematically deepen understanding of the model by exploring the importance of single variables or pairs of variables to an in-depth analysis of the relationship between selected variables to the model outcome. See @Bucker2022 for a broader discussion of what the model exploration process looks like.

This explanatory model analysis (EMA) process can focus on a single observation, in which case we speak of local model analysis, or for a set of observations, in which case we speak of global data analysis. Below, we will present these two scenarios in separate subsections. See @fig-dalex-fig-plot-01 for an overview of key functions that will be discussed.

```{r interpretation-012, echo=FALSE, fig.cap='Taxonomy of methods for model exploration presented in this chapter. Left part overview methods for global level exploration while the right part is related to local level model exploration.', out.width = '92%', fig.align='center'}
#| label: fig-dalex-fig-plot-01
knitr::include_graphics("Figures/DALEX_ema_process.png")
```

Predictive models in R have different internal structures. To be able to analyse them systematically, an intermediate object -- a wrapper -- is needed to provide a consistent interface for accessing the model. Working with explanations in the `r ref_pkg("DALEX")` package always starts with the creation of such a wrapper with the use of the `r ref("DALEX::explain()")` function. This function has several arguments that allow the model created by the various frameworks to be parameterised accordingly. For models created in the `r mlr3` package, it is more convenient to use the `r ref("DALEXtra::explain_mlr3()")`.

```{r interpretation-019, message=FALSE, warning=FALSE}
library("DALEX")
library("DALEXtra")

ranger_exp = DALEX::explain(learner,
  data = penguins[test_set, ],
  y = penguins[test_set, "species"],
  label = "Ranger Penguins",
  colorize = FALSE)
```

The `r ref("DALEX::explain()")` function performs a series of internal checks so the output is a bit verbose. Turn the `verbose = FALSE` argument to make it less wordy.

### Global level exploration {#sec-interpretability-dataset-level}

The global model analysis aims to understand how a model behaves on average on a set of observations, most commonly a test set. In the `r ref_pkg("DALEX")` package, functions for global analysis have names starting with the prefix `model_`.

#### Model Performance

As shown in Figure @fig-dalex-fig-plot-01, it starts by evaluating the performance of a model. This can be done with a variety of tools, in the `r ref_pkg("DALEX")` package the default is to use the `r ref("DALEX::model_performance")` function. Since the `explain` function checks what type of task is being analysed, it can select the appropriate performance measures for it. In our illustration, we have a multi-label classification, so measures such as micro-aggregated F1, macro-aggregated F1 etc. are calculated in the following snippet. One of the calculated measures is cross entropy and it will be used later in the following sections.

Each explanation can be drawn with the generic `plot()` function, for multi-label classification the distribution of residuals is drawn by default.

```{r interpretation-020a, message=FALSE, warning=FALSE, fig.width=6, fig.height=5, out.width = '60%', fig.align='center'}
perf_penguin = model_performance(ranger_exp)
perf_penguin

library("ggplot2")
old_theme = set_theme_dalex("ema") 
plot(perf_penguin)
```

The task of classifying the penguin species is rather easy, which is why there are so many values of 1 in the performance assessment of this model.

#### Permutational Variable Importance

A popular technique for assessing variable importance in a model-agnostic manner is the permutation variable importance. It is based on the difference (or ratio) in the selected loss function after the selected variable or set of variables has been permuted. Read more about this technique in [Variable-importance Measures](https://ema.drwhy.ai/featureImportance.html) chapter.

The `r ref("DALEX::model_parts()")` function calculates the importance of variables and its results can be visualized with the generic `plot()` function.

```{r interpretation-021, message=FALSE, warning=FALSE, fig.width=8, fig.height=4, out.width = '90%', fig.align='center'}
ranger_effect = model_parts(ranger_exp)
head(ranger_effect)

plot(ranger_effect, show_boxplots = FALSE) 
```

The bars start in loss (here cross-entropy loss) for the selected data and end in a loss for the data after the permutation of the selected variable. The more important the variable, the more the model will lose after its permutation.

#### Partial Dependence

Once we know which variables are most important, we can use [Partial Dependence Plots](https://ema.drwhy.ai/partialDependenceProfiles.html) to show how the model, on average, changes with changes in selected variables.

The `r ref("DALEX::model_profile()")` function calculates the partial dependence profiles. The `type` argument of this function also allows *Marginal profiles* and *Accumulated Local profiles* to be calculated. Again, the result of the explanation can be model_profile with the generic function `plot()`.

```{r interpretation-024, message=FALSE, warning=FALSE, fig.width=8, fig.height=7, out.width = '90%', fig.align='center'}
ranger_profiles = model_profile(ranger_exp)
ranger_profiles

plot(ranger_profiles) + 
  theme(legend.position = "top") + 
  ggtitle("Partial Dependence for Penguins","")
```

For the multi-label classification model, profiles are drawn for each class separately by indicating them with different colours. We already know which variable is the most important, so now we can read how the model result changes with the change of this variable. In our example, based on `bill_length_mm` we can separate *Adelie* from *Chinstrap* and based on `flipper_length_mm` we can separate *Adelie* from *Gentoo*.

### Local level explanation {#sec-interpretability-instance-level}

The local model analysis aims to understand how a model behaves for a single observation. In the `r ref_pkg("DALEX")` package, functions for local analysis have names starting with the prefix `predict_`.

We will carry out the following examples using Steve the penguin of the `Adelie` species as an example.

```{r interpretation-025a, message=FALSE, warning=FALSE}
steve = penguins[1,]
steve
```

#### Model Prediction

As shown in Figure @fig-dalex-fig-plot-01, the local analysis starts with the calculation of a model prediction.

For Steve, the species was correctly predicted as `Adelie` with high probability.

```{r interpretation-025, message=FALSE, warning=FALSE}
predict(ranger_exp, steve)
```

#### Break Down

A popular technique for assessing the contributions of variables to model prediction is Break Down (see [Introduction to Break Down](https://ema.drwhy.ai/breakDown.html) chapter for more information about this method).

The function `r ref("DALEX::predict_parts()")` function calculates the attributions of variables and its results can be visualized with the generic `plot()` function.

```{r interpretation-027, message=FALSE, warning=FALSE, fig.width=8, fig.height=5.5, out.width = '90%', fig.align='center'}
ranger_attributions = predict_parts(ranger_exp, new_observation = steve)
plot(ranger_attributions) + ggtitle("Break Down for Steve") 
```

Looking at the plots above, we can read that the biggest contributors to the final prediction were for Steve the variables bill length and flipper.

#### Shapley Values

By far the most popular technique for local model exploration [@Holzinger2022] is Shapley values and the most popular algorithm for estimating these values is the SHAP algorithm. Find a detailed description of the method and algorithm in the chapter [SHapley Additive exPlanations (SHAP)](https://ema.drwhy.ai/shapley.html).

The function `r ref("DALEX::predict_parts()")` calculates SHAP attributions, you just need to set `type = "shap"`. Its results can be visualized with a generic `plot()` function.

```{r interpretation-028, message=FALSE, warning=FALSE, fig.width=8, fig.height=5.5, out.width = '90%', fig.align='center'}
ranger_shap = predict_parts(ranger_exp, new_observation = steve, 
             type = "shap")
plot(ranger_shap, show_boxplots = FALSE) + 
             ggtitle("Shapley values for Steve", "") 
```

The results for Break Down and SHAP methods are generally similar. Differences will emerge if there are many complex interactions in the model.

#### Ceteris Paribus

In the previous section, we've introduced a global explanation -- Partial Dependence plots. Ceteris Paribus plots are the local level version of that plot. Read more about this technique in the chapter [Ceteris Paribus](https://ema.drwhy.ai/ceterisParibus.html) and note that these profiles are also called Individual Conditional Expectations (ICE). They show the response of a model when only one variable is changed while others stay unchanged.

The function `r ref("DALEX::predict_profile()")` calculates Ceteris paribus profiles which can be visualized with the generic `plot()` function.

```{r interpretation-029, message=FALSE, warning=FALSE, fig.width=8, fig.height=6, out.width = '90%', fig.align='center'}
ranger_ceteris = predict_profile(ranger_exp, steve)
plot(ranger_ceteris) + ggtitle("Ceteris paribus for Steve", " ") 
```

Blue dot stands for the prediction for Steve. Only a big change in bill length could convince the model of Steve's different species.

## Exercises

Model explanation allows us to confront our expert knowledge related to the problem with relations learned by the model. Following tasks are based on predictions of the value of football players based on data from the FIFA game. It is a graceful example, as most people have some intuition about how a footballer's age or skill can affect their value. The latest FIFA statistics can be downloaded from [kaggle.com](https://www.kaggle.com/), but also one can use the 2020 data avaliable in the `DALEX` packages(see `DALEX::fifa` dataset). The following exercises can be performed in both the `iml` and `DALEX` packages and we have provided solutions for both.

1.  Prepare a `mlr3` regression task for `fifa` data. Select only variables describing the age and skills of footballers. Train any predictive model for this task, e.g. `regr.ranger`.

2.  Use the permutation importance method to calculate variable importance ranking. Which variable is the most important? Is it surprising?

3.  Use the Partial Dependence profile to draw the global behavior of the model for this variable. Is it aligned with your expectations?

4 Choose one of the football players. You can choose some well-known striker (e.g. Robert Lewandowski) or a well-known goalkeeper (e.g. Manuel Neuer). The following tasks are worth repeating for several different choices.

5.  For the selected footballer, calculate and plot the Shapley values. Which variable is locally the most important and has the strongest influence on the valuation of the footballer?

6.  For the selected footballer, calculate the Ceteris Paribus / Individual Conditional Expectatons profiles to draw the local behaviour of the model for this variable. Is it different from the global behaviour?
