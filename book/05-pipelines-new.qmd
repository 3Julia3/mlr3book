# Pipelines {#pipelines}

```{r pipelines-setup, include = F}
library(mlr3)
library(mlr3book)
library(mlr3pipelines)
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("warn")
knitr::opts_chunk$set(fig.width=7, fig.height=5)
```

Machine learning toolkits often try to abstract away the processes happening inside machine learning algorithms.
This makes it easy for the user to switch out one algorithm for another without having to worry about what is happening inside, what kind of data it is able to operate on etc.
The benefit of using `r mlr3book::mlr_pkg("mlr3")`, for example, is that one can use any `r ref("Learner")`, `r ref("Task")`, or `r ref("Resampling")` etc. and use them for typical machine learning operations, mostly independently of what algorithms or datasets they represent.
In the following code snippet, for example, it would be trivial to swap in a different `r ref("Learner")` than `"classif.rpart"` without having to worry about implementation details of the Learners involved:
```{r 05-pipelines-in-depth-002, eval = FALSE}
task = as_task_classif(iris, target = "Species")
lrn = lrn("classif.rpart")
rsmp = rsmp("holdout")
resample(task, lrn, rsmp)
```

However, this modularity breaks down as soon as the learning algorithm encompasses more than just model fitting, like data preprocessing, ensembles or even more complicated meta-models.
This is where `r mlr3book::cran_pkg("mlr3pipelines")` steps in: it takes modularity one step further than `r mlr3book::mlr_pkg("mlr3")` and makes it possible to build individual steps within a `r ref("Learner")` out of building blocks that manipulate data, called `r ref("PipeOp")`s.

`r mlr3book::mlr_pkg("mlr3pipelines")` [@mlr3pipelines] is a dataflow programming toolkit for `r mlr3book::mlr_pkg("mlr3")`.
It provides an expressive and intuitive language for defining processes such as data preprocessing, model fitting, ensemble learning and prediction post-processsing.
Individual, frequently encountered building blocks, such as missing value imputation or majority vote ensembling, are provided as (R6-)objects, so-called PipeOps.
These PipeOps can be connected using directed edges inside a Graph (or Pipeline) to represent the flow of data between operations.

Some examples of what can be implemented with `r mlr3book::mlr_pkg("mlr3pipelines")` are:

* Data manipulation and preprocessing operations, e.g. PCA, feature filtering, imputation.
* Task subsampling for speed or for handling imbalanced classes.
* `r mlr3book::mlr_pkg("mlr3")` Learner operations for prediction and stacking.
* Ensemble methods and aggregation of predictions.

A very simple sequential Graph that does various preprocessing operations before fitting a Learner looks like the following:
```{r 05-pipelines-002, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/single_pipe.svg")
```

An example for a more elaborate Pipeline that does alternative path branching and can therefore be [tuned](#optimization) to use the best preprocessing operation `r mlr3book::mlr_pkg("mlr3pipelines")` is depicted here:
```{r 05-pipelines-003, echo = FALSE, width = 10, height = 10, eval = TRUE, message=FALSE}
# This just produces a plot, not visible to the user.
library("mlr3pipelines")

graph = po("branch", c("nop", "pca", "scale")) %>>%
  gunion(list(
    po("nop", id = "null1"),
    po("pca"),
    po("scale")
  ))
gr = graph %>>%
  po("unbranch", c("nop", "pca", "scale")) %>>%
  po("learner", lrn("classif.rpart"))

gr$plot(html = FALSE)
```

In the following, we will # TODO

# Quick Introduction{#pipelines-quick-introduction}
 <!-- 6 pages -->

While the following chapters will give an in-depth introduction to `r mlr3book::mlr_pkg("mlr3pipelines")` that explain its concepts in detail, we will first present a quick introduction sufficient to get started.
To use `mlr3pipelines`, it suffices to load the package:
```{r pipeop-quickstart-library}
library("mlr3pipelines")
```

## Creating and Configuring `PipeOP`s

`PipeOp`s can easily be created using the `po()` short access function.
Just like `Learner`s, they have hyperparameters that can be set during construction as well as using the `$param_set$values` slot.
The ID of a `PipeOp` can be set during construction as well; this may be necessary to avoid nameclashes later on, because when `PipeOp`s are combined in a `Graph`, they need to have differing IDs.

```{r pipeop-quickstart-construction-1}
pca = po("pca", scale. = TRUE)
pca
pca2 = po("pca", id = "pca2")
pca2$param_set$values$center = FALSE
pca2
```

An `mlr3` `Learner` can be turned into a `PipeOp` (a `PipeOpLearner`, in fact) using `as_pipeop()`.
This is, however, often not necessary since this happens automatically in most places where a `PipeOp` is expected.
```{r pipeop-quickstart-construction-2}
lp = as_pipeop(lrn("classif.rpart"))
lp
```

More about `PipeOp`s and how they are created and configured is described in Chapter TODO.

## Creating `Graph`s

The easiest way to combine `PipeOp`s is to use the `%>>%`-operator.
It combines its left hand side with its right hand side sequentially, so that the right hand side gets the result from its left hand side as input.
The `$plot()` method can be used to show what `Graph` was created:

```{r pipeop-quickstart-graphs-1}
gr = pca %>>% lp
gr$plot(html = FALSE)
```

Chapter TODO gives a more in-depth introduction into how simple `Graph`s are constructed.

## Using `Graph`s as `Learners`s

`Graph`s in which the last operation is a `PipeOpLearner` can be turned into a `Learner` (a `GraphLearner`, to be precise) that performs the operations represented by the `Graph` in order by using `as_learner()`.

```{r pipeop-quickstart-graphlearner-1}
gl = as_learner(gr)
gl
gl$train(tsk("iris"))
gl$model$classif.rpart$model
```

Notice how the decision variables in the model are not the columns of the original dataset, but instead the principal components extracted by the `"pca"`-`PipeOp`.
The `Learner` can be `resample()`d, `benchmark()`d and `tune()`d just as a normal `Learner`.

```{r pipeop-quickstart-graphlearner-2}
rr = resample(tsk("iris"), gl, rsmp("cv"))
rr$aggregate()
```

When evaluating the performance of a machine learning process consisting e.g. of preprocessing, followed by model fitting, it is strongly advised to always put the entire preprocessing operation *inside* the resampling loop, as is done here -- `mlr3pipelines` encourages this more accurate approach.
Making a PCA on the entire dataset (i.e. `tsk("iris")`) instead, followed by resampling the `"classif.rpart"` `Learner` on it, would effectively leak information from the training set to the test set, leading to biased results.

See Chapter TODO about how to effectively use `Graph`s as `Learner`s in `mlr3`.


## Tuning `GraphLearner`s

`mlr3pipelines` is well suited for tuning entire machine learning pipelines.
For one because performance evaluations are more accurate, see above, but also because the constructed `Graph` objects have a combined hyperparameter space that can be tuned together.
Notice how the hyperparameters of both the `"pca"` `PipeOp`, as well as of the `"classif.rpart"` `Learner`, are present.
They are prefixed by each object's ID to avoid possible name clashes.

```{r pipeop-quickstart-graphlearner-3}
gl$param_set
```

This `GraphLearner` behaves like any other `Learner` for the sake of tuning. The following tunes the number of principal components to use, along with the tree size to use:
```{r pipeop-quickstart-tuning-1}
library("mlr3tuning")
gl$param_set$values$pca.rank. = to_tune(1, 4)
gl$param_set$values$classif.rpart.maxdepth = to_tune(2, 6)

tr = tune(tnr("grid_search"), tsk("iris"), gl, rsmp("cv"))
tr$result
```

Chapter TODO gives more details on the topic of tuning `Graph`s.

## Non-Sequential `Graph`s

One distinguishing feature of `mlr3pipelines` is that it can represent `Graph`s with parallel paths, for example alternative preprocessing operations that then get combined together in a "`cbind`"-operation. To arrange `Graph`s in parallel, the `gunion()`-function can be used. The `%>>%` operator tries to automatically connect all outputs of its left hand side to all inputs of its right hand side. The following fits a model on both the centered and the non-centered principal components of the `iris` dataset. The features get combined using the `"featureunion"` `PipeOp` and then given to the `"classif.rpart"` `Learner`.
```{r pipeop-quickstart-nonseq-1}
gr2 = gunion(list(po("pca", center = TRUE), po("pca", center = FALSE, id = "pca2"))) %>>%
  po("featureunion", innum = c("centered", "noncentered")) %>>% lrn("classif.rpart")
gr2$keep_results = TRUE
gr2$plot(html = FALSE)
gl2 = as_learner(gr2)
gl2$train(tsk("iris"))
```
This already shows some advanced usage necessary for this scenario: The ID of one of the `"pca"` `PipeOp`s needs to be changed to avoid name collisions, and the `innum` construction argument of `"featureunion"` is set to a vector of column prefixes to use, as both `"pca"` `PipeOp`s produce columns with the same name.
Setting the`$keep_results` debug-flag of the `Graph` is not necessary, but it allows us to find out what was actually returned by the `"featureunion"` `PipeOp` during training:
```{r pipeop-quickstart-nonseq-2}
gl2$graph_model$pipeops$featureunion$.result
```
We can also look at the trained `"classif.rpart"`-model to see that both centered as well as noncentered features were used
```{r pipeop-quickstart-nonseq-3}
gl2$model$classif.rpart$model
```

See Chapter TODO for more on non-sequential `Graph`s.

## Common `Graph` Patterns

There are various patterns that frequently occur when building machine learning pipelines that consist of more than just a single `PipeOp` operation. A collection is provided in the `mlr_graphs`-`Dictionary` and can be accessed using the `ppl()` short access function. Examples are `"robustify"`, which does preprocessing that ensures that a given `Task` is compatible with a given `Learner`, `"branch"`, which does alternative path branching and introduces a hyperparameter that controls which part of a `Graph` gets executed, and `"stacking"`, which uses the prediction of one or more `Learner`s to support the training of another `Learner`.

The following is an example, using the `"classif.ranger"` and `"classif.multinom"` model predictions as features for a `"classif.rpart"` model:
```{r pipeop-quickstart-ppl-1}
library("mlr3learners")
gr3 = ppl("stacking", lrns(c("classif.ranger", "classif.multinom")), lrn("classif.rpart"))
gr3$plot(html = FALSE)
```

The `mlr_graphs` `Dictionary` is presented in Chapter TODO.

# `r ref("PipeOp")`: Pipeline Operators

The most basic unit of functionality within `r mlr3book::cran_pkg("mlr3pipelines")` is the `r ref("PipeOp")`, short for "pipeline operator", which represents a transformative operation on input (for example a training dataset) leading to output.
It can therefore be seen as a generalized notion of a function, with a certain twist: `r ref("PipeOp")`s behave differently during a "training phase" and a "prediction phase".
The training phase will typically generate a certain model of the data that is saved as internal state.
The prediction phase will then operate on the input data depending on the trained model.

An example of this behavior is the *principal component analysis* operation ("`r ref("PipeOpPCA")`"):
```{r pipeop-intro-1}
pca = po("pca")
pca
```

Training is done using the `$train()`-method. Unlike the `Learner`'s `$train()` method, the `PipeOp`'s `$train()` can have multiple inputs, as well as multiple outputs. This is realized through `list`s: Both the `$train()` input as well as output of a `PipeOp` is always a `list`; depending on the operation, these lists have one or more elements. To use the `"pca"` `PipeOp`, for example, it is called with a list with a single element, the training `Task`. In return it produces the resulting `Task` with features replaced by their principal components.

```{r 05-pipelines-in-depth-003}
poin = list(tsk("iris"))
poout = pca$train(poin)
poout
poout[[1]]$data()
```

During training, the PCA transforms incoming data by rotating it in a way that leads to uncorrelated features ordered by their contribution to total variance.
The rotation matrix is also saved to be used for new data during the "prediction phase".
This makes it possible to perform "prediction" with single rows of new data, where a row's scores on each of the principal components (the components of the *training* data!) is computed.

Similarly to `$train()`, the `$predict()` function operates on `list`s:

```{r 05-pipelines-in-depth-004}
single_line_task = tsk("iris")$filter(1)
single_line_task$data()
poin = list(single_line_task)
poout = pca$predict(poin)
poout[[1]]$data()
```

The internal state that is trained in the `$train()` call is saved in the `$state` property. Its function is comparable to that of the `$model` of a `Learner`, and its content depends on the class of `PipeOp` being used.

```{r 05-pipelines-in-depth-005}
pca$state
```

<!--
It is important to take a moment and notice the importance of a `$state` variable and the `$train()` / `$predict()` dichotomy in a `r ref("PipeOp")`.
There are many preprocessing methods, for example scaling of parameters or imputation, that could in theory just be applied to training data and prediction / validation data separately, or they could be applied to a task before resampling is performed.
This would, however, be fallacious:

* The preprocessing of each instance of prediction data should not depend on the remaining prediction dataset.
A prediction on a single instance of new data should give the same result as prediction performed on a whole dataset.
* If preprocessing is performed on a task *before* resampling is done, information about the test set can leak into the training set.
Resampling should evaluate the generalization performance of the *entire* machine learning method, therefore the behavior of this entire method must only depend only on the content of the *training* split during resampling.
-->


<!--
This shows the most important primitives incorporated in a `r ref("PipeOp")`:
* **`$train()`**, taking a list of input arguments, turning them into a list of outputs, meanwhile saving a state in `$state`
* **`$predict()`**, taking a list of input arguments, turning them into a list of outputs, making use of the saved `$state`
* **`$state`**, the "model" trained with `$train()` and utilized during `$predict()`.
-->

<!--
Schematically we can represent the `r ref("PipeOp")` like so:

```{r 05-pipelines-in-depth-006, echo = FALSE}
knitr::include_graphics("images/po_viz.png")
```
-->


## Creating `r ref("PipeOp")`s

Each `r ref("PipeOp")` is an instance of an "`R6`" class, many of which are provided by the `r mlr3book::cran_pkg("mlr3pipelines")` package itself.
They can be retrieved from the `r ref("mlr_pipeops")` dictionary, most easily by using the `po()` short access function.
Besides that, they can also be constructed explicitly ("`PipeOpPCA$new()`").
When retrieving `r ref("PipeOp")`s from the `r ref("mlr_pipeops")` dictionary, it is also possible to give additional constructor arguments, such as an [id](#pipeop-ids-and-id-name-clashes) or [parameter values](#hyperparameters). Setting the ID explicitly is necessary when using multiple instances of the same class of `PipeOp` in a single `Graph`, since otherwise a name collision would occur, see [LINK](#ids-and-id-name-clashes)

```{r 05-pipelines-in-depth-009}
po("pca", rank. = 3, id = "pca2")
```

Some `PipeOp`s, in fact, require construction arguments, for example when they are operators that wrap another `mlr3`-object.
```{r 05-pipelines-pipeops-005, eval = FALSE}
learner = po("learner")

# Error in as_learner(learner) : argument "learner" is missing, with no default
```

```{r 05-pipelines-pipeops-006}
learner = po("learner", lrn("classif.rpart"))
```

Calling `po()` by itself prints all available `PipeOp`s, and `as.data.table(po())` will give a more detailed list with more meta-data.
```{r 05-pipelines-in-depth-010}
po()
```

<!--

Additionally, we implement several meta operators that can be used to construct powerful pipelines:

* Simultaneous path branching (data going both ways)
* Alternative path branching (data going one specific way, controlled by hyperparameters)

An extensive introduction to creating custom **PipeOps** (PO's) can be found in the [technical introduction](#extending-pipeops).

Using methods from `r mlr3book::mlr_pkg("mlr3tuning")`, it is even possible to simultaneously optimize parameters of multiple processing units.

A predecessor to this package is the `r mlr3book::cran_pkg("mlrCPO")` package, which works with `r mlr3book::cran_pkg("mlr")` 2.x.
Other packages that provide, to varying degree, some preprocessing functionality or machine learning domain specific language, are:

* the `r mlr3book::cran_pkg("caret")` package and the related `r mlr3book::cran_pkg("recipes")`  project
* the `r mlr3book::cran_pkg("dplyr")` package

An example for a Pipeline that can be constructed using `r mlr3book::mlr_pkg("mlr3pipelines")` is depicted below:

```{r 05-pipelines-003-x, echo = FALSE, width = 10, height = 10, eval = TRUE, message=FALSE}
# This just produces a plot, not visible to the user.
library("mlr3pipelines")

graph = po("branch", c("nop", "pca", "scale")) %>>%
  gunion(list(
    po("nop", id = "null1"),
    po("pca"),
    po("scale")
  ))
gr = graph %>>%
  po("unbranch", c("nop", "pca", "scale")) %>>%
  po("learner", lrn("classif.rpart"))

gr$plot(html = FALSE)
```
-->

# `r ref("Graph")`: Networks of `r ref("PipeOp")`s


## Basics: Sequential `Graph`s

`PipeOp`s are used to represent individual computational steps in machine learning pipelines, whereas these pipelines themselves are defined by `Graph` objects.
A `r ref("Graph")` is a collection of `r ref("PipeOp")`s with "edges" that mandate that data should be flowing along them.

It is most convenient to build up a `r ref("Graph")` from a sequence of `PipeOp`s, which can be done using the **`%>>%`** ("double-arrow") operator.
When given two `r ref("PipeOp")`s, it creates a `Graph` that executes first the left-hand `PipeOp`, followed by the right-hand `PipeOp`.
It can also be used to sequence a `r ref("Graph")` with a `PipeOp`, or another `Graph`. The following example creates a `Graph` that first adds a `Petal.Area` feature to a given dataset, and then performs scaling and centering of all numeric features.
```{r 05-sequential-01}
p_area = po("mutate", mutation = list(Petal.Area = ~Petal.Width * Petal.Length))
p_scale = po("scale")
gr = p_area %>>% p_scale
```

While the printer of a `r ref("Graph")` gives some information about the layout of a `Graph`, the most intuitive way of visualizing it is using the `$plot()` function.

```{r 05-pipelines-in-depth-017}
print(gr)
```

```{r 05-pipelines-in-depth-018, fig.width = 8, fig.height = 8}
gr$plot(html = FALSE)
```


## `Graph`s are Nodes with Edges

<!-- What is the advantage of this tedious way of declaring input and output channels and handling in/output through lists?
Because each `r ref("PipeOp")` has a known number of input and output channels that always produce or accept data of a known type, it is possible to network them together in **`r ref("Graph")`**s. -->

Internally, `Graph`s are collections of `PipeOp`s with edges that connect them. 
The collection of `r ref("PipeOp")`s inside a `r ref("Graph")` can be accessed through the **`$pipeops`** slot.
The set of edges in the Graph can be inspected through the **`$edges`** slot.

```{r 05-pipelines-in-depth-018-2}
gr$pipeops
gr$edges
```

Besides using the `%>>%`-operator (and a few more advanced operations presented in Section TODO) to create `Graph`s, it is also possible to create them explicitly. 
A `r ref("Graph")` is empty when first created, and `r ref("PipeOp")`s can be added using the **`$add_pipeop()`** method.
The **`$add_edge()`** method is used to create connections between them.
The above `Graph` can therefore also be created in the following way:


```{r 05-pipelines-in-depth-016}
gr = Graph$new()
gr$add_pipeop(p_area)
gr$add_pipeop(p_scale)
gr$add_edge("mutate", "scale")  # address by PipeOp-ID
```

<!--
Edges always pass between `r ref("PipeOp")` *channels*, so it is not only possible to explicitly prescribe which position of an input or output list an edge refers to, it makes it possible to make different components of a `r ref("PipeOp")`'s output flow to multiple different other `r ref("PipeOp")`s, as well as to have a `r ref("PipeOp")` gather its input from multiple other `r ref("PipeOp")`s.


A schema of a simple graph of `r ref("PipeOp")`s:

```{r 05-pipelines-in-depth-015, echo = FALSE}
knitr::include_graphics("images/po_multi_viz.png")
```
-->

:::{.callout-warning}
Although it is also possible to modify individual `PipeOps` and edges in a Graph through the `$pipeops` and `$edges` slots, this is not recommended, because no error checking is performed and it may put the `r ref("Graph")` in an unsupported state.
:::

## Using a `Graph`

A `r ref("Graph")` itself has a **`$train()`** and a **`$predict()`** method that accept some data and propagate this data through the network of `r ref("PipeOp")`s.
The return value corresponds to the output of the `r ref("PipeOp")` outputs that are not connected to other `r ref("PipeOp")`s. Just like for `PipeOp`s, the output is a list.

```{r 05-pipelines-in-depth-019}
result = gr$train(task)
result
result[[1]]$data()
```

```{r 05-pipelines-in-depth-020}
result = gr$predict(single_line_task)
result[[1]]$data()
```

## Debugging a `Graph` with Intermediate Results

When `Graph`s are evaluated, they do not keep intermediate results for memory efficiency, unless the `$keep_results` flag is set first. Inspecting these results may help understanding the inner workings of `Graph`s, in particular when they produce unexpected results. 

```{r 05-pipelines-in-depth-021}
gr$keep_results
result = gr$predict(single_line_task)
intermediate = gr$pipeops$scale$.result
intermediate
intermediate[[1]]$data()
```

# Sequential `Learner`-Pipelines

Probably the most common application for `mlr3pipelines` is to use it to perform basic preprocessing tasks, such as missing value imputation or factor encoding, and to then feed the resulting data into a `Learner`.
A `Graph` representing this workflow manipulates data and fits a `Learner`-model during training, and uses the fitted model with data that was likewise preprocessed during prediction.
Conceptually, the process may look as follows:

```{r 05-pipelines-modeling-002, echo=FALSE }
knitr::include_graphics("images/pipe_action.svg")
```

While a `Learner` is not a `PipeOp` by itself, it can easily be converted into one using `as_pipeop()`, or alternatively `po("learner")`, which creates a `PipeOpLearner`-wrapper-class.
```{r 05-pipelines-modeling-0}
l_rpart = lrn("classif.rpart")
p_rpart = po("learner", l_rpart)
p_rpart
```

However, this is rarely necessary, since the `%>>%`-operator automatically converts `Learner`s to `PipeOp`s. The following code creates a `Graph` that adds a `Petal.Area` feature, followed by fitting a `"classif.rpart"` decision tree model.

```{r 05-pipelines-modeling-1}
p_area = po("mutate", mutation = list(Petal.Area = ~Petal.Width * Petal.Length))
gr = p_area %>>% l_rpart  # could just as well use p_rpart
gr$plot(html = FALSE)
```

While the whole `Graph` object behaves similarly to a `Learner`, it is still a distinct kind of object. To use a `Graph` as a `Learner` within `mlr3`, it is necessary to use a wrapper-class, `GraphLearner`. This can be constructed by calling `as_learner()`:
```{r 05-pipelines-modeling-2}
glrn = as_learner(gr)
```
This `Learner` can be used like any other `mlr3`-`Learner`. In particular it can be used with `resample()` and `benchmark()`. Let us compare our sequential pipeline with the `"classif.rpart"`-`Learner` by itself:
```{r 05-pipelines-modeling-3}
grid = benchmark_grid(tsks("iris"), list(glrn, l_rpart), rsmps("repeated_cv")))
bmr = benchmark(grid)
bmr$aggregate()
```

## Accessing Pipeline Objects

The `glrn` variable containing the `GraphLearner` object can be used as an ordinary `Learner`. However, it is really a wrapper around a `Graph`, which contains `PipeOp`s, which themselves contain things. The following demonstrates how the flow of data in a `GraphLearner` can be analyzed. First, the `$keep_results` flag is set so intermediate results are retained. The `Graph` can be accessed through the `$graph_model` slot.
```{r 05-pipelines-modeling-debugging}
glrn$graph_model$keep_results = TRUE
glrn$train(tsk("iris"))
```

It is now possible to investigate what data was given to the `"classif.rpart"` `Learner` by looking at the output of the `mutate`-`PipeOp`.
As expected, it contains the additional feature `Petal.Area`.
```{r 05-pipelines-modeling-debugging-1}
mutate_result = glrn$graph_model$pipeops$mutate$.result
mutate_result
mutate_result[[1]]$data()
```

One can also look at the `$state` of the various `PipeOp`s to investigate the trained model. Here the trained `"classif.rpart"` classification tree is interesting. However, it is wrapped inside a `PipeOpLearner`. The trained `Learner` has to be extracted from the `PipeOp` before inspection.
```{r 05-pipelines-modeling-debugging-2}
trained_p_rpart = glrn$graph_model$pipeops$classif.rpart
trained_p_rpart
trained_l_rpart = trained_p_rpart$learner_model
trained_l_rpart
trained_l_rpart$model
```

## Pipeline Hyperparameters


# Advanced Graphs

## IDs and Name Clashes

## Parallel `PipeOp`s

* `gunion()`
* `%>>%` on parallel pipeops

## Alternative Path Branching

## `PipeOpSelect` and `PipeOpFeatureUnion`

## `PipeOpLearnerCV` and Stacking

# Tuning Graphs

## Tuning Combined Spaces

## Tuning Alternative Paths

## Tuning `PipeOpProxy`

# Common Patterns and `ppl()`

# Graph, The Whole Story

## `PipeOp` Channels

## Edges

## How `%>>%` Works

# Specific PipeOps

## Imputation: `PipeOpImpute`

## Feature Engineering: `PipeOpMutate`

## Training on data subsets: `PipeOpChunk`

## Feature Selection: `PipeOpFilter` and `PipeOpSelect`

# Comparison to Recipes, SKLearn etc.

<!--
('sequential graphs' continuation)

Together with the `r ref("gunion()")` operation, which takes `r ref("PipeOp")`s or `r ref("Graph")`s and arranges them next to each other akin to a (disjoint) graph union, the above network can more easily be constructed as follows:

```{r 05-pipelines-in-depth-023}
gr = po("copy", outnum = 2) %>>%
  gunion(list(po("scale"), po("pca"))) %>>%
  po("featureunion", innum = 2)

gr$plot(html = FALSE)
```
-->

<!--


### Networks

The example above showed a linear preprocessing pipeline, but it is in fact possible to build true "graphs" of operations, as long as no loops are introduced^[It is tempting to denote this as a "directed acyclic graph", but this would not be entirely correct because edges run between channels of `r ref("PipeOp")`s, not `r ref("PipeOp")`s themselves.].
`r ref("PipeOp")`s with multiple output channels can feed their data to multiple different subsequent `r ref("PipeOp")`s, and `r ref("PipeOp")`s with multiple input channels can take results from different `r ref("PipeOp")`s.
When a `r ref("PipeOp")` has more than one input / output channel, then the `r ref("Graph")`'s `$add_edge()` method needs an additional argument that indicates which channel to connect to.
This argument can be given in the form of an integer, or as the name of the channel.

The following constructs a `r ref("Graph")` that copies the input and gives one copy each to a "scale" and a "pca" `r ref("PipeOp")`.
The resulting columns of each operation are put next to each other by "featureunion".

```{r 05-pipelines-in-depth-021, tidy = FALSE}
gr = Graph$new()$
  add_pipeop(po("copy", outnum = 2))$
  add_pipeop(po("scale"))$
  add_pipeop(po("pca"))$
  add_pipeop(po("featureunion", innum = 2))

gr$
  add_edge("copy", "scale", src_channel = 1)$        # designating channel by index
  add_edge("copy", "pca", src_channel = "output2")$  # designating channel by name
  add_edge("scale", "featureunion", dst_channel = 1)$
  add_edge("pca", "featureunion", dst_channel = 2)

gr$plot(html = FALSE)
```
```{r 05-pipelines-in-depth-022}
gr$train(iris_first_half)[[1]]$data()
```
-->


<!--
### `r ref("PipeOp")` IDs and ID Name Clashes

`r ref("PipeOp")`s within a graph are addressed by their **`$id`**-slot.
It is therefore necessary for all `r ref("PipeOp")`s within a `r ref("Graph")` to have a unique `$id`.
The `$id` can be set during or after construction, but it should not directly be changed after a `r ref("PipeOp")` was inserted in a `r ref("Graph")`.
At that point, the **`$set_names()`**-method can be used to change `r ref("PipeOp")` ids.

```{r 05-pipelines-in-depth-024, error = TRUE}
po1 = po("scale")
po2 = po("scale")
po1 %>>% po2 # name clash
```

```{r 05-pipelines-in-depth-025}
po2$id = "scale2"
gr = po1 %>>% po2
gr
```

```{r 05-pipelines-in-depth-026}
# Alternative ways of getting new ids:
po("scale", id = "scale2")
po("scale", id = "scale2")
```

```{r 05-pipelines-in-depth-027, error = TRUE}
# sometimes names of PipeOps within a Graph need to be changed
gr2 = po("scale") %>>% po("pca")
gr %>>% gr2
```

```{r 05-pipelines-in-depth-028}
gr2$set_names("scale", "scale3")
gr %>>% gr2
```
-->


