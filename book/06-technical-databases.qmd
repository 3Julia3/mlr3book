# Database Backends {#backends}

{{< include _setup.qmd >}}

In mlr3, `r ref("Task", text = "Tasks")` store their data in an abstract data object, the `r ref("DataBackend")`.
The default backend uses `r cran_pkg("data.table")` via the `r ref("DataBackendDataTable")` as an very fast and efficient in-memory data base.

For bigger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data to reduce the memory requirements.
There are multiple options to archive this:

1. `r ref("DataBackendDplyr")` which interfaces the R package `r cran_pkg("dbplyr")`, extending `r cran_pkg("dplyr")` to work on many popular data bases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).
2. `r ref("DataBackendDuckDB")` for the impressive [DuckDB](https://duckdb.org/) data base connected via `r cran_pkg("duckdb")`: a fast, zero configuration alternative to SQLite.
3. `r ref("DataBackendDuckDB")`, again, but to work directly on [parquet files](https://parquet.apache.org/).


## DataBackendDplyr

To demonstrate the `r ref("DataBackendDplyr")` we use the NYC flights data set from the `r cran_pkg("nycflights13")` package and move it into a SQLite data base.
Although `as_sqlite_backend()` provides a convenient function to perform this step, we contruct the data base manually here.

```{r 06-technical-databases-001, message = FALSE}
# load data
requireNamespace("DBI")
requireNamespace("RSQLite")
requireNamespace("nycflights13")
data("flights", package = "nycflights13")
str(flights)

# add column of unique row ids
flights$row_id = 1:nrow(flights)

# create sqlite database in temporary file
path = tempfile("flights", fileext = ".sqlite")
con = DBI::dbConnect(RSQLite::SQLite(), path)
tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))
DBI::dbDisconnect(con)

# remove in-memory data
rm(flights)
```

### Preprocessing with `dplyr`

With the SQLite database in `path`, we now re-establish a connection and switch to `r cran_pkg("dplyr")`/`r cran_pkg("dbplyr")` for some essential preprocessing.

```{r 06-technical-databases-002}
# establish connection
con = DBI::dbConnect(RSQLite::SQLite(), path)

# select the "flights" table, enter dplyr
library("dplyr")
library("dbplyr")
tbl = tbl(con, "flights")
```

First, we select a subset of columns to work on:

```{r 06-technical-databases-003}
keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",
  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")
tbl = select(tbl, all_of(keep))
```

Additionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:

```{r 06-technical-databases-004}
tbl = filter(tbl, !is.na(arr_delay))
```

To keep runtime reasonable for this toy example, we filter the data to only use every second row:

```{r 06-technical-databases-005}
tbl = filter(tbl, row_id %% 2 == 0)
```

The factor levels of the feature `carrier` are merged so that infrequent carriers are replaced by level "other":

```{r 06-technical-databases-006}
tbl = mutate(tbl, carrier = case_when(
  carrier %in% c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN") ~ "other",
  TRUE ~ carrier))
```

### Creating the Backend

The processed table is now used to create a `r ref("mlr3db::DataBackendDplyr")` from `r mlr_pkg("mlr3db")`:

```{r 06-technical-databases-007}
library("mlr3db")
b = as_data_backend(tbl, primary_key = "row_id")
```

We can now use the interface of `r ref("DataBackend")` to query some basic information of the data:

```{r 06-technical-databases-008}
b$nrow
b$ncol
b$head()
```

Note that the `r ref("DataBackendDplyr")` does not know about any rows or columns we have filtered out with `r cran_pkg("dplyr")` before, it just operates on the view we provided.

### Model fitting

We create the following `r mlr_pkg("mlr3")` objects:

* A `r ref("TaskRegr", text = "regression task")`, based on the previously created `r ref("mlr3db::DataBackendDplyr")`.
* A regression learner (`r ref("mlr_learners_regr.rpart", text = "regr.rpart")`).
* A resampling strategy: 3 times repeated subsampling using 2\% of the observations for training ("`r ref("mlr_resamplings_subsampling", text = "subsampling")`")
* Measures "`r ref("mlr_measures_regr.mse", text = "mse")`", "`r ref("mlr_measures_time_train", text = "time_train")`" and "`r ref("mlr_measures_time_predict", text = "time_predict")`"

```{r 06-technical-databases-009}
task = as_task_regr(b, id = "flights_sqlite", target = "arr_delay")
learner = lrn("regr.rpart")
measures = mlr_measures$mget(c("regr.mse", "time_train", "time_predict"))
resampling = rsmp("subsampling", repeats = 3, ratio = 0.02)
```

We pass all these objects to `r ref("resample()")` to perform a simple resampling with three iterations.
In each iteration, only the required subset of the data is queried from the SQLite data base and passed to `r ref("rpart::rpart()")`:

```{r 06-technical-databases-010}
rr = resample(task, learner, resampling)
print(rr)
rr$aggregate(measures)
```

### Cleanup

Finally, we remove the `tbl` object and close the connection.

```{r 06-technical-databases-011}
rm(tbl)
DBI::dbDisconnect(con)
```


## DataBackendDuckDB

### Working with DuckDBs

While storing the Task's data in memory is most efficient w.r.t. accessing it, this has two major disadvantages:

1. Although you might only need a small proportion of the data, the complete data frame sits in memory and consumes memory.
  This is especially a problem if you work with many tasks simultaneously.
2. During parallelization, the complete data needs to be transferred to the workers which can cause a significant overhead.

A very simple way to avoid this is given by just converting the `r ref("DataBackendDataTable")` to a `r ref("DataBackendDuckDB")`.

...

### Working with Parquet Files
