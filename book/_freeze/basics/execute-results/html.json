{
  "hash": "c45c689329324595df34f57dd236a4c8",
  "result": {
    "markdown": "# Basics {#basics}\n\n::: {.cell}\n\n:::\n\n\n\nThis chapter will teach you the essential building blocks of [mlr3](https://mlr3.mlr-org.com), as well as its [R6](https://cran.r-project.org/package=R6) classes and operations used for machine learning.\n\nHow these building blocks interoperate is summarized in the following figure.\n\n\n::: {.cell layout-align=\"center\" hash='basics_cache/html/basics-001_caaa3ae7ef1a10bf668ce9b80b882d88'}\n::: {.cell-output-display}\n![](images/ml_abstraction.svg){fig-align='center'}\n:::\n:::\n\n\nThe data, which [mlr3](https://mlr3.mlr-org.com) encapsulates in [tasks](#tasks), is split into non-overlapping training and test sets.\nAs we are interested in models that extrapolate to new data rather than just memorizing the training data, the separate test data allows us to objectively evaluate models with respect to their generalization.\nThe training data is given to a machine learning algorithm, which we call a [learner](#learners) in [mlr3](https://mlr3.mlr-org.com).\nThe [learner](#learners) uses the training data to build a model of the relationship of the input features to the output target values.\nThis model is then used to produce [predictions](#predicting) on the test data, which are compared to the ground truth values to assess the quality of the model.\n[mlr3](https://mlr3.mlr-org.com) offers a number of different [measures](#measure) to quantify how well a model performs based on the difference between predicted and actual values.\nUsually, this [measure](#measure) is a numeric score.\n\nSplitting data into training and test sets, building a model, and evaluating it can be repeated several times, [resampling](#resampling) different training and test sets from the original data each time.\nMultiple [resampling iterations](#resampling) allow us to get a better and less biased generalizable performance estimate for a particular type of model.\nAs data are usually partitioned randomly into training and test sets, a single split can, for example, produce training and test sets that are very different, hence creating the misleading impression that the particular type of model does not perform well.\n\n:::{.callout-note}\nReal-world problems usually require preprocessing operations such as normalization or imputation of missing values.\nIn the sketched workflow above, these steps would also be part of the learner.\nThis will be covered in the chapter @pipelines.\n:::\n\nThis chapter covers the following topics:\n\n1. [Tasks](#tasks) encapsulate the data with meta-information, such as the name of the prediction target column.\n    We cover how to:\n\n    * access [predefined tasks](#tasks-predefined),\n    * specify a [task type](#tasks-types),\n    * create a [task](#tasks-creation),\n    * work with a task's [API](#tasks-api),\n    * assign roles to [rows and columns](#tasks-roles) of a task,\n    * implement [task mutators](#tasks-mutators), and\n    * [retrieve the data](#tasks-api) that is stored in a task.\n\n2. [Learners](#learners) encapsulate machine learning algorithms to train models and make predictions for a [task](#tasks).\n    Other packages provide these.\n    We cover how to:\n\n    * access the set of [classification and regression learners](#predefined-learners) that come with [mlr3](https://mlr3.mlr-org.com) and retrieve a specific learner (more types of learners are covered later in the book),\n    * access the set of [hyperparameter values](#learner-api) of a learner and modify them.\n\n3. How to [train and predict](#train-predict). In particular, we cover how to:\n\n    * properly set up [tasks](#train-predict-objects) and [learners](#train-predict-objects) for training and prediction,\n    * set up [train and test splits](#split-data) for a task,\n    * [train](#training) the learner on the training set to produce a model,\n    * run the model on the test set to produce [predictions](#predicting), and\n    * assess the [performance](#measure) of the model by comparing predicted and actual values.\n\n\n## Tasks {#tasks}\n\nTasks are objects that contain the (usually tabular) data and additional meta-data to define a machine learning problem.\nThe meta-data is, for example, the name of the target variable for supervised machine learning problems, or the type of the dataset (e.g. a _spatial_ or _survival_ task).\nThis information is used by specific operations that can be performed on a task.\n\n### Task Types {#tasks-types}\n\nTo create a task object, you first need to choose the right task type:\n\n* **Classification Task**: The target is a label (stored as `character` or `factor`) with only relatively few distinct values → [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html).\n\n* **Regression Task**: The target is a numeric quantity (stored as `integer` or `numeric`) → [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html).\n\n* **Survival Task**: The target is the (right-censored) time to an event. More censoring types are currently in development → [`mlr3proba::TaskSurv`](https://mlr3proba.mlr-org.com/reference/TaskSurv.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com).\n\n* **Density Task**: An unsupervised task to estimate the density → [`mlr3proba::TaskDens`](https://mlr3proba.mlr-org.com/reference/TaskDens.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com).\n\n* **Cluster Task**: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → [`mlr3cluster::TaskClust`](https://mlr3cluster.mlr-org.com/reference/TaskClust.html) in add-on package [mlr3cluster](https://mlr3cluster.mlr-org.com).\n\n* **Spatial Task**: Observations in the task have spatio-temporal information (e.g. coordinates) → [`mlr3spatiotempcv::TaskRegrST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskRegrST.html) or [`mlr3spatiotempcv::TaskClassifST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskClassifST.html) in add-on package [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com).\n\n* **Ordinal Regression Task**: The target is ordinal → `TaskOrdinal` in add-on package [mlr3ordinal](https://github.com/mlr-org/mlr3ordinal) (still in development).\n\n### Task Creation {#tasks-creation}\n\nAs an example, we will create a regression task using the [`mtcars`](https://www.rdocumentation.org/packages/datasets/topics/mtcars) data set from package `datasets` (ships with R).\nIt contains characteristics for different types of cars, along with their fuel consumption.\nWe predict the numeric target variable stored in column `\"mpg\"` (miles per gallon).\nHere, we only consider the first two features in the dataset for brevity:\n\n\n::: {.cell hash='basics_cache/html/basics-002_51a81b7603829cea4d856c6d79ec247b'}\n\n```{.r .cell-code}\ndata(\"mtcars\", package = \"datasets\")\ndata = mtcars[, 1:3]\nstr(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'data.frame':\t32 obs. of  3 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n```\n:::\n:::\n\n\nNext, we create a regression task, i.e. we construct a new instance of the R6 class [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html).\nFormally, the intended way to initialize an R6 object is to call the constructor `TaskRegr$new()`.\nHere instead, we are calling the converter [`as_task_regr()`](https://mlr3.mlr-org.com/reference/as_task_regr.html) to convert our `data.frame()` stored in the object `data` to a regression task and provide the following additional information:\n\n1. `x`: Object to convert.\n  Works for rectangular data formats such as `data.frame()`, `data.table()`, or `tibble()`.\n  Internally, the data is converted and stored in an abstract [`DataBackend`](https://mlr3.mlr-org.com/reference/DataBackend.html).\n  This allows connecting to out-of-memory storage systems like SQL servers via the extension package [mlr3db](https://mlr3db.mlr-org.com).\n1. `target`: The name of the prediction target column for the regression problem, here miles per gallon (`\"mpg\"`).\n1. `id` (optional): An arbitrary identifier for the task, used in plots and summaries.\n   If not provided, the deparsed name of `x` will be used.\n\n\n::: {.cell hash='basics_cache/html/basics-003_2b2bb9a6e632058f007e272875dbe2a9'}\n\n```{.r .cell-code}\nlibrary(\"mlr3\")\n\ntask_mtcars = as_task_regr(data, target = \"mpg\", id = \"cars\")\nprint(task_mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskRegr:cars> (32 x 3)\n* Target: mpg\n* Properties: -\n* Features (2):\n  - dbl (2): cyl, disp\n```\n:::\n:::\n\n\nThe `print()` method gives a short summary of the task:\nIt has 32 observations and 3 columns, of which 2 are features stored in double-precision floating point format.\n\nWe can also plot the task using the [mlr3viz](https://mlr3viz.mlr-org.com) package, which gives a graphical summary of its properties:\n\n\n::: {.cell hash='basics_cache/html/basics-004_95e9472b40f1d4a7680d9c381b21a2b0'}\n\n```{.r .cell-code}\nlibrary(\"mlr3viz\")\nautoplot(task_mtcars, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-004-1.png){width=672}\n:::\n:::\n\n\n:::{.callout-tip}\nInstead of loading multiple extension packages individually, it is often more convenient to load the [mlr3verse](https://mlr3verse.mlr-org.com) package instead.\n`mlr3verse` imports the Namespace of most [mlr3](https://mlr3.mlr-org.com) packages and re-exports functions which are used for common machine learning and data science tasks.\n:::\n\n### Predefined tasks {#tasks-predefined}\n\n[mlr3](https://mlr3.mlr-org.com) includes a few predefined machine learning tasks.\nAll tasks are stored in an R6 [`Dictionary`](https://mlr3misc.mlr-org.com/reference/Dictionary.html) (a key-value store) named [`mlr_tasks`](https://mlr3.mlr-org.com/reference/mlr_tasks.html).\nPrinting it gives the keys (the names of the datasets):\n\n\n::: {.cell hash='basics_cache/html/basics-005_b3a9f0bcfcf99cd30dda3b71dc87014f'}\n\n```{.r .cell-code}\nmlr_tasks\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryTask> with 11 stored values\nKeys: boston_housing, breast_cancer, german_credit, iris, mtcars,\n  penguins, pima, sonar, spam, wine, zoo\n```\n:::\n:::\n\n\nWe can get a more informative summary of the example tasks by converting the dictionary to a [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package) object:\n\n\n::: {.cell hash='basics_cache/html/basics-006_19f71b7020fdee5cd3a4ec7696904bb5'}\n\n```{.r .cell-code}\nas.data.table(mlr_tasks)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               key                   label task_type nrow ncol properties lgl\n 1: boston_housing   Boston Housing Prices      regr  506   19              0\n 2:  breast_cancer Wisconsin Breast Cancer   classif  683   10   twoclass   0\n 3:  german_credit           German Credit   classif 1000   21   twoclass   0\n 4:           iris            Iris Flowers   classif  150    5 multiclass   0\n 5:         mtcars            Motor Trends      regr   32   11              0\n 6:       penguins         Palmer Penguins   classif  344    8 multiclass   0\n 7:           pima    Pima Indian Diabetes   classif  768    9   twoclass   0\n 8:          sonar  Sonar: Mines vs. Rocks   classif  208   61   twoclass   0\n 9:           spam       HP Spam Detection   classif 4601   58   twoclass   0\n10:           wine            Wine Regions   classif  178   14 multiclass   0\n11:            zoo             Zoo Animals   classif  101   17 multiclass  15\n6 variables not shown: [int, dbl, chr, fct, ord, pxc]\n```\n:::\n:::\n\n\nAbove, the columns `\"lgl\"` ([`logical`](https://www.rdocumentation.org/packages/base/topics/logical)), `\"int\"` ([`integer`](https://www.rdocumentation.org/packages/base/topics/integer)), `\"dbl\"` ([`double`](https://www.rdocumentation.org/packages/base/topics/double)), `\"chr\"` ([`character`](https://www.rdocumentation.org/packages/base/topics/character)), `\"fct\"` ([`factor`](https://www.rdocumentation.org/packages/base/topics/factor)), `\"ord\"` ([`ordered factor`](https://www.rdocumentation.org/packages/base/topics/factor)) and `\"pxc\"` ([`POSIXct`](https://www.rdocumentation.org/packages/base/topics/DateTimeClasses) time) show the number of features in the task of the respective type.\n\nTo get a task from the dictionary, use the `$get()` method from the `mlr_tasks` class and assign the return value to a new variable\nAs getting a task from a dictionary is a very common problem, [mlr3](https://mlr3.mlr-org.com) provides the shortcut function [`tsk()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html).\nHere, we retrieve the [`palmer penguins classification task`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html), which is provided by the imported package [palmerpenguins](https://cran.r-project.org/package=palmerpenguins):\n\n\n::: {.cell hash='basics_cache/html/basics-007_732d31091df039e6080c2c7e4d294ecc'}\n\n```{.r .cell-code}\ntask_penguins = tsk(\"penguins\")\nprint(task_penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskClassif:penguins> (344 x 8): Palmer Penguins\n* Target: species\n* Properties: multiclass\n* Features (7):\n  - int (3): body_mass, flipper_length, year\n  - dbl (2): bill_depth, bill_length\n  - fct (2): island, sex\n```\n:::\n:::\n\n\n:::{.callout-note}\nLoading extension packages can add elements to dictionaries such as [`mlr_tasks`](https://mlr3.mlr-org.com/reference/mlr_tasks.html).\nFor example, [mlr3data](https://mlr3data.mlr-org.com) adds some more example and toy tasks for regression and classification, and [mlr3proba](https://mlr3proba.mlr-org.com) adds survival and density estimation tasks.\n:::\n\nTo get more information about a particular task, it is easiest to use the `help()` method that all [mlr3](https://mlr3.mlr-org.com)-objects come with:\n\n::: {.cell hash='basics_cache/html/basics-008_5295eb19fcbe30159563d80ebf6a0f63'}\n\n```{.r .cell-code}\ntask_penguins$help()\n```\n:::\n\nAlternatively, the corresponding man page can be found under `mlr_tasks_[id]`, e.g. [`mlr_tasks_penguins`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html):\n\n::: {.cell hash='basics_cache/html/basics-009_d3527286e6247b40714e859bf92b54ac'}\n\n```{.r .cell-code}\nhelp(\"mlr_tasks_penguins\")\n```\n:::\n\n\n\n:::{.callout-tip}\nThousands more data sets are readily available via [Openml.org](https://openml.org) ([@openml2013]) and [mlr3oml](https://mlr3oml.mlr-org.com).\nE.g., to download the data set [\"credit-g\"](https://www.openml.org/search?type=data&id=31) with data id `31` and automatically convert it to a classification task:\n\n\n::: {.cell hash='basics_cache/html/basics-010_f4ec0c02a59ec21153b3503c1d298cb1'}\n\n```{.r .cell-code}\nlibrary(\"mlr3oml\")\ntsk(\"oml\", task_id = 31)\n```\n:::\n\n:::\n\n### Task API {#tasks-api}\n\nAll properties and characteristics of tasks can be queried using the task's public fields and methods (see [`Task`](https://mlr3.mlr-org.com/reference/Task.html)).\nMethods can also be used to change the stored data and the behavior of the task.\n\n#### Retrieving Data {#tasks-retrieving}\n\nThe [`Task`](https://mlr3.mlr-org.com/reference/Task.html) object primarily represents a tabular dataset, combined with meta-data about which columns of that data should be used to predict which other columns in what way, as well as some more information about column data types.\n\nVarious fields can be used to retrieve meta-data about a task. The dimensions can, for example, be retrieved using `$nrow` and `$ncol`:\n\n::: {.cell hash='basics_cache/html/basics-011_7ac683b37b8f7704ecd8e2ca397198c8'}\n\n```{.r .cell-code}\ntask_mtcars$nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 32\n```\n:::\n\n```{.r .cell-code}\ntask_mtcars$ncol\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3\n```\n:::\n:::\n\n\nThe names of the feature and target columns are stored in the `$feature_names` and `$target_names` slots, respectively. Here, \"target\" refers to the variable we want to predict and \"feature\" to the predictors for the task.\n\n::: {.cell hash='basics_cache/html/basics-012_c72d3dc091e3db3afb7172045b2cf416'}\n\n```{.r .cell-code}\ntask_mtcars$feature_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"cyl\"  \"disp\"\n```\n:::\n\n```{.r .cell-code}\ntask_mtcars$target_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"mpg\"\n```\n:::\n:::\n\n\nFor the most common tasks, regression and classification, the target will only be the name of a single column.\nTasks with other task types, such as for survival estimation, may have more than one target column while clustering tasks have no target at all:\n\n::: {.cell hash='basics_cache/html/basics-013_949b57a41a92950e7383627ea354edcb'}\n\n```{.r .cell-code}\nrequireNamespace(\"mlr3proba\", quietly = TRUE)\ntsk(\"unemployment\")$target_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"spell\"   \"censor1\"\n```\n:::\n\n```{.r .cell-code}\nrequireNamespace(\"mlr3cluster\", quietly = TRUE)\ntsk(\"usarrests\")$target_names\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncharacter(0)\n```\n:::\n:::\n\n\nWhile the columns of a task have unique `character`-valued names, their rows are identified by unique natural numbers, their row-IDs. They can be accessed through the `$row_ids` slot:\n\n::: {.cell hash='basics_cache/html/basics-014_1b01c817071b01fc10e43a7ebf2c5be2'}\n\n```{.r .cell-code}\nhead(task_mtcars$row_ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3 4 5 6\n```\n:::\n:::\n\n:::{.callout-warning}\nAlthough the row IDs are typically just the sequence from `1` to `nrow(data)`, they are only guaranteed to be unique natural numbers. It is possible that they do not start at `1`, that they are not increasing by 1 each, or that they are not even in increasing order.\nThe reasoning behind this is simple: we allow to transparently operate on real database management systems, and the uniqueness is the only requirement for primary keys in data bases. For more info on connecting to data bases, see [backends](#backends).\n:::\n\nThe data contained in a task can be accessed through `$data()`, which returns a `data.table` object.\nIt has optional `rows` and `cols` arguments to specify subsets of the data to retrieve.\nWhen a database backend is used, then this avoids loading unnecessary data into memory, making it more efficient than retrieving the entire data first and then subsetting it using `[<rows>, <cols>]`.\n\n::: {.cell hash='basics_cache/html/basics-015_50d0af502a6c072492764c4fc9120db0'}\n\n```{.r .cell-code}\ntask_mtcars$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     mpg cyl  disp\n 1: 21.0   6 160.0\n 2: 21.0   6 160.0\n 3: 22.8   4 108.0\n 4: 21.4   6 258.0\n 5: 18.7   8 360.0\n---               \n28: 30.4   4  95.1\n29: 15.8   8 351.0\n30: 19.7   6 145.0\n31: 15.0   8 301.0\n32: 21.4   4 121.0\n```\n:::\n\n```{.r .cell-code}\n# retrieve data for rows with ids 1, 5, and 10 and select column \"mpg\"\ntask_mtcars$data(rows = c(1, 5, 10), cols = \"mpg\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    mpg\n1: 21.0\n2: 18.7\n3: 19.2\n```\n:::\n:::\n\n\nTo extract the complete data from the task, one can also convert it to a `data.table`:\n\n::: {.cell hash='basics_cache/html/basics-016_0d6053e7886394b5eeb53d1ade93f932'}\n\n```{.r .cell-code}\n# show summary of entire data\nsummary(as.data.table(task_mtcars))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      mpg             cyl             disp      \n Min.   :10.40   Min.   :4.000   Min.   : 71.1  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8  \n Median :19.20   Median :6.000   Median :196.3  \n Mean   :20.09   Mean   :6.188   Mean   :230.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0  \n```\n:::\n:::\n\n\n#### Task Mutators {#tasks-mutators}\n\nIt is often necessary to create tasks that encompass subsets of other tasks' data, for example to manually create [train-test-splits](#train-test-splits), or to fit models on a subset of given features. Restricting tasks to a given set of features can be done by calling `$select()` with the desired feature names. Restriction to rows is done with `$filter()` with the row-IDs.\n\n\n::: {.cell hash='basics_cache/html/basics-017_02933af6906c34a07073d5115268376c'}\n\n```{.r .cell-code}\ntask_penguins_small = tsk(\"penguins\")\ntask_penguins_small$select(c(\"body_mass\", \"flipper_length\")) # keep only these features\ntask_penguins_small$filter(2:4) # keep only these rows\ntask_penguins_small$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species body_mass flipper_length\n1:  Adelie      3800            186\n2:  Adelie      3250            195\n3:  Adelie        NA             NA\n```\n:::\n:::\n\n\nThese methods are so-called *mutators*, they modify the given `Task` in-place. If you want to have an unmodified version of the task, you need to use the `$clone()` method to create a copy first.\n\n\n::: {.cell hash='basics_cache/html/basics-018_99e44affd074485c475bd5a98a54ffc4'}\n\n```{.r .cell-code}\ntask_penguins_smaller = task_penguins_small$clone()\ntask_penguins_smaller$filter(2)\ntask_penguins_smaller$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species body_mass flipper_length\n1:  Adelie      3800            186\n```\n:::\n\n```{.r .cell-code}\ntask_penguins_small$data()  # this task is unmodified\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species body_mass flipper_length\n1:  Adelie      3800            186\n2:  Adelie      3250            195\n3:  Adelie        NA             NA\n```\n:::\n:::\n\n\nNote also how the last call to `$filter(2)` did not select the second row of the `task_penguins_small`, but selected the row with ID 2, which is the *first* row of `task_penguins_small`.\n\n:::{.callout-tip}\nIf you ever really need to work with row numbers instead of row-IDs, you can work-around by operating on the row ids and pass the result back to the task:\n\n\n::: {.cell hash='basics_cache/html/basics-019_bddada4e3ef80a5fcaaa947cd9de7a28'}\n\n```{.r .cell-code}\n# keep the 2nd row:\nkeep = task$row_ids[2] # extracts id of 2nd row\ntask_penguins_smaller$filter(keep)\n```\n:::\n\n:::\n\nWhile the methods above allow us to subset the data, the methods `$rbind()` and `$cbind()` allow adding extra rows and columns to a task.\n\n\n::: {.cell hash='basics_cache/html/basics-020_788383688123776335caca41db9aeb01'}\n\n```{.r .cell-code}\ntask_penguins_smaller$rbind( # add another row\n  data.frame(body_mass = 1e9, flipper_length = 1e9, species = \"GigaPeng\")\n)\ntask_penguins_smaller$cbind(data.frame(letters = letters[2:3])) # add column with letters\ntask_penguins_smaller$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    species  body_mass flipper_length letters\n1:   Adelie       3800            186       b\n2: GigaPeng 1000000000     1000000000       c\n```\n:::\n:::\n\n\n#### Roles (Rows and Columns) {#tasks-roles}\n\nWe have seen that certain columns are designated as \"targets\" and \"features\" during task creation, their \"roles\":\nTarget refers to the variable(s) we want to predict and features are the predictors (also called co-variates) for the target.\nBesides these two, there are other possible roles for columns, see the documentation of [`Task`](https://mlr3.mlr-org.com/reference/Task.html). These roles affect the behavior of the task for different operations.\n\nThe previously-constructed `task_penguins_small` task, for example, has the following column roles:\n\n\n::: {.cell hash='basics_cache/html/basics-021_4bcbf1c4dfb25d882495ea36c367c9f9'}\n\n```{.r .cell-code}\ntask_penguins_small$col_roles\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$feature\n[1] \"body_mass\"      \"flipper_length\"\n\n$target\n[1] \"species\"\n\n$name\ncharacter(0)\n\n$order\ncharacter(0)\n\n$stratum\ncharacter(0)\n\n$group\ncharacter(0)\n\n$weight\ncharacter(0)\n```\n:::\n:::\n\n\nColumns can have multiple roles. It is also possible for a column to have no role at all, in which case they are ignored. This is, in fact, how `$select()` and `$filter()` operate: They unassign the `\"feature\"` (for columns) or `\"use\"` (for rows) role without modifying the data which is stored in an immutable backend:\n\n\n::: {.cell hash='basics_cache/html/basics-022_88435586942ed17343b679fd39a9797d'}\n\n```{.r .cell-code}\ntask_penguins_small$backend\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DataBackendDataTable> (344x9)\n species    island bill_length bill_depth flipper_length body_mass    sex year\n  Adelie Torgersen        39.1       18.7            181      3750   male 2007\n  Adelie Torgersen        39.5       17.4            186      3800 female 2007\n  Adelie Torgersen        40.3       18.0            195      3250 female 2007\n  Adelie Torgersen          NA         NA             NA        NA   <NA> 2007\n  Adelie Torgersen        36.7       19.3            193      3450 female 2007\n  Adelie Torgersen        39.3       20.6            190      3650   male 2007\n1 variable not shown: [..row_id]\n[...] (338 rows omitted)\n```\n:::\n:::\n\n\nThere are two main ways to manipulate the col roles of a `Task`:\n\n1. Use the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) method `$set_col_roles()` (recommended).\n1. Simply modify the field `$col_roles`, which is a named list of vectors of column names.\n   Each vector in this list corresponds to a column role, and the column names contained in that vector have that role.\n\nJust as `$select()`/`$filter()`, these are in-place operations, so the task object itself is modified. To retain another unmodified version of a task, use `$clone()`.\n\nChanging the column or row roles, whether by `$select()`/`$filter()` or directly, does not change the underlying data, it just updates the view on it.\nBecause the underlying data is still there (and accessible through `$backend`), we can add the `\"bill_length\"` column back into the task by setting its col role to `\"feature\"`.\n\n::: {.cell hash='basics_cache/html/basics-023_bc7608421c56ebd149d3de65b11fa0dd'}\n\n```{.r .cell-code}\ntask_penguins_small$set_col_roles(\"bill_length\", roles = \"feature\")\ntask_penguins_small$feature_names  # bill_length is now a feature again\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"body_mass\"      \"flipper_length\" \"bill_length\"   \n```\n:::\n\n```{.r .cell-code}\ntask_penguins_small$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   species body_mass flipper_length bill_length\n1:  Adelie      3800            186        39.5\n2:  Adelie      3250            195        40.3\n3:  Adelie        NA             NA          NA\n```\n:::\n:::\n\n\nSupported column roles can be found in the manual of [`Task`](https://mlr3.mlr-org.com/reference/Task.html), or just by printing the names of the field `$col_roles`:\n\n\n::: {.cell hash='basics_cache/html/basics-024_807d9025668a3e619cd03b27d27196be'}\n\n```{.r .cell-code}\n# supported column roles, see ?Task\nnames(task_penguins_small$col_roles)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"feature\" \"target\"  \"name\"    \"order\"   \"stratum\" \"group\"   \"weight\" \n```\n:::\n:::\n\n\nJust like columns, it is also possible to assign different roles to rows. Rows can have two different roles:\n\n1. Role `use`:\n  Rows that are generally available for model fitting (although they may also be used as test set in resampling).\n  This role is the default role. The `$filter()` call changes this role, in the same way that `$select()` changes the `\"feature\"` role.\n2. Role `validation`:\n  Rows that are not used for training.\n  Rows that have missing values in the target column during task creation are automatically set to the validation role.\n\nThere are several reasons to hold some observations back or treat them differently:\n\n1. It is often good practice to validate the final model on an external validation set to identify possible overfitting.\n2. Some observations may be unlabeled, e.g. in competitions like [Kaggle](https://www.kaggle.com/).\n\nThese observations cannot be used for training a model, but can be used to get predictions.\n\n### Task API Extensions\n\nWhile the previous section described (a subset of) the API all tasks have in common, some tasks come with additional getters or setters.\n\nFor example, classification problems with a target variable with only two classes are called binary classification tasks.\nThey are special in the sense that one of these classes is denoted *positive* and the other one *negative*.\nYou can specify the *positive class* within the [`classification task`](https://mlr3.mlr-org.com/reference/TaskClassif.html) object during task creation.\nIf not explicitly set during construction, the positive class defaults to the first level of the target variable.\n\n\n::: {.cell hash='basics_cache/html/basics-025_38f2da24be28f4c31b09fefc5477549c'}\n\n```{.r .cell-code}\n# during construction\ndata(\"Sonar\", package = \"mlbench\")\ntask = as_task_classif(Sonar, target = \"Class\", positive = \"R\")\n\n# switch positive class to level 'M'\ntask$positive = \"M\"\n```\n:::\n\n\n\n### Plotting Tasks {#autoplot-task}\n\nThe [mlr3viz](https://mlr3viz.mlr-org.com) package provides plotting facilities for many classes implemented in [mlr3](https://mlr3.mlr-org.com).\nThe available plot types depend on the class, but all plots are returned as [ggplot2](https://cran.r-project.org/package=ggplot2) objects which can be easily customized.\n\nFor classification tasks (inheriting from [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html)), see the documentation of [`mlr3viz::autoplot.TaskClassif`](https://mlr3viz.mlr-org.com/reference/autoplot.TaskClassif.html) for the implemented plot types.\nHere are some examples to get an impression:\n\n\n::: {.cell hash='basics_cache/html/basics-026_09019f38edc73a2b75e6fffe94ae48ea'}\n\n```{.r .cell-code}\nlibrary(\"mlr3viz\")\n\n# get the pima indians task\ntask = tsk(\"pima\")\n\n# subset task to only use the 3 first features\ntask$select(head(task$feature_names, 3))\n\n# default plot: class frequencies\nautoplot(task)\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-026-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# pairs plot (requires package GGally)\nautoplot(task, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-026-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# duo plot (requires package GGally)\nautoplot(task, type = \"duo\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-026-3.png){width=672}\n:::\n:::\n\n\nOf course, you can do the same for regression tasks (inheriting from [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html)) as documented in [`mlr3viz::autoplot.TaskRegr`](https://mlr3viz.mlr-org.com/reference/autoplot.TaskRegr.html):\n\n\n::: {.cell hash='basics_cache/html/basics-027_9fed95be403a59a0124ad557cc6aaeab'}\n\n```{.r .cell-code}\nlibrary(\"mlr3viz\")\n\n# get the complete mtcars task\ntask = tsk(\"mtcars\")\n\n# subset task to only use the 3 first features\ntask$select(head(task$feature_names, 3))\n\n# default plot: boxplot of target variable\nautoplot(task)\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-027-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# pairs plot (requires package GGally)\nautoplot(task, type = \"pairs\")\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-027-2.png){width=672}\n:::\n:::\n\n\n## Learners {#learners}\n\nObjects of class [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) provide a unified interface to many popular machine learning algorithms in R.\nThey consist of methods to train and predict a model for a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.\n\nThe base class of each learner is [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), specialized for regression as [`LearnerRegr`](https://mlr3.mlr-org.com/reference/LearnerRegr.html) and for classification as [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html).\nOther types of learners, provided by extension packages, also inherit from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) base class, e.g. [`mlr3proba::LearnerSurv`](https://mlr3proba.mlr-org.com/reference/LearnerSurv.html) or [`mlr3cluster::LearnerClust`](https://mlr3cluster.mlr-org.com/reference/LearnerClust.html).\n\nAll Learners work in a two-stage procedure:\n\n* **Training stage**: The training data (features and target) is passed to the Learner's `$train()` function which trains and stores a model, i.e. the relationship of the target and features.\n* **Predict stage**: The new data, usually a different slice of the original data than used for training, is passed to the `$predict()` method of the Learner.\n  The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.\n\n\n::: {.cell layout-align=\"center\" hash='basics_cache/html/basics-028_15bf5ea22c589272647c4da0145b3e9d'}\n::: {.cell-output-display}\n![](images/learner.svg){fig-align='center'}\n:::\n:::\n\n\n\n### Predefined Learners\n\nThe [mlr3](https://mlr3.mlr-org.com) package ships with the following set of classification and regression learners.\nWe deliberately keep this small to avoid unnecessary dependencies:\n\n* [`classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html): Simple baseline classification learner.\n  The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a \"[fallback learner](fallback-learners)\" to make predictions in case another, more sophisticated, learner failed for some reason.\n* [`regr.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.featureless.html): Simple baseline regression learner.\n  The default is to always predict the mean of the target in training set. Similar to [`mlr_learners_classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html), it makes for a good \"[fallback learner](fallback-learners)\"\n* [`classif.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html): Single classification tree from package [rpart](https://cran.r-project.org/package=rpart).\n* [`regr.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.rpart.html): Single regression tree from package [rpart](https://cran.r-project.org/package=rpart).\n\nThis set of baseline learners is usually insufficient for a real data analysis.\nThus, we have cherry-picked implementations of the most popular machine learning method and collected them in the [mlr3learners](https://mlr3learners.mlr-org.com) package:\n\n* Linear ([`regr.lm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.lm.html)) and logistic ([`classif.log_reg`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.log_reg.html)) regression\n* Penalized Generalized Linear Models ([`regr.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.glmnet.html), [`classif.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html)), possibly with built-in optimization of the penalization parameter ([`regr.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.cv_glmnet.html), [`classif.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.cv_glmnet.html))\n* (Kernelized) $k$-Nearest Neighbors regression ([`regr.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.kknn.html)) and classification ([`classif.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.kknn.html)).\n* Kriging / Gaussian Process Regression ([`regr.km`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.km.html))\n* Linear ([`classif.lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html)) and Quadratic ([`classif.qda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.qda.html)) Discriminant Analysis\n* Naive Bayes Classification ([`classif.naive_bayes`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.naive_bayes.html))\n* Support-Vector machines ([`regr.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.svm.html), [`classif.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html))\n* Gradient Boosting ([`regr.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.xgboost.html), [`classif.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html))\n* Random Forests for regression and classification ([`regr.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html), [`classif.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html))\n\nMore machine learning methods and alternative implementations are collected in the [mlr3extralearners repository](https://github.com/mlr-org/mlr3extralearners/).\n\n:::{.callout-tip}\nA full list of available learners across all [mlr3](https://mlr3.mlr-org.com) packages is hosted on our website: [list of learners](https://mlr-org.com/learners.html).\n:::\n\n\nAnalogously to [`mlr_tasks`](https://mlr3.mlr-org.com/reference/mlr_tasks.html) storing the shipped taks, the dictionary [`mlr_learners`](https://mlr3.mlr-org.com/reference/mlr_learners.html) stores implemented learners.\n\n\n::: {.cell hash='basics_cache/html/basics-029_89b5f326aa2882116f834ee8ac19ab12'}\n\n```{.r .cell-code}\nlibrary(\"mlr3learners\")       # load recommended learners provided by mlr3learners package\nlibrary(\"mlr3extralearners\")  # this loads further less-well-supported learners\nlibrary(\"mlr3proba\")          # this loads some survival and density estimation learners\nlibrary(\"mlr3cluster\")        # this loads some learners for clustering\n\nmlr_learners\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryLearner> with 134 stored values\nKeys: classif.AdaBoostM1, classif.bart, classif.C50, classif.catboost,\n  classif.cforest, classif.ctree, classif.cv_glmnet, classif.debug,\n  classif.earth, classif.featureless, classif.fnn, classif.gam,\n  classif.gamboost, classif.gausspr, classif.gbm, classif.glmboost,\n  classif.glmnet, classif.IBk, classif.J48, classif.JRip, classif.kknn,\n  classif.ksvm, classif.lda, classif.liblinear, classif.lightgbm,\n  classif.LMT, classif.log_reg, classif.lssvm, classif.mob,\n  classif.multinom, classif.naive_bayes, classif.nnet, classif.OneR,\n  classif.PART, classif.qda, classif.randomForest, classif.ranger,\n  classif.rfsrc, classif.rpart, classif.svm, classif.xgboost,\n  clust.agnes, clust.ap, clust.cmeans, clust.cobweb, clust.dbscan,\n  clust.diana, clust.em, clust.fanny, clust.featureless, clust.ff,\n  clust.hclust, clust.kkmeans, clust.kmeans, clust.MBatchKMeans,\n  clust.meanshift, clust.pam, clust.SimpleKMeans, clust.xmeans,\n  dens.hist, dens.kde, dens.kde_ks, dens.locfit, dens.logspline,\n  dens.mixed, dens.nonpar, dens.pen, dens.plug, dens.spline, regr.bart,\n  regr.catboost, regr.cforest, regr.ctree, regr.cubist, regr.cv_glmnet,\n  regr.debug, regr.earth, regr.featureless, regr.fnn, regr.gam,\n  regr.gamboost, regr.gausspr, regr.gbm, regr.glm, regr.glmboost,\n  regr.glmnet, regr.IBk, regr.kknn, regr.km, regr.ksvm, regr.liblinear,\n  regr.lightgbm, regr.lm, regr.lmer, regr.M5Rules, regr.mars, regr.mob,\n  regr.randomForest, regr.ranger, regr.rfsrc, regr.rpart, regr.rvm,\n  regr.svm, regr.xgboost, surv.akritas, surv.blackboost, surv.cforest,\n  surv.coxboost, surv.coxph, surv.coxtime, surv.ctree,\n  surv.cv_coxboost, surv.cv_glmnet, surv.deephit, surv.deepsurv,\n  surv.dnnsurv, surv.flexible, surv.gamboost, surv.gbm, surv.glmboost,\n  surv.glmnet, surv.kaplan, surv.loghaz, surv.mboost, surv.nelson,\n  surv.obliqueRSF, surv.parametric, surv.pchazard, surv.penalized,\n  surv.ranger, surv.rfsrc, surv.rpart, surv.svm, surv.xgboost\n```\n:::\n:::\n\n\nTo obtain an object from the dictionary, use the syntactic sugar function [`lrn()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html):\n\n\n::: {.cell hash='basics_cache/html/basics-030_36a547ee44bd7bd924c32baa4b853e17'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\n### Learner API\n\nEach learner provides the following meta-information:\n\n* `$feature_types`: the type of features the learner can deal with.\n* `$packages`: the packages required to train a model with this learner and make predictions.\n* `$properties`: additional properties and capabilities.\n  For example, a learner has the property \"missings\" if it is able to handle missing feature values, and \"importance\" if it computes and allows to extract data on the relative importance of the features.\n* `$predict_types`: possible prediction types. For example, a classification learner can predict labels (\"response\") or probabilities (\"prob\").\n\nThis information can be queried through these slots, or seen at a glance from the printer:\n\n::: {.cell hash='basics_cache/html/basics-031_61289eadce5c04c10d91e96a3022b8eb'}\n\n```{.r .cell-code}\nprint(learner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRpart:classif.rpart>: Classification Tree\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n:::\n:::\n\n\nFurthermore, each learner has hyperparameters that control its behavior, for example the minimum number of samples in the leaf of a decision tree, or whether to provide verbose output durning training.\nSetting hyperparameters to values appropriate for a given machine learning task is crucial.\nThe field `param_set` stores a description of the hyperparameters the learner has, their ranges, defaults, and current values:\n\n\n::: {.cell hash='basics_cache/html/basics-032_5d5a58b0bbc4fbcd95c5c76d9a9cc18d'}\n\n```{.r .cell-code}\nlearner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                id    class lower upper nlevels        default value\n 1:             cp ParamDbl     0     1     Inf           0.01      \n 2:     keep_model ParamLgl    NA    NA       2          FALSE      \n 3:     maxcompete ParamInt     0   Inf     Inf              4      \n 4:       maxdepth ParamInt     1    30      30             30      \n 5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n 7:       minsplit ParamInt     1   Inf     Inf             20      \n 8: surrogatestyle ParamInt     0     1       2              0      \n 9:   usesurrogate ParamInt     0     2       3              2      \n10:           xval ParamInt     0   Inf     Inf             10     0\n```\n:::\n:::\n\n\nThe set of current hyperparameter values is stored in the `values` field of the `param_set` field.\nYou can access and change the current hyperparameter values by accessing this field, it is a named list:\n\n\n::: {.cell hash='basics_cache/html/basics-033_20cd403e26a113098eb156dc5210c0e1'}\n\n```{.r .cell-code}\nlearner$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$xval\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nlearner$param_set$values$cp = 0.01\nlearner$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$xval\n[1] 0\n\n$cp\n[1] 0.01\n```\n:::\n:::\n\n\n:::{.callout-tip}\nIt is possible to assign all hyperparameters in one go by assigning a named list to `$values`: `learner$param_set$values = list(cp = 0.01, xval = 0)`. However, be aware that this operation also removes all previously set hyperparameters.\n:::\n\nThe [`lrn()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) function also accepts additional arguments to update hyperparameters or set fields of the learner in one go:\n\n\n::: {.cell hash='basics_cache/html/basics-034_fdb05984aa144235b27907e5c6c1d123'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\", id = \"rp\", cp = 0.001)\nlearner$id\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"rp\"\n```\n:::\n\n```{.r .cell-code}\nlearner$param_set$values\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$xval\n[1] 0\n\n$cp\n[1] 0.001\n```\n:::\n:::\n\n\nMore on this is discussed in the section on [Hyperparameter Tuning](#tuning).\n\n## Train, Predict, Assess Performance {#train-predict}\n\nIn this section, we explain how [tasks](#tasks) and [learners](#learners) can be used to train a model and predict on a new dataset.\nTraining a [learner](#learners) means fitting a model to a given data set -- essentially, an optimization problem that determines the best parameters (not hyperparameters!) of the model given the data.\nWe then [predict](#predicting) the label for observations that the model has not seen during training.\nWe will then compare the predictions to ground truth values to assess the quality of a prediction.\n\nThe concept is demonstrated on a supervised classification task using the [`pima`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) dataset, in which patient data is used to diagnostically predict diabetes, and the [`rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html) learner, which builds a classification tree.\nAs shown in the previous chapters, we load these objects using the short access functions [`tsk()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) and  [`lrn()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html).\n\n\n::: {.cell hash='basics_cache/html/basics-035_6a60265cdab0493b24ade38271bf949b'}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\n### Training the learner {#training}\n\nThe field `$model` stores the fitted model in the training step.\nBefore the `$train()` method is called on a learner object, this field is `NULL`:\n\n\n::: {.cell hash='basics_cache/html/basics-036_95b466d31f9ea4978c6359ea1b193e94'}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nNow we fit the classification tree using the training set of the task by calling the `$train()` method of [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html):\n\n\n::: {.cell hash='basics_cache/html/basics-037_1123dfdd92b1b84869330c171afd74bc'}\n\n```{.r .cell-code}\nlearner$train(task)\n```\n:::\n\n\nThis operation modifies the learner in-place by adding the fitted model to the existing object.\nWe can now access the stored model via the field `$model`:\n\n\n::: {.cell hash='basics_cache/html/basics-038_1c7213035c0b00d9d4cf94c13188f2b2'}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 768 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n  1) root 768 268 neg (0.34895833 0.65104167)  \n    2) glucose>=127.5 283 109 pos (0.61484099 0.38515901)  \n      4) mass>=29.95 208  58 pos (0.72115385 0.27884615)  \n        8) glucose>=157.5 92  12 pos (0.86956522 0.13043478) *\n        9) glucose< 157.5 116  46 pos (0.60344828 0.39655172)  \n         18) age>=30.5 66  19 pos (0.71212121 0.28787879) *\n         19) age< 30.5 50  23 neg (0.46000000 0.54000000)  \n           38) pressure< 73 21   8 pos (0.61904762 0.38095238) *\n           39) pressure>=73 29  10 neg (0.34482759 0.65517241)  \n             78) mass>=41.8 9   3 pos (0.66666667 0.33333333) *\n             79) mass< 41.8 20   4 neg (0.20000000 0.80000000) *\n      5) mass< 29.95 75  24 neg (0.32000000 0.68000000) *\n    3) glucose< 127.5 485  94 neg (0.19381443 0.80618557)  \n      6) age>=28.5 214  71 neg (0.33177570 0.66822430)  \n       12) insulin>=142.5 56  26 neg (0.46428571 0.53571429)  \n         24) age< 56.5 41  16 pos (0.60975610 0.39024390) *\n         25) age>=56.5 15   1 neg (0.06666667 0.93333333) *\n       13) insulin< 142.5 158  45 neg (0.28481013 0.71518987)  \n         26) glucose>=99.5 102  41 neg (0.40196078 0.59803922)  \n           52) mass>=26.35 84  41 neg (0.48809524 0.51190476)  \n            104) pedigree>=0.2045 65  27 pos (0.58461538 0.41538462)  \n              208) pregnant>=5.5 32   8 pos (0.75000000 0.25000000) *\n              209) pregnant< 5.5 33  14 neg (0.42424242 0.57575758)  \n                418) age>=34.5 19   7 pos (0.63157895 0.36842105) *\n                419) age< 34.5 14   2 neg (0.14285714 0.85714286) *\n            105) pedigree< 0.2045 19   3 neg (0.15789474 0.84210526) *\n           53) mass< 26.35 18   0 neg (0.00000000 1.00000000) *\n         27) glucose< 99.5 56   4 neg (0.07142857 0.92857143) *\n      7) age< 28.5 271  23 neg (0.08487085 0.91512915) *\n```\n:::\n:::\n\n\nInspecting the output, we see that the learner has identified features in the task that are predictive of the class (diabetes status) and uses them to partition observations in the tree.\nThere are additional details on how the data is partitioned across branches of the tree; the textual representation of the model depends on the type of learner.\nFor more information on this particular type of model and its output, see [`rpart::print.rpart()`](https://www.rdocumentation.org/packages/rpart/topics/print.rpart).\n\n### Predicting {#predicting}\n\nAfter the model has been fitted to the training data, we can now use it for prediction. A common case is that a model was fitted on all training data that was available, and should now be used to make predictions for new data for which the actual labels are unknown:\n\n\n::: {.cell hash='basics_cache/html/basics-039_dfd0b06fe2a56a256d2828b67bd71a93'}\n\n```{.r .cell-code}\npima_new = data.table::fread(\"\nage, glucose, insulin, mass, pedigree, pregnant, pressure, triceps\n24,  145,     306,     41.7, 0.5,      3,        52,       36\n47,  133,     NA,      23.3, 0.2,      7,        83,       28\n\")\npima_new\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   age glucose insulin mass pedigree pregnant pressure triceps\n1:  24     145     306 41.7      0.5        3       52      36\n2:  47     133      NA 23.3      0.2        7       83      28\n```\n:::\n:::\n\n\nThe learner does not need to know more meta-information about this data to make a prediction, such as which columns are features and targets, since this was already included in the training task.\nInstead, this data can directly be used to make a prediction using `$predict_newdata()`:\n\n\n::: {.cell hash='basics_cache/html/basics-040_d36741e3cf76392493239bb1cae832f4'}\n\n```{.r .cell-code}\nprediction = learner$predict_newdata(pima_new)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 2 observations:\n row_ids truth response\n       1  <NA>      pos\n       2  <NA>      neg\n```\n:::\n:::\n\n\nThis method returns a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object.\nMore precisely, because the `learner` is a [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html), it returns a [`PredictionClassif`](https://mlr3.mlr-org.com/reference/PredictionClassif.html) object. The easiest way to access information from it is to convert it to a `data.table`:\n\n::: {.cell hash='basics_cache/html/basics-041_72e6a714fce7ff57819893b96a177504'}\n\n```{.r .cell-code}\nas.data.table(prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   row_ids truth response\n1:       1  <NA>      pos\n2:       2  <NA>      neg\n```\n:::\n:::\n\n\nHere the `\"truth\"` column is `NA`, since the target column was not provided in the `pima_new` data frame.\nIf we add the column, we will have the true and predicted labels side by side in the prediction object.\n\n::: {.cell hash='basics_cache/html/basics-042_9f60a3932a8d18b48aabe684bd3a8abe'}\n\n```{.r .cell-code}\npima_new_known = cbind(pima_new, diabetes = factor(\"pos\", levels = c(\"pos\", \"neg\")))\nprediction = learner$predict_newdata(pima_new_known)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 2 observations:\n row_ids truth response\n       1   pos      pos\n       2   pos      neg\n```\n:::\n:::\n\n\nNote that it is sometimes helpful first to convert the data to predict on a task.\nPredicting on the task's data works analogously, you only need to call the `$predict()` method instead of `$predict_newdata()`:\n\n\n::: {.cell hash='basics_cache/html/basics-043_245a7bf1c2841b0b9a0871cdae4bd63a'}\n\n```{.r .cell-code}\ntask_pima_new = as_task_classif(pima_new_known, target = \"diabetes\")\nprediction = learner$predict(task_pima_new)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 2 observations:\n row_ids truth response\n       1   pos      pos\n       2   pos      neg\n```\n:::\n:::\n\n\n\n### Changing the Predict Type {#predict-type}\n\nClassification learners default to predicting the class label.\nHowever, many classifiers also tell you how sure they are about the predicted label by providing posterior probabilities for the classes.\nTo predict these probabilities, the `predict_type` field of a [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html) must be changed from `\"response\"` (the default) to `\"prob\"` before training:\n\n\n::: {.cell hash='basics_cache/html/basics-044_4fa2514c64e6272325afb4d8c20d0a10'}\n\n```{.r .cell-code}\nlearner$predict_type = \"prob\"\n\n# re-fit the model\nlearner$train(task)\n\n# rebuild prediction object\nprediction = learner$predict(task_pima_new)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 2 observations:\n row_ids truth response  prob.pos  prob.neg\n       1   pos      pos 0.6190476 0.3809524\n       2   pos      neg 0.3200000 0.6800000\n```\n:::\n:::\n\n\nThe prediction object now contains probabilities for all class labels in addition to the predicted label (the one with the highest probability):\n\n\n::: {.cell hash='basics_cache/html/basics-045_ed24210e18efbfacd795efd1d343162e'}\n\n```{.r .cell-code}\n# directly access the predicted labels:\nprediction$response\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] pos neg\nLevels: pos neg\n```\n:::\n\n```{.r .cell-code}\n# directly access the matrix of probabilities:\nprediction$prob\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           pos       neg\n[1,] 0.6190476 0.3809524\n[2,] 0.3200000 0.6800000\n```\n:::\n\n```{.r .cell-code}\n# data.table conversion\nas.data.table(prediction)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   row_ids truth response  prob.pos  prob.neg\n1:       1   pos      pos 0.6190476 0.3809524\n2:       2   pos      neg 0.3200000 0.6800000\n```\n:::\n:::\n\n\nSimilarly to predicting probabilities for classification, many [`regression learners`](https://mlr3.mlr-org.com/reference/LearnerRegr.html) support the extraction of standard error estimates for predictions by setting the predict type to `\"se\"`.\n\n### Thresholding\n\nModels trained on binary classification tasks that predict the probability for the positive class usually use a simple rule to determine the predicted class label: if the probability is more than 50%, predict the positive label, otherwise, predict the negative label.\nIn some cases, you may want to adjust this threshold, for example, if the classes are very unbalanced (i.e., one is much more prevalent than the other).\n\nIn the example below, we change the threshold to 0.2, making the model predict `\"pos\"` for both example rows:\n\n\n::: {.cell hash='basics_cache/html/basics-046_683e6adf1b06b635b5867ffc92192c0d'}\n\n```{.r .cell-code}\nprediction$set_threshold(0.2)\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 2 observations:\n row_ids truth response  prob.pos  prob.neg\n       1   pos      pos 0.6190476 0.3809524\n       2   pos      pos 0.3200000 0.6800000\n```\n:::\n:::\n\n\n### Predicting on known data and train/test splits\n\nWe will usually not want to wait with performance evaluation until new data becomes available.\nInstead, we will work with all the training data available at a given point.\nHowever, when evaluating the performance of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), it is also important to score predictions made on data that have not been seen during training, since making predictions on training data is too easy in general -- a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) could just memorize the training data responses and get a perfect score.\n\n[mlr3](https://mlr3.mlr-org.com) makes it easy to only train on subsets of given tasks. We first create a vector indicating on what row IDs of the task the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) should be trained, and another that indicates the remaining rows that should be used for prediction. These vectors indicate the train-test-split we are using.\nThis is done manually here for demonstration purposes: In @sec-resampling, we show how [mlr3](https://mlr3.mlr-org.com) can automatically create training and test sets based on resampling strategies that can be more elaborate.\n\nWe will use 67% of all available observations to train and predict on the remaining 33%.\n\n\n::: {.cell hash='basics_cache/html/basics-047_871b3c78871c9565fd01734d668dd2a0'}\n\n```{.r .cell-code}\nset.seed(7)\ntrain_set = sample(task$row_ids, 0.67 * task$nrow)\ntest_set = setdiff(task$row_ids, train_set)\n```\n:::\n\n\n:::{.callout-caution}\nDo not use constructs like `sample(task$nrow, ...)` to subset tasks, since rows are always identified by their `$row_ids`.\nThese are not guaranteed to range from 1 to `task$nrow` and could be any positive integer.\n:::\n\nBoth `$train()` and `$predict()` have an optional `row_ids`-argument that determines which rows are used. Note that it is not a problem to run `$train()` with a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) that has already been trained: the old model is automatically discarded, and the learner trains from scratch.\n\n\n::: {.cell hash='basics_cache/html/basics-048_c7053b28245442bd23df270584eb19f0'}\n\n```{.r .cell-code}\n# train on the training set\nlearner$train(task, row_ids = train_set)\n\n# predict on the test set\nprediction = learner$predict(task, row_ids = test_set)\n\n# the prediction naturally knows about the \"truth\" from the task\nprediction\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<PredictionClassif> for 254 observations:\n    row_ids truth response   prob.pos  prob.neg\n          8   neg      neg 0.37500000 0.6250000\n         12   pos      pos 0.84905660 0.1509434\n         19   neg      neg 0.37500000 0.6250000\n---                                            \n        762   pos      pos 0.84905660 0.1509434\n        765   neg      neg 0.09954751 0.9004525\n        768   neg      neg 0.09954751 0.9004525\n```\n:::\n:::\n\n\n### Performance assessment {#measure}\n\nThe last step of modeling is usually assessing the performance of the trained model. For this, the predictions made by the model are compared with the known ground-truth values that are stored in the [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object.\nThe exact nature of this comparison is defined by a measure, which is given by a `\"Measure\"` object.\nIf the prediction was made on a dataset without the target column, i.e., without known true labels, then performance can not be calculated.\n\nAvailable measures can be retrieved using the [`msr()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) function, which accesses objects in [`mlr_measures`](https://mlr3.mlr-org.com/reference/mlr_measures.html):\n\n\n::: {.cell hash='basics_cache/html/basics-049_fa6d3f1453a3edca2a1e11cebd52021f'}\n\n```{.r .cell-code}\nmlr_measures\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<DictionaryMeasure> with 88 stored values\nKeys: aic, bic, classif.acc, classif.auc, classif.bacc, classif.bbrier,\n  classif.ce, classif.costs, classif.dor, classif.fbeta, classif.fdr,\n  classif.fn, classif.fnr, classif.fomr, classif.fp, classif.fpr,\n  classif.logloss, classif.mbrier, classif.mcc, classif.npv,\n  classif.ppv, classif.prauc, classif.precision, classif.recall,\n  classif.sensitivity, classif.specificity, classif.tn, classif.tnr,\n  classif.tp, classif.tpr, clust.ch, clust.db, clust.dunn,\n  clust.silhouette, clust.wss, debug, dens.logloss, oob_error,\n  regr.bias, regr.ktau, regr.mae, regr.mape, regr.maxae, regr.medae,\n  regr.medse, regr.mse, regr.msle, regr.pbias, regr.rae, regr.rmse,\n  regr.rmsle, regr.rrse, regr.rse, regr.rsq, regr.sae, regr.smape,\n  regr.srho, regr.sse, selected_features, sim.jaccard, sim.phi,\n  surv.brier, surv.calib_alpha, surv.calib_beta, surv.chambless_auc,\n  surv.cindex, surv.dcalib, surv.graf, surv.hung_auc, surv.intlogloss,\n  surv.logloss, surv.mae, surv.mse, surv.nagelk_r2, surv.oquigley_r2,\n  surv.rcll, surv.rmse, surv.schmid, surv.song_auc, surv.song_tnr,\n  surv.song_tpr, surv.uno_auc, surv.uno_tnr, surv.uno_tpr, surv.xu_r2,\n  time_both, time_predict, time_train\n```\n:::\n:::\n\n\nWe choose accuracy ([`classif.acc`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.acc.html)) as our specific performance measure here and call the method `$score()` of the `prediction` object to quantify the predictive performance of our model.\n\n\n::: {.cell hash='basics_cache/html/basics-050_c725aad0f61592b7f4eb4e3d5e42146c'}\n\n```{.r .cell-code}\nmeasure = msr(\"classif.acc\")\nmeasure\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<MeasureClassifSimple:classif.acc>: Classification Accuracy\n* Packages: mlr3, mlr3measures\n* Range: [0, 1]\n* Minimize: FALSE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n```\n:::\n\n```{.r .cell-code}\nprediction$score(measure)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.acc \n  0.7244094 \n```\n:::\n:::\n\n\n:::{.callout-note}\n`$score()` can called without a given measure. In this case, classification defaults to classification error ([`classif.ce`](https://mlr3.mlr-org.com/reference/mlr_measures_classif.ce.html), which is one minus accuracy) and regression to the mean squared error ([`regr.mse`](https://mlr3.mlr-org.com/reference/mlr_measures_regr.mse.html)).\n:::\n\nIt is possible to calculate multiple measures at the same time by passing a list to `$score()`. Such a list can easily be constructed using the \"plural\" `msrs()` function. If one wanted to have both the \"true positive rate\" (`\"classif.tpr\"`) and the \"true negative rate\" (`\"classif.tnr\"`), one would use:\n\n\n::: {.cell hash='basics_cache/html/basics-051_678685cb81b7f6b6cf820f212f508fc2'}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.tpr\", \"classif.tnr\"))\nprediction$score(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.tpr classif.tnr \n  0.4639175   0.8853503 \n```\n:::\n:::\n\n\n#### Confusion Matrix\n\nA special case of performance evaluation is the confusion matrix, which shows, for each class, how many observations were predicted to be in that class and how many were actually in it (more information on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)).\nThe entries along the diagonal denote the correctly classified observations.\n\n\n::: {.cell hash='basics_cache/html/basics-052_104f32b31a83d1f536b27e12bd931fb4'}\n\n```{.r .cell-code}\nprediction$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse pos neg\n     pos  45  18\n     neg  52 139\n```\n:::\n:::\n\n\nIn this case, we can see that our classifier seems to misclassify a relatively large number of positive samples as negative. In fact, a positive case is still more likely to be classified as `\"neg\"` than `\"pos'`. Depending on the application being considered, it is possible that it is more important to keep false positives (lower left element of the confusion matrix) low. Lowering the threshold, so that ambiguous samples are more readily classified as positive rather than negative, can help in this case, although it will also lead to negative cases being classified as `\"pos\"` more often.\n\n\n::: {.cell hash='basics_cache/html/basics-053_624cb7f8a8adae69b2ee0f50256c66af'}\n\n```{.r .cell-code}\nprediction$set_threshold(0.3)\nprediction$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse pos neg\n     pos  75  65\n     neg  22  92\n```\n:::\n:::\n\n\n:::{.callout-tip}\nThresholds can be tuned automatically with the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package, i.e. using [`PipeOpTuneThreshold`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_tunethreshold.html).\n:::\n\n\n### Plotting Predictions {#autoplot-prediction}\n\nSimilarly to [plotting tasks](#autoplot-task), [mlr3viz](https://mlr3viz.mlr-org.com) provides an [`autoplot()`](https://www.rdocumentation.org/packages/ggplot2/topics/autoplot) method for [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) objects.\nAll available types are listed in the manual pages for [`autoplot.PredictionClassif()`](https://mlr3viz.mlr-org.com/reference/autoplot.PredictionClassif.html), [`autoplot.PredictionRegr()`](https://mlr3viz.mlr-org.com/reference/autoplot.PredictionRegr.html) and the other prediction types (defined by extension packages).\n\n\n::: {.cell hash='basics_cache/html/basics-054_46e843f75e44dc9887b3e93d8b54efd6'}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nlearner$train(task)\nprediction = learner$predict(task)\n\nlibrary(\"mlr3viz\")\nautoplot(prediction)\n```\n\n::: {.cell-output-display}\n![](basics_files/figure-html/basics-054-1.png){width=672}\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}