{
  "hash": "8365aaa6ba02e5c1d479ce3215813285",
  "result": {
    "markdown": "# Technical {#technical }\n\n::: {.cell}\n\n:::\n\n\n\nThis chapter provides an overview of technical details of the [mlr3](https://mlr3.mlr-org.com) framework.\nThis includes the following topics:\n\n* Parallelization with the [future](https://cran.r-project.org/package=future) framework,\n* how to handle errors and troubleshoot,\n* working with out-of-memory data, e.g., data stored in data bases,\n* working with hyperparameters (define parameter spaces, sample from parameter spaces, apply transformations), and\n* adjust the logger to your needs.\n\n## Parallelization {#parallelization}\n\nParallelization refers to running multiple jobs in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes.\nThis process allows for significant savings in computing power.\nWe distinguish between implicit parallelism and explicit parallelism.\n\n:::{.callout-note}\nWe don't cover parallelization on GPUs here.\n`mlr3` only parallelizes the fitting of multiple learners in parallel, e.g., during resampling, benchmarking, or tuning.\nOn this rather abstract level, GPU parallelization doesn't work efficiently.\nInstead, some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU during a single model fit.\nWe refer to the respective documentation of the learner's implementation, e.g., [here](https://xgboost.readthedocs.io/en/stable/gpu/index.html) for [`xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\n:::\n\n### Implicit Parallelization\n\nWe talk about implicit parallelization in this context if we call external code (i.e., code from foreign CRAN packages) which runs in parallel.\nMany machine learning algorithms can parallelize their model fit using threading, e.g., [`ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html)\nor [`xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html).\nUnfortunately, threading conflicts with certain parallel backends used during explicit parallelization, causing the system to be overutilized in the best case and causing hangs or segfaults in the worst case.\nFor this reason, we introduced the convention that implicit parallelization is turned off per default.\nHyperparameters that control the number of threads are tagged with the label `\"threads\"`.\n\n\n::: {.cell hash='technical_cache/html/technical-001_db310e56f381d2b4ceb7ee2cbe2d197f'}\n\n```{.r .cell-code}\nlibrary(\"mlr3learners\") # for the ranger learner\n\nlearner = lrn(\"classif.ranger\")\nlearner$param_set$ids(tags = \"threads\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"num.threads\"\n```\n:::\n:::\n\n\nTo enable the parallelization for this learner, we provide the helper function [`set_threads()`](https://mlr3.mlr-org.com/reference/set_threads.html) which\n\n::: {.cell hash='technical_cache/html/technical-002_e4d4308faac0e1f82781857554a4e503'}\n\n```{.r .cell-code}\n# use 4 CPUs\nset_threads(learner, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=4\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n```\n:::\n\n```{.r .cell-code}\n# auto-detect cores on the local machine\nset_threads(learner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=8\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n```\n:::\n:::\n\n\n:::{.callout-caution}\nAutomatic detection of the number of CPUs is sometimes flaky, and utilizing all available cores is occasionally counterproductive as overburdening the system often has negative effects on the overall runtime.\nThe function which determines the number of CPUs is implemented in [`parallelly::availableCores()`](https://www.rdocumentation.org/packages/parallelly/topics/availableCores) and already comes with reasonable heuristics for many setups.\nHowever, there are still some scenarios where it is better to reduce the number of utilized CPUs manually:\n\n* You want to simultaneously work on the same system, e.g., write a report and do some data analysis.\n* You are on a multi-user system and want to spare some resources for other users.\n* You have energy-efficient CPU cores, for example, the \"Icestorm\" cores on a Mac M1 chip.\n  These are comparably slower than the high-performance \"Firestorm\" cores and not well suited for heavy computations.\n* You have linked R to a parallel [BLAS](https://www.wikiwand.com/en/Basic_Linear_Algebra_Subprograms) implementation like [OpenBLAS](https://www.openblas.net/), and your learners make heavy use of linear algebra.\n\nYou can set the number of CPUs via option `\"mc.cores\"`:\n\n\n::: {.cell hash='technical_cache/html/technical-003_8b5f57a17b84d682a9cd17db7844bb94'}\n\n```{.r .cell-code}\noptions(mc.cores = 4)\n```\n:::\n\nWe recommend setting this in your system's `.Rprofile` file, c.f. [`Startup`](https://www.rdocumentation.org/packages/base/topics/Startup).\n:::\n\nSettings threads for implicit parallelization also works for filters from the [mlr3filters](https://mlr3filters.mlr-org.com) package as well as lists of objects, even if some objects do not support threading at all:\n\n::: {.cell hash='technical_cache/html/technical-004_ab18aa2a947d680de64df92e68a1793f'}\n\n```{.r .cell-code}\nlibrary(\"mlr3filters\")\n\n# retrieve 2 filters\n# * variance filter with no support for threading\n# * mrmr filter with threading support\nfilters = flts(c(\"variance\", \"mrmr\"))\n\n# set threads for all filters which support it\nset_threads(filters, n = 4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n<FilterVariance:variance>: Variance\nTask Types: NA\nTask Properties: -\nPackages: mlr3filters, stats\nFeature types: integer, numeric\n\n[[2]]\n<FilterMRMR:mrmr>: Minimum Redundancy Maximal Relevancy\nTask Types: classif, regr\nTask Properties: -\nPackages: mlr3filters, praznik\nFeature types: integer, numeric, factor, ordered\n```\n:::\n\n```{.r .cell-code}\n# variance filter is unchanged\nfilters[[1]]$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n      id    class lower upper nlevels default value\n1: na.rm ParamLgl    NA    NA       2    TRUE      \n```\n:::\n\n```{.r .cell-code}\n# mrmr now works in parallel with 4 cores\nfilters[[2]]$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n        id    class lower upper nlevels default value\n1: threads ParamInt     0   Inf     Inf       0     4\n```\n:::\n:::\n\n\n### Explicit Parallelization\n\nWe talk about explicit parallelization if [mlr3](https://mlr3.mlr-org.com) starts the parallelization itself.\nThe abstraction implemented in [future](https://cran.r-project.org/package=future) is used to support a broad range of parallel backends.\nThere are two use cases where [mlr3](https://mlr3.mlr-org.com) calls the `future` package: [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) and [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html).\nDuring resampling, because all resampling iterations are independent of each other, all iterations can be executed in parallel.\nThe same holds for benchmarking, where additionally, all combinations in the provided design are also independent.\nThese iterations are performed by [future](https://cran.r-project.org/package=future) using the parallel backend configured with [`future::plan()`](https://www.rdocumentation.org/packages/future/topics/plan).\nExtension packages like [mlr3tuning](https://mlr3tuning.mlr-org.com) internally call [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) during tuning and thus work in parallel, too.\n\n\n:::{.callout-tip}\nWhen computational problems are so easy to parallelize, they are often referred to as \"embarrassingly parallel\".\n\nWhenever you loop over elements with a map-like function (e.g., [`lapply()`](https://www.rdocumentation.org/packages/base/topics/lapply), [`sapply()`](https://www.rdocumentation.org/packages/base/topics/lapply), [`mapply()`](https://www.rdocumentation.org/packages/base/topics/mapply), [`vapply()`](https://www.rdocumentation.org/packages/base/topics/lapply) or a function from [purrr](https://cran.r-project.org/package=purrr)), you are facing an embarrassingly parallel problem.\nSuch problems are straight-forward to parallelize, e.g., with the [furrr](https://cran.r-project.org/package=furrr) package providing map-like functions executed in parallel via the [future](https://cran.r-project.org/package=future) framework.\nThe same holds for `for`-loops with independent iterations, i.e., loops where the current iteration does not rely on the results of previous iterations.\n:::\n\nIn this section, we will use the [`spam task`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html) and a simple [`classification tree`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html) to showcase the explicit parallelization.\nWe use the [`future::multisession`](https://www.rdocumentation.org/packages/future/topics/multisession) parallel backend that should work on all systems.\n\n\n::: {.cell hash='technical_cache/html/technical-005_56b9b746041ec16312ad94776bc97339'}\n\n```{.r .cell-code}\n# select the multisession backend\nfuture::plan(\"multisession\")\n\ntask = tsk(\"spam\")\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"subsampling\")\n\ntime = Sys.time()\nresample(task, learner, resampling)\nSys.time() - time\n```\n:::\n\n\nBy default, all CPUs of your machine are used unless you specify the argument `workers` in [`future::plan()`](https://www.rdocumentation.org/packages/future/topics/plan) (possible problems with this default have already been discussed for implicit parallelization).\nYou should see a decrease in the reported elapsed time, but in practice, you cannot expect the runtime to fall linearly as the number of cores increases ([Amdahl's law](https://www.wikiwand.com/en/Amdahl%27s_law)).\nIn contrast to threads and depending on the parallel backend, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers can be quite large.\nTherefore, it is advised only consider parallelization for resamplings where each iteration runs for multiple seconds.\n\n:::{.callout-note}\nIf you are transitioning from [mlr](https://cran.r-project.org/package=mlr), you might be used to selecting different parallelization levels, e.g., for resampling, benchmarking, or tuning.\nIn [mlr3](https://mlr3.mlr-org.com), this is no longer required (except for nested resampling, briefly described in the following section).\nAll kind of experiments are rolled out on the same level.\nTherefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.\n\nJust lean back and let the machine do the work :-)\n:::\n\n\n### Nested Resampling Parallelization {#nested-resampling-parallelization}\n\n[Nested resampling](#nested-resampling) results in two nested resampling loops.\nWe can choose different parallelization backends for the inner and outer resampling loop, respectively.\nWe just have to pass a list of [future](https://cran.r-project.org/package=future) backends:\n\n\n::: {.cell hash='technical_cache/html/technical-006_5c0b92609497357d68571ffceb67c60b'}\n\n```{.r .cell-code}\n# Runs the outer loop in parallel and the inner loop sequentially\nfuture::plan(list(\"multisession\", \"sequential\"))\n# Runs the outer loop sequentially and the inner loop in parallel\nfuture::plan(list(\"sequential\", \"multisession\"))\n```\n:::\n\n\nWhile nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups.\nIt can be achieved with [future](https://cran.r-project.org/package=future) by forcing a fixed number of workers for each loop:\n\n\n::: {.cell hash='technical_cache/html/technical-007_bc1c0c70c6b6c134b0284269a45f9732'}\n\n```{.r .cell-code}\n# Runs both loops in parallel\nfuture::plan(list(future::tweak(\"multisession\", workers = 2),\n  future::tweak(\"multisession\", workers = 4)))\n```\n:::\n\n\nThis example would run on 8 cores (`= 2 * 4`) on the local machine.\nThe [vignette](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html) of the [future](https://cran.r-project.org/package=future) package gives more insight into nested parallelization.\nFor more background information about parallelization during tuning, see Section 6.7 of @hpo_practical.\n\n:::{.callout-caution}\nDuring tuning with [mlr3tuning](https://mlr3tuning.mlr-org.com), you can often adjust the **batch size** of the [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html), i.e., control how many hyperparameter configurations are evaluated in parallel.\nIf you want full parallelization, make sure that the batch size multiplied by the number of (inner) resampling iterations is at least equal to the number of cores or workers.\n\nIn general, larger batches mean more parallelization, while smaller batches imply a more fine-grained checking of termination criteria.\nWe default to a `batch_size` of 1 that ensures that all [`Terminators`](https://bbotk.mlr-org.com/reference/Terminator.html) work as intended, and you cannot exceed the computational budget.\n:::\n\n## Error Handling {#error-handling}\n\nTo demonstrate how to properly deal with misbehaving learners, [mlr3](https://mlr3.mlr-org.com) ships with the learner [`classif.debug`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.debug.html):\n\n\n::: {.cell hash='technical_cache/html/technical-008_0131c2b47e2fa8ed52388756b0e92cc2'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nprint(learner)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifDebug:classif.debug>: Debug Learner for Classification\n* Model: -\n* Parameters: list()\n* Packages: mlr3\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_forward, missings, multiclass, twoclass\n```\n:::\n:::\n\n\nThis learner comes with special hyperparameters that let us control\n\n1. what conditions should be signaled (message, warning, error, segfault) with what probability.\n1. during which stage the conditions should be signaled (train or predict).\n1. the ratio of predictions being `NA` (`predict_missing`).\n\n\n::: {.cell hash='technical_cache/html/technical-009_6a5fcc61908aaf6ca7fcedead020c821'}\n\n```{.r .cell-code}\nlearner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                      id    class lower upper nlevels        default value\n 1:        error_predict ParamDbl     0     1     Inf              0      \n 2:          error_train ParamDbl     0     1     Inf              0      \n 3:      message_predict ParamDbl     0     1     Inf              0      \n 4:        message_train ParamDbl     0     1     Inf              0      \n 5:      predict_missing ParamDbl     0     1     Inf              0      \n 6: predict_missing_type ParamFct    NA    NA       2             na      \n 7:           save_tasks ParamLgl    NA    NA       2          FALSE      \n 8:     segfault_predict ParamDbl     0     1     Inf              0      \n 9:       segfault_train ParamDbl     0     1     Inf              0      \n10:          sleep_train ParamUty    NA    NA     Inf <NoDefault[3]>      \n11:        sleep_predict ParamUty    NA    NA     Inf <NoDefault[3]>      \n12:              threads ParamInt     1   Inf     Inf <NoDefault[3]>      \n13:      warning_predict ParamDbl     0     1     Inf              0      \n14:        warning_train ParamDbl     0     1     Inf              0      \n15:                    x ParamDbl     0     1     Inf <NoDefault[3]>      \n16:                 iter ParamInt     1   Inf     Inf              1      \n```\n:::\n:::\n\n\nWith the learner's default settings, the learner will do nothing special: The learner remembers a random label and creates constant predictions.\n\n\n::: {.cell hash='technical_cache/html/technical-010_1189e908b0915ae652d3967268e6aeed'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner$train(task)$predict(task)$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            truth\nresponse     setosa versicolor virginica\n  setosa         50         50        50\n  versicolor      0          0         0\n  virginica       0          0         0\n```\n:::\n:::\n\n\nWe now set a hyperparameter to let the debug learner signal an error during the train step.\nBy default, [mlr3](https://github.com/mlr-org/mlr3) does not catch conditions such as warnings or errors raised by third-party code like learners:\n\n\n::: {.cell hash='technical_cache/html/technical-011_f71299701d6fc26d2ac57196bcd4ae62'}\n\n```{.r .cell-code}\nlearner$param_set$values = list(error_train = 1)\nlearner$train(tsk(\"iris\"))\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in .__LearnerClassifDebug__.train(self = self, private = private, : Error from classif.debug->train()\n```\n:::\n:::\n\nIf this would be a regular learner, we could now start debugging with [`traceback()`](https://www.rdocumentation.org/packages/base/topics/traceback) (or create a [MRE](https://stackoverflow.com/help/minimal-reproducible-example) to file a bug report).\n\nHowever, machine learning algorithms raising errors is not uncommon as algorithms typically cannot process all possible data.\nThus, we need a mechanism to\n\n  1. capture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc, and\n  1. a statistically sound way to proceed the calculation and be able to aggregate over partial results.\n\nThese two mechanisms are explained in the following subsections.\n\n### Encapsulation {#encapsulation}\n\nWith encapsulation, exceptions do not stop the program flow and all output is logged to the learner (instead of printed to the console).\nEach [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) has a field `encapsulate` to control how the train or predict steps are executed.\nOne way to encapsulate the execution is provided by the package [evaluate](https://cran.r-project.org/package=evaluate) (see the documentation of the [`encapsulate()`](https://mlr3misc.mlr-org.com/reference/encapsulate.html) helper function for more details):\n\n\n::: {.cell hash='technical_cache/html/technical-012_e904ab124f659296bb03d07f05693eeb'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nlearner$param_set$values = list(warning_train = 1, error_train = 1)\nlearner$encapsulate = c(train = \"evaluate\", predict = \"evaluate\")\n\nlearner$train(task)\n```\n:::\n\n\nAfter training the learner, one can access the recorded log via the fields `log`, `warnings` and `errors`:\n\n\n::: {.cell hash='technical_cache/html/technical-013_9f5e0d3b508541d60bedbe0ab920ead8'}\n\n```{.r .cell-code}\nlearner$log\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   stage   class                                 msg\n1: train warning Warning from classif.debug->train()\n2: train   error   Error from classif.debug->train()\n```\n:::\n\n```{.r .cell-code}\nlearner$warnings\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Warning from classif.debug->train()\"\n```\n:::\n\n```{.r .cell-code}\nlearner$errors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Error from classif.debug->train()\"\n```\n:::\n:::\n\n\nAnother method for encapsulation is implemented in the [callr](https://cran.r-project.org/package=callr) package.\n[callr](https://cran.r-project.org/package=callr) spawns a new R process to execute the respective step and thus even guards the current R session against segfaults.\nOn the downside, starting new processes comes with comparably more computational overhead.\n\n\n::: {.cell hash='technical_cache/html/technical-014_21cc308d198a8d11a04d8ee8857f2156'}\n\n```{.r .cell-code}\nlearner$encapsulate = c(train = \"callr\", predict = \"callr\")\nlearner$param_set$values = list(segfault_train = 1)\nlearner$train(task = task)\nlearner$errors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"callr process exited with status -11\"\n```\n:::\n:::\n\n\nWithout a model, it is not possible to get predictions though:\n\n\n::: {.cell hash='technical_cache/html/technical-015_8d9a6769425d1e534891528fe2fcdaab'}\n\n```{.r .cell-code}\nlearner$predict(task)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError: Cannot predict, Learner 'classif.debug' has not been trained yet\n```\n:::\n:::\n\n\nTo handle the missing predictions in a graceful way during [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) or [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html), fallback learners are introduced next.\n\n### Fallback learners\n\nFallback learners have the purpose of allowing scoring results in cases where a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) is misbehaving.\nWe will first handle the case that a learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory.\nThere are three possibilities to proceed:\n\n1. Ignore missing scores.\n  Although this is arguably the most frequent approach in practice, this is not statistically sound.\n  For example, consider the case where a researcher wants a specific learner to look better in a benchmark study.\n  To do this, the researcher takes an existing learner but introduces a small adaptation: If an internal goodness-of-fit measure is not achieved, an error is thrown.\n  In other words, the learner only fits a model if the model can be reasonably well learned on the given training data.\n  In comparison with the learning procedure without this adaptation and a good threshold, however, we now compare the mean over only the \"easy\" splits with the mean over all splits - an unfair advantage.\n2. Penalize failing learners.\n  If a score is missing, we can simply impute the worst possible score (as defined by the [`Measure`](https://mlr3.mlr-org.com/reference/Measure.html)) and thereby heavily penalize the learner for failing.\n  However, this often seems too harsh for many problems.\n3. Impute a value that corresponds to a (weak) baseline.\n  Instead of imputing with the worst possible score, impute with a reasonable baseline, e.g., by just predicting the majority class or the mean of the training data with a featureless learner.\n  Note that the imputed value depends on the data in the training split.\n  Retrieving the right values after a larger benchmark is possible but tedious.\n\n\nWe strongly recommend option (3).\nTo make this procedure very convenient during resampling and benchmarking, we support fitting a proper baseline with a fallback learner.\nIn the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner.\nSo whenever the debug learner fails (which is every single time with the given parametrization) and encapsulation is enabled, [mlr3](https://mlr3.mlr-org.com) falls back to the predictions of the featureless learner internally:\n\n\n::: {.cell hash='technical_cache/html/technical-016_edf917be2101dd48b767648820671490'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\nlearner$param_set$values = list(error_train = 1)\nlearner$fallback = lrn(\"classif.featureless\")\nlearner$train(task)\nlearner\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifDebug:classif.debug>: Debug Learner for Classification\n* Model: -\n* Parameters: error_train=1\n* Packages: mlr3\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_forward, missings, multiclass, twoclass\n* Errors: Error from classif.debug->train()\n```\n:::\n:::\n\n\nNote that we don't have to enable encapsulation explicitly; it is automatically set to `\"evaluate\"` for the training and the predict step while setting a fallback learner for a learner without encapsulation enabled.\nFurthermore, the log contains the captured error (which is also included in the print output), and although we don't have a model, we can still get predictions:\n\n\n::: {.cell hash='technical_cache/html/technical-017_ea3469d3add71d6c70b319fe098cad44'}\n\n```{.r .cell-code}\nlearner$model\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n\n```{.r .cell-code}\nprediction = learner$predict(task)\nprediction$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.6666667 \n```\n:::\n:::\n\n\nIn this this stepwise train-predict procedure, the fallback learner is of limited use.\nHowever, it is invaluable for larger benchmark studies.\n\nIn the following snippet we compare the previously created debug learner with a simple classification tree.\nWe re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step:\n\n\n::: {.cell hash='technical_cache/html/technical-018_86dd2634450284ba25c1f53de8879870'}\n\n```{.r .cell-code}\nlearner$param_set$values = list(error_train = 0.3)\n\nbmr = benchmark(benchmark_grid(tsk(\"iris\"), list(learner, lrn(\"classif.rpart\")), rsmp(\"cv\")))\naggr = bmr$aggregate(conditions = TRUE)\naggr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id    learner_id resampling_id iters warnings\n1:  1 <ResampleResult[21]>    iris classif.debug            cv    10        0\n2:  2 <ResampleResult[21]>    iris classif.rpart            cv    10        0\n2 variables not shown: [errors, classif.ce]\n```\n:::\n:::\n\n\nEven though the debug learner occasionally failed to provide predictions, we still got a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree.\nTo further investigate the errors, we can also extract the `\"ResampleResult\")`:\n\n\n::: {.cell hash='technical_cache/html/technical-019_33a2e2cb2b474b5c7917a85750cccbc5'}\n\n```{.r .cell-code}\nrr = aggr[learner_id == \"classif.debug\"]$resample_result[[1L]]\nrr$errors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration                               msg\n1:         1 Error from classif.debug->train()\n2:         2 Error from classif.debug->train()\n3:         5 Error from classif.debug->train()\n4:         6 Error from classif.debug->train()\n```\n:::\n:::\n\n\n\nA similar problem emerges when a learner predicts only a subset of the observations in the test set (and predicts `NA` for others).\nA typical case is, e.g., when new and unseen factor levels are encountered in the test data.\nImagine again that our goal is to benchmark two algorithms using a cross validation on some binary classification task:\n\n* Algorithm A is an ordinary logistic regression.\n* Algorithm B is also an ordinary logistic regression, but with a twist:\n  If the logistic regression is rather certain about the predicted label (> 90% probability), it returns the label and returns a missing value otherwise.\n\nClearly, at its core, this is the same problem as outlined before.\nAlgorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for all observations.\nLong story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner.\nThis is illustrated in the following example:\n\n::: {.cell hash='technical_cache/html/technical-020_a4b816efc76e65ba865b44d07088fa85'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nlearner = lrn(\"classif.debug\")\n\n# this hyperparameter sets the ratio of missing predictions\nlearner$param_set$values = list(predict_missing = 0.5)\n\n# without fallback\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    setosa versicolor  virginica       <NA> \n        75          0          0         75 \n```\n:::\n\n```{.r .cell-code}\n# with fallback\nlearner$fallback = lrn(\"classif.featureless\")\np = learner$train(task)$predict(task)\ntable(p$response, useNA = \"always\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n    setosa versicolor  virginica       <NA> \n        75          0         75          0 \n```\n:::\n:::\n\n\nSummed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or unstable learning algorithms in a convenient and statistically sound fashion.\n\n## Database Backends {#backends}\n\nIn mlr3, [`Tasks`](https://mlr3.mlr-org.com/reference/Task.html) store their data in an abstract data object, the [`DataBackend`](https://mlr3.mlr-org.com/reference/DataBackend.html).\nThe default backend uses [data.table](https://cran.r-project.org/package=data.table) via the [`DataBackendDataTable`](https://mlr3.mlr-org.com/reference/DataBackendDataTable.html) as an very fast and efficient in-memory data base.\n\nFor bigger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data to reduce the memory requirements.\nThere are multiple options to archive this:\n\n1. [`DataBackendDplyr`](https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html) which interfaces the R package [dbplyr](https://cran.r-project.org/package=dbplyr), extending [dplyr](https://cran.r-project.org/package=dplyr) to work on many popular data bases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).\n2. [`DataBackendDuckDB`](https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html) for the impressive [DuckDB](https://duckdb.org/) data base connected via [duckdb](https://cran.r-project.org/package=duckdb): a fast, zero configuration alternative to SQLite.\n3. [`DataBackendDuckDB`](https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html), again, but to work directly on [parquet files](https://parquet.apache.org/).\n\n\n### DataBackendDplyr\n\nTo demonstrate the [`DataBackendDplyr`](https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html) we use the NYC flights data set from the [nycflights13](https://cran.r-project.org/package=nycflights13) package and move it into a SQLite data base.\nAlthough `as_sqlite_backend()` provides a convenient function to perform this step, we contruct the data base manually here.\n\n\n::: {.cell hash='technical_cache/html/technical-021_8234d3d2bd4b0974c014946826439915'}\n\n```{.r .cell-code}\n# load data\nrequireNamespace(\"DBI\")\nrequireNamespace(\"RSQLite\")\nrequireNamespace(\"nycflights13\")\ndata(\"flights\", package = \"nycflights13\")\nstr(flights)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n```\n:::\n\n```{.r .cell-code}\n# add column of unique row ids\nflights$row_id = 1:nrow(flights)\n\n# create sqlite database in temporary file\npath = tempfile(\"flights\", fileext = \".sqlite\")\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\ntbl = DBI::dbWriteTable(con, \"flights\", as.data.frame(flights))\nDBI::dbDisconnect(con)\n\n# remove in-memory data\nrm(flights)\n```\n:::\n\n\n#### Preprocessing with `dplyr`\n\nWith the SQLite database in `path`, we now re-establish a connection and switch to [dplyr](https://cran.r-project.org/package=dplyr)/[dbplyr](https://cran.r-project.org/package=dbplyr) for some essential preprocessing.\n\n\n::: {.cell hash='technical_cache/html/technical-022_75f60b38aa7a3e16d6eeab3bae7796a6'}\n\n```{.r .cell-code}\n# establish connection\ncon = DBI::dbConnect(RSQLite::SQLite(), path)\n\n# select the \"flights\" table, enter dplyr\nlibrary(\"dplyr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(\"dbplyr\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dbplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    ident, sql\n```\n:::\n\n```{.r .cell-code}\ntbl = tbl(con, \"flights\")\n```\n:::\n\n\nFirst, we select a subset of columns to work on:\n\n\n::: {.cell hash='technical_cache/html/technical-023_ce94f97007c00ef9f076376aefd6ac6a'}\n\n```{.r .cell-code}\nkeep = c(\"row_id\", \"year\", \"month\", \"day\", \"hour\", \"minute\", \"dep_time\",\n  \"arr_time\", \"carrier\", \"flight\", \"air_time\", \"distance\", \"arr_delay\")\ntbl = select(tbl, all_of(keep))\n```\n:::\n\n\nAdditionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:\n\n\n::: {.cell hash='technical_cache/html/technical-024_b9dc3ac533a5e8c971b8576bb6d4aa73'}\n\n```{.r .cell-code}\ntbl = filter(tbl, !is.na(arr_delay))\n```\n:::\n\n\nTo keep runtime reasonable for this toy example, we filter the data to only use every second row:\n\n\n::: {.cell hash='technical_cache/html/technical-025_cdc22db422af37633c3f6840e7a63c0f'}\n\n```{.r .cell-code}\ntbl = filter(tbl, row_id %% 2 == 0)\n```\n:::\n\n\nThe factor levels of the feature `carrier` are merged so that infrequent carriers are replaced by level \"other\":\n\n\n::: {.cell hash='technical_cache/html/technical-026_ac538a5d2a818bd73d73be4842c6be78'}\n\n```{.r .cell-code}\ntbl = mutate(tbl, carrier = case_when(\n  carrier %in% c(\"OO\", \"HA\", \"YV\", \"F9\", \"AS\", \"FL\", \"VX\", \"WN\") ~ \"other\",\n  TRUE ~ carrier))\n```\n:::\n\n\n#### Creating the Backend\n\nThe processed table is now used to create a [`mlr3db::DataBackendDplyr`](https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html) from [mlr3db](https://mlr3db.mlr-org.com):\n\n\n::: {.cell hash='technical_cache/html/technical-027_920b91181d69c3d8d0a52d32ba6e4ba9'}\n\n```{.r .cell-code}\nlibrary(\"mlr3db\")\nb = as_data_backend(tbl, primary_key = \"row_id\")\n```\n:::\n\n\nWe can now use the interface of [`DataBackend`](https://mlr3.mlr-org.com/reference/DataBackend.html) to query some basic information of the data:\n\n\n::: {.cell hash='technical_cache/html/technical-028_5fa75aacafb56c3917c7e69660e53939'}\n\n```{.r .cell-code}\nb$nrow\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 163707\n```\n:::\n\n```{.r .cell-code}\nb$ncol\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13\n```\n:::\n\n```{.r .cell-code}\nb$head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   row_id year month day hour minute dep_time arr_time carrier flight air_time\n1:      2 2013     1   1    5     29      533      850      UA   1714      227\n2:      4 2013     1   1    5     45      544     1004      B6    725      183\n3:      6 2013     1   1    5     58      554      740      UA   1696      150\n4:      8 2013     1   1    6      0      557      709      EV   5708       53\n5:     10 2013     1   1    6      0      558      753      AA    301      138\n6:     12 2013     1   1    6      0      558      853      B6     71      158\n2 variables not shown: [distance, arr_delay]\n```\n:::\n:::\n\n\nNote that the [`DataBackendDplyr`](https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html) does not know about any rows or columns we have filtered out with [dplyr](https://cran.r-project.org/package=dplyr) before, it just operates on the view we provided.\n\n#### Model fitting\n\nWe create the following [mlr3](https://mlr3.mlr-org.com) objects:\n\n* A [`regression task`](https://mlr3.mlr-org.com/reference/TaskRegr.html), based on the previously created [`mlr3db::DataBackendDplyr`](https://mlr3db.mlr-org.com/reference/DataBackendDplyr.html).\n* A regression learner ([`regr.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.rpart.html)).\n* A resampling strategy: 3 times repeated subsampling using 2\\% of the observations for training (\"[`subsampling`](https://mlr3.mlr-org.com/reference/mlr_resamplings_subsampling.html)\")\n* Measures \"[`mse`](https://mlr3.mlr-org.com/reference/mlr_measures_regr.mse.html)\", \"[`time_train`](https://mlr3.mlr-org.com/reference/mlr_measures_elapsed_time.html)\" and \"[`time_predict`](https://mlr3.mlr-org.com/reference/mlr_measures_elapsed_time.html)\"\n\n\n::: {.cell hash='technical_cache/html/technical-029_bf03417b0b39a8bf8aede842223dba77'}\n\n```{.r .cell-code}\ntask = as_task_regr(b, id = \"flights_sqlite\", target = \"arr_delay\")\nlearner = lrn(\"regr.rpart\")\nmeasures = mlr_measures$mget(c(\"regr.mse\", \"time_train\", \"time_predict\"))\nresampling = rsmp(\"subsampling\", repeats = 3, ratio = 0.02)\n```\n:::\n\n\nWe pass all these objects to [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) to perform a simple resampling with three iterations.\nIn each iteration, only the required subset of the data is queried from the SQLite data base and passed to [`rpart::rpart()`](https://www.rdocumentation.org/packages/rpart/topics/rpart):\n\n\n::: {.cell hash='technical_cache/html/technical-030_d3fa13fc84bece23c78de78261cf8cb9'}\n\n```{.r .cell-code}\nrr = resample(task, learner, resampling)\nprint(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 3 iterations\n* Task: flights_sqlite\n* Learner: regr.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n\n```{.r .cell-code}\nrr$aggregate(measures)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    regr.mse   time_train time_predict \n    1250.042        0.000        0.000 \n```\n:::\n:::\n\n\n#### Cleanup\n\nFinally, we remove the `tbl` object and close the connection.\n\n\n::: {.cell hash='technical_cache/html/technical-031_46ff37a417adc05b4bfc8fe8b22f8c28'}\n\n```{.r .cell-code}\nrm(tbl)\nDBI::dbDisconnect(con)\n```\n:::\n\n\n\n### DataBackendDuckDB\n\n#### Working with DuckDBs\n\nWhile storing the Task's data in memory is most efficient w.r.t. accessing it, this has two major disadvantages:\n\n1. Although you might only need a small proportion of the data, the complete data frame sits in memory and consumes memory.\n  This is especially a problem if you work with many tasks simultaneously.\n2. During parallelization, the complete data needs to be transferred to the workers which can cause a significant overhead.\n\nA very simple way to avoid this is given by just converting the [`DataBackendDataTable`](https://mlr3.mlr-org.com/reference/DataBackendDataTable.html) to a [`DataBackendDuckDB`](https://mlr3db.mlr-org.com/reference/DataBackendDuckDB.html).\n\n...\n\n#### Working with Parquet Files\n\n## Parameters (using paradox) {#paradox}\n\nThe [paradox](https://paradox.mlr-org.com) package offers a language for the description of *parameter spaces*, as well as tools for useful operations on these parameter spaces.\nA parameter space is often useful when describing:\n\n* A set of sensible input values for an R function\n* The set of possible values that slots of a configuration object can take\n* The search space of an optimization process\n\nThe tools provided by [paradox](https://paradox.mlr-org.com) therefore relate to:\n\n* **Parameter checking**: Verifying that a set of parameters satisfies the conditions of a parameter space\n* **Parameter sampling**: Generating parameter values that lie in the parameter space for systematic exploration of program behavior depending on these parameters\n\n[paradox](https://paradox.mlr-org.com) is, by nature, an auxiliary package that derives its usefulness from other packages that make use of it.\nIt is heavily utilized in other [mlr-org](https://github.com/mlr-org) packages such as [mlr3](https://mlr3.mlr-org.com), [mlr3pipelines](https://mlr3pipelines.mlr-org.com), and [mlr3tuning](https://mlr3tuning.mlr-org.com).\n\n### Reference Based Objects\n\n[paradox](https://paradox.mlr-org.com) is the spiritual successor to the [ParamHelpers](https://cran.r-project.org/package=ParamHelpers) package and was written from scratch using the [R6](https://cran.r-project.org/package=R6) class system.\nThe most important consequence of this is that all objects created in [paradox](https://paradox.mlr-org.com) are \"reference-based\", unlike most other objects in R.\nWhen a change is made to a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) object, for example by adding a parameter using the `$add()` function, all variables that point to this [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) will contain the changed object.\nTo create an independent copy of a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), the `$clone()` method needs to be used:\n\n\n::: {.cell hash='technical_cache/html/technical-032_ddfba0d88759f50bc54ad4c4e7f2906d'}\n\n```{.r .cell-code}\nlibrary(\"paradox\")\n\nps = ParamSet$new()\nps2 = ps\nps3 = ps$clone(deep = TRUE)\nprint(ps) # the same for ps2 and ps3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\nEmpty.\n```\n:::\n:::\n\n::: {.cell hash='technical_cache/html/technical-033_39fdb209723b5d89de83e040f878f3e1'}\n\n```{.r .cell-code}\nps$add(ParamLgl$new(\"a\"))\n```\n:::\n\n::: {.cell hash='technical_cache/html/technical-034_106d7585c4cc52b80d3ec7c6aa74cb80'}\n\n```{.r .cell-code}\nprint(ps)  # ps was changed\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  a ParamLgl    NA    NA       2 <NoDefault[3]>      \n```\n:::\n\n```{.r .cell-code}\nprint(ps2) # contains the same reference as ps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  a ParamLgl    NA    NA       2 <NoDefault[3]>      \n```\n:::\n\n```{.r .cell-code}\nprint(ps3) # is a \"clone\" of the old (empty) ps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\nEmpty.\n```\n:::\n:::\n\n\n### Defining a Parameter Space\n\n#### Single Parameters\n\nThe basic building block for describing parameter spaces is the **[`Param`](https://paradox.mlr-org.com/reference/Param.html)** class.\nIt represents a single parameter, which usually can take a single atomic value.\nConsider, for example, trying to configure the [rpart](https://cran.r-project.org/package=rpart) package's `rpart.control` object.\nIt has various components (`minsplit`, `cp`, ...) that all take a single value, and that would all be represented by a different instance of a [`Param`](https://paradox.mlr-org.com/reference/Param.html) object.\n\nThe [`Param`](https://paradox.mlr-org.com/reference/Param.html) class has various sub-classes that represent different value types:\n\n* [`ParamInt`](https://paradox.mlr-org.com/reference/ParamInt.html): Integer numbers\n* [`ParamDbl`](https://paradox.mlr-org.com/reference/ParamDbl.html): Real numbers\n* [`ParamFct`](https://paradox.mlr-org.com/reference/ParamFct.html): String values from a set of possible values, similar to R `factor`s\n* [`ParamLgl`](https://paradox.mlr-org.com/reference/ParamLgl.html): Truth values (`TRUE` / `FALSE`), as `logical`s in R\n* [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html): Parameter that can take any value\n\nA particular instance of a parameter is created by calling the attached `$new()` function:\n\n\n::: {.cell hash='technical_cache/html/technical-035_8f780e256ec4e244f52955cb8542f1c2'}\n\n```{.r .cell-code}\nlibrary(\"paradox\")\nparA = ParamLgl$new(id = \"A\")\nparB = ParamInt$new(id = \"B\", lower = 0, upper = 10, tags = c(\"tag1\", \"tag2\"))\nparC = ParamDbl$new(id = \"C\", lower = 0, upper = 4, special_vals = list(NULL))\nparD = ParamFct$new(id = \"D\", levels = c(\"x\", \"y\", \"z\"), default = \"y\")\nparE = ParamUty$new(id = \"E\", custom_check = function(x) checkmate::checkFunction(x))\n```\n:::\n\n\nEvery parameter must have:\n\n* **id** - A name for the parameter within the parameter set\n* **default** - A default value\n* **special_vals** - A list of values that are accepted even if they do not conform to the type\n* **tags** - Tags that can be used to organize parameters\n\nThe numeric (`Int` and `Dbl`) parameters furthermore allow for specification of a **lower** and **upper** bound.\nMeanwhile, the `Fct` parameter must be given a vector of **levels** that define the possible states its parameter can take.\nThe `Uty` parameter can also have a **`custom_check`** function that must return `TRUE` when a value is acceptable and may return a `character(1)` error description otherwise.\nThe example above defines `parE` as a parameter that only accepts functions.\n\nAll values which are given to the constructor are then accessible from the object for inspection using `$`.\nAlthough all these values can be changed for a parameter after construction, this can be a bad idea and should be avoided when possible.\n\nInstead, a new parameter should be constructed.\nBesides the possible values that can be given to a constructor, there are also the `$class`, `$nlevels`, `$is_bounded`, `$has_default`, `$storage_type`, `$is_number` and `$is_categ` slots that give information about a parameter.\n\nA list of all slots can be found in [`?Param`](https://paradox.mlr-org.com/reference/Param.html).\n\n\n::: {.cell hash='technical_cache/html/technical-036_b5e4e1922fbee5dd0698b516c22f580a'}\n\n```{.r .cell-code}\nparB$lower\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\nparA$levels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  TRUE FALSE\n```\n:::\n\n```{.r .cell-code}\nparE$class\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ParamUty\"\n```\n:::\n:::\n\n\nIt is also possible to get all information of a [`Param`](https://paradox.mlr-org.com/reference/Param.html) as `data.table` by calling [`as.data.table()`](https://www.rdocumentation.org/packages/data.table/topics/as.data.table).\n\n\n::: {.cell hash='technical_cache/html/technical-037_3a675b27ae8ed4e69f5dd40414b77ff1'}\n\n```{.r .cell-code}\nas.data.table(parA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id    class lower upper      levels nlevels is_bounded special_vals        default storage_type tags\n1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]> <NoDefault[3]>      logical     \n```\n:::\n:::\n\n\n##### Type / Range Checking\n\nA [`Param`](https://paradox.mlr-org.com/reference/Param.html) object offers the possibility to check whether a value satisfies its condition, i.e. is of the right type, and also falls within the range of allowed values, using the `$test()`, `$check()`, and `$assert()` functions.\n`test()` should be used within conditional checks and returns `TRUE` or `FALSE`, while `check()` returns an error description when a value does not conform to the parameter (and thus plays well with the `\"checkmate::assert()\"` function).\n`assert()` will throw an error whenever a value does not fit.\n\n\n::: {.cell hash='technical_cache/html/technical-038_1abe69e521a9615f99d8cef06187bd6d'}\n\n```{.r .cell-code}\nparA$test(FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nparA$test(\"FALSE\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n\n```{.r .cell-code}\nparA$check(\"FALSE\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Must be of type 'logical flag', not 'character'\"\n```\n:::\n:::\n\n\nInstead of testing single parameters, it is often more convenient to check a whole set of parameters using a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\n\n#### Parameter Sets\n\nThe ordered collection of parameters is handled in a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)^[Although the name is suggestive of a \"Set\"-valued [`Param`](https://paradox.mlr-org.com/reference/Param.html), this is unrelated to the other objects that follow the `ParamXxx` naming scheme.].\nIt is initialized using the `$new()` function and optionally takes a list of [`Param`](https://paradox.mlr-org.com/reference/Param.html)s as argument.\nParameters can also be added to the constructed [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) using the `$add()` function.\nIt is even possible to add whole [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s to other [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s.\n\n\n::: {.cell hash='technical_cache/html/technical-039_8f25270114a12e1c01dac787b071537c'}\n\n```{.r .cell-code}\nps = ParamSet$new(list(parA, parB))\nps$add(parC)\nps$add(ParamSet$new(list(parD, parE)))\nprint(ps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n2:  B ParamInt     0    10      11 <NoDefault[3]>      \n3:  C ParamDbl     0     4     Inf <NoDefault[3]>      \n4:  D ParamFct    NA    NA       3              y      \n5:  E ParamUty    NA    NA     Inf <NoDefault[3]>      \n```\n:::\n:::\n\n\nThe individual parameters can be accessed through the `$params` slot.\nIt is also possible to get information about all parameters in a vectorized fashion using mostly the same slots as for individual [`Param`](https://paradox.mlr-org.com/reference/Param.html)s (i.e. `$class`, `$levels` etc.), see `?ParamSet` for details.\n\nIt is possible to reduce [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s using the **`$subset`** method.\nBe aware that it modifies a ParamSet in-place, so a \"clone\" must be created first if the original [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) should not be modified.\n\n\n::: {.cell hash='technical_cache/html/technical-040_77e37d62a3c45019e3580aae29694bcc'}\n\n```{.r .cell-code}\npsSmall = ps$clone()\npsSmall$subset(c(\"A\", \"B\", \"C\"))\nprint(psSmall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n   id    class lower upper nlevels        default value\n1:  A ParamLgl    NA    NA       2 <NoDefault[3]>      \n2:  B ParamInt     0    10      11 <NoDefault[3]>      \n3:  C ParamDbl     0     4     Inf <NoDefault[3]>      \n```\n:::\n:::\n\n\nJust as for [`Param`](https://paradox.mlr-org.com/reference/Param.html)s, and much more useful, it is possible to get the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) as a `data.table` using [`as.data.table()`](https://www.rdocumentation.org/packages/data.table/topics/as.data.table).\nThis makes it easy to subset parameters on certain conditions and aggregate information about them, using the variety of methods provided by `data.table`.\n\n\n::: {.cell hash='technical_cache/html/technical-041_55abb49fa5a16c25f816831b0b19e987'}\n\n```{.r .cell-code}\nas.data.table(ps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id    class lower upper      levels nlevels is_bounded special_vals        default storage_type      tags\n1:  A ParamLgl    NA    NA  TRUE,FALSE       2       TRUE    <list[0]> <NoDefault[3]>      logical          \n2:  B ParamInt     0    10                  11       TRUE    <list[0]> <NoDefault[3]>      integer tag1,tag2\n3:  C ParamDbl     0     4                 Inf       TRUE    <list[1]> <NoDefault[3]>      numeric          \n4:  D ParamFct    NA    NA       x,y,z       3       TRUE    <list[0]>              y    character          \n5:  E ParamUty    NA    NA                 Inf      FALSE    <list[0]> <NoDefault[3]>         list          \n```\n:::\n:::\n\n\n##### Type / Range Checking\n\nSimilar to individual [`Param`](https://paradox.mlr-org.com/reference/Param.html)s, the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) provides `$test()`, `$check()` and `$assert()` functions that allow for type and range checking of parameters.\nTheir argument must be a named list with values that are checked against the respective parameters.\nIt is possible to check only a subset of parameters.\n\n\n::: {.cell hash='technical_cache/html/technical-042_e2d5af7103f2b0a632ab8adf25cbd248'}\n\n```{.r .cell-code}\nps$check(list(A = TRUE, B = 0, E = identity))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"A: Must be of type 'logical flag', not 'double'\"\n```\n:::\n\n```{.r .cell-code}\nps$check(list(Z = 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Parameter 'Z' not available. Did you mean 'A' / 'B' / 'C'?\"\n```\n:::\n:::\n\n\n##### Values in a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)\n\nAlthough a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) fundamentally represents a value space, it also has a slot `$values` that can contain a point within that space.\nThis is useful because many things that define a parameter space need similar operations (like parameter checking) that can be simplified.\nThe `$values` slot contains a named list that is always checked against parameter constraints.\nWhen trying to set parameter values, e.g. for [mlr3](https://mlr3.mlr-org.com) [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)s, it is the `$values` slot of its `$param_set` that needs to be used.\n\n\n::: {.cell hash='technical_cache/html/technical-043_fb166015959cd0b218ffd3a199b82891'}\n\n```{.r .cell-code}\nps$values = list(A = TRUE, B = 0)\nps$values$B = 1\nprint(ps$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$A\n[1] TRUE\n\n$B\n[1] 1\n```\n:::\n:::\n\n\nThe parameter constraints are automatically checked:\n\n\n::: {.cell hash='technical_cache/html/technical-044_46d0e52fe0a8c02980b4b2b5a44b7c87'}\n\n```{.r .cell-code}\nps$values$B = 100\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in self$assert(xs): Assertion on 'xs' failed: B: Element 1 is not <= 10.\n```\n:::\n:::\n\n\n##### Dependencies\n\nIt is often the case that certain parameters are irrelevant or should not be given depending on values of other parameters.\nAn example would be a parameter that switches a certain algorithm feature (for example regularization) on or off, combined with another parameter that controls the behavior of that feature (e.g. a regularization parameter).\nThe second parameter would be said to *depend* on the first parameter having the value `TRUE`.\n\nA dependency can be added using the `$add_dep` method, which takes both the ids of the \"depender\" and \"dependee\" parameters as well as a `Condition` object.\nThe `Condition` object represents the check to be performed on the \"dependee\".\nCurrently it can be created using `CondEqual$new()` and `CondAnyOf$new()`.\nMultiple dependencies can be added, and parameters that depend on others can again be depended on, as long as no cyclic dependencies are introduced.\n\nThe consequence of dependencies are twofold:\nFor one, the `$check()`, `$test()` and `$assert()` tests will not accept the presence of a parameter if its dependency is not met.\nFurthermore, when sampling or creating grid designs from a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), the dependencies will be respected (see [Parameter Sampling](#parameter-sampling), in particular [Hierarchical Sampler](#hierarchical-sampler)).\n\nThe following example makes parameter `D` depend on parameter `A` being `FALSE`, and parameter `B` depend on parameter `D` being one of `\"x\"` or `\"y\"`.\nThis introduces an implicit dependency of `B` on `A` being `FALSE` as well, because `D` does not take any value if `A` is `TRUE`.\n\n\n::: {.cell hash='technical_cache/html/technical-045_5d89733db1f17bf9a5483edd3bf70e20'}\n\n```{.r .cell-code}\nps$add_dep(\"D\", \"A\", CondEqual$new(FALSE))\nps$add_dep(\"B\", \"D\", CondAnyOf$new(c(\"x\", \"y\")))\n```\n:::\n\n::: {.cell hash='technical_cache/html/technical-046_cd60175309ceac42e5c903d9bd5b1cd9'}\n\n```{.r .cell-code}\nps$check(list(A = FALSE, D = \"x\", B = 1))          # OK: all dependencies met\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = FALSE, D = \"z\", B = 1))          # B's dependency is not met\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the current parameter value is: D=z\"\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = FALSE, B = 1))                   # B's dependency is not met\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = FALSE, D = \"z\"))                 # OK: B is absent\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = TRUE))                           # OK: neither B nor D present\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = TRUE, D = \"x\", B = 1))           # D's dependency is not met\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The parameter 'D' can only be set if the following condition is met 'A = FALSE'. Instead the current parameter value is: A=TRUE\"\n```\n:::\n\n```{.r .cell-code}\nps$check(list(A = TRUE, B = 1))                    # B's dependency is not met\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The parameter 'B' can only be set if the following condition is met 'D ∈ {x, y}'. Instead the parameter value for 'D' is not set at all. Try setting 'D' to a value that satisfies the condition\"\n```\n:::\n:::\n\n\nInternally, the dependencies are represented as a `data.table`, which can be accessed listed in the **`$deps`** slot.\nThis `data.table` can even be mutated, to e.g. remove dependencies.\nThere are no sanity checks done when the `$deps` slot is changed this way.\nTherefore it is advised to be cautious.\n\n\n::: {.cell hash='technical_cache/html/technical-047_f68c0e9f8a4200746dac5896986df0ad'}\n\n```{.r .cell-code}\nps$deps\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id on           cond\n1:  D  A <CondEqual[9]>\n2:  B  D <CondAnyOf[9]>\n```\n:::\n:::\n\n\n#### Vector Parameters\n\nUnlike in the old [ParamHelpers](https://ParamHelpers.mlr-org.com) package, there are no more vectorial parameters in [paradox](https://paradox.mlr-org.com).\nInstead, it is now possible to create multiple copies of a single parameter using the `$rep` function.\nThis creates a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) consisting of multiple copies of the parameter, which can then (optionally) be added to another [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\n\n\n::: {.cell hash='technical_cache/html/technical-048_fe6f381995494e83c209cc7636c66236'}\n\n```{.r .cell-code}\nps2d = ParamDbl$new(\"x\", lower = 0, upper = 1)$rep(2)\nprint(ps2d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n        id    class lower upper nlevels        default value\n1: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>      \n2: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>      \n```\n:::\n:::\n\n::: {.cell hash='technical_cache/html/technical-049_1588bf3c9074fe06cfb86e1601898190'}\n\n```{.r .cell-code}\nps$add(ps2d)\nprint(ps)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n        id    class lower upper nlevels        default parents value\n1:       A ParamLgl    NA    NA       2 <NoDefault[3]>          TRUE\n2:       B ParamInt     0    10      11 <NoDefault[3]>       D     1\n3:       C ParamDbl     0     4     Inf <NoDefault[3]>              \n4:       D ParamFct    NA    NA       3              y       A      \n5:       E ParamUty    NA    NA     Inf <NoDefault[3]>              \n6: x_rep_1 ParamDbl     0     1     Inf <NoDefault[3]>              \n7: x_rep_2 ParamDbl     0     1     Inf <NoDefault[3]>              \n```\n:::\n:::\n\n\nIt is also possible to use a [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html) to accept vectorial parameters, which also works for parameters of variable length.\nA [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) containing a [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html) can be used for parameter checking, but not for [sampling](#parameter-sampling).\nTo sample values for a method that needs a vectorial parameter, it is advised to use a [parameter transformation](#transformation-between-types) function that creates a vector from atomic values.\n\nAssembling a vector from repeated parameters is aided by the parameter's `$tags`: Parameters that were generated by the `$rep()` command automatically get tagged as belonging to a group of repeated parameters.\n\n\n::: {.cell hash='technical_cache/html/technical-050_704e98d4e494f5ccd317349dc4cfa3cc'}\n\n```{.r .cell-code}\nps$tags\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$A\ncharacter(0)\n\n$B\n[1] \"tag1\" \"tag2\"\n\n$C\ncharacter(0)\n\n$D\ncharacter(0)\n\n$E\ncharacter(0)\n\n$x_rep_1\n[1] \"x_rep\"\n\n$x_rep_2\n[1] \"x_rep\"\n```\n:::\n:::\n\n\n### Parameter Sampling\n\nIt is often useful to have a list of possible parameter values that can be systematically iterated through, for example to find parameter values for which an algorithm performs particularly well (tuning).\n[paradox](https://paradox.mlr-org.com) offers a variety of functions that allow creating evenly-spaced parameter values in a \"grid\" design as well as random sampling.\nIn the latter case, it is possible to influence the sampling distribution in more or less fine detail.\n\nA point to always keep in mind while sampling is that only numerical and factorial parameters that are bounded can be sampled from, i.e. not [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html).\nFurthermore, for most samplers [`ParamInt`](https://paradox.mlr-org.com/reference/ParamInt.html) and [`ParamDbl`](https://paradox.mlr-org.com/reference/ParamDbl.html) must have finite lower and upper bounds.\n\n#### Parameter Designs\n\nFunctions that sample the parameter space fundamentally return an object of the [`Design`](https://paradox.mlr-org.com/reference/Design.html) class.\nThese objects contain the sampled data as a `data.table` under the `$data` slot, and also offer conversion to a list of parameter-values using the **`$transpose()`** function.\n\n#### Grid Design\n\nThe [`generate_design_grid()`](https://paradox.mlr-org.com/reference/generate_design_grid.html) function is used to create grid designs that contain all combinations of parameter values: All possible values for [`ParamLgl`](https://paradox.mlr-org.com/reference/ParamLgl.html) and [`ParamFct`](https://paradox.mlr-org.com/reference/ParamFct.html), and values with a given resolution for [`ParamInt`](https://paradox.mlr-org.com/reference/ParamInt.html) and [`ParamDbl`](https://paradox.mlr-org.com/reference/ParamDbl.html).\nThe resolution can be given for all numeric parameters, or for specific named parameters through the `param_resolutions` parameter.\n\n\n::: {.cell hash='technical_cache/html/technical-051_89bfe9effbef92f6120d003b4d0707bd'}\n\n```{.r .cell-code}\ndesign = generate_design_grid(psSmall, 2)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 8 rows:\n       A  B C\n1:  TRUE  0 0\n2:  TRUE  0 4\n3:  TRUE 10 0\n4:  TRUE 10 4\n5: FALSE  0 0\n6: FALSE  0 4\n7: FALSE 10 0\n8: FALSE 10 4\n```\n:::\n:::\n\n::: {.cell hash='technical_cache/html/technical-052_823673007e65393ffabfdffd3db150d2'}\n\n```{.r .cell-code}\ngenerate_design_grid(psSmall, param_resolutions = c(B = 1, C = 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 4 rows:\n   B C     A\n1: 0 0  TRUE\n2: 0 0 FALSE\n3: 0 4  TRUE\n4: 0 4 FALSE\n```\n:::\n:::\n\n\n#### Random Sampling\n\n[paradox](https://paradox.mlr-org.com) offers different methods for random sampling, which vary in the degree to which they can be configured.\nThe easiest way to get a uniformly random sample of parameters is [`generate_design_random()`](https://paradox.mlr-org.com/reference/generate_design_random.html).\nIt is also possible to create \"[latin hypercube](https://en.wikipedia.org/wiki/Latin_hypercube_sampling)\" sampled parameter values using [`generate_design_lhs()`](https://paradox.mlr-org.com/reference/generate_design_lhs.html), which utilizes the [lhs](https://cran.r-project.org/package=lhs) package.\nLHS-sampling creates low-discrepancy sampled values that cover the parameter space more evenly than purely random values.\n\n\n::: {.cell hash='technical_cache/html/technical-053_049041e872ce09b46b7abbacbbc7227c'}\n\n```{.r .cell-code}\npvrand = generate_design_random(ps2d, 500)\npvlhs = generate_design_lhs(ps2d, 500)\n```\n:::\n\n::: {.cell layout=\"[[40,40]]\" hash='technical_cache/html/technical-054_25b466292ee6227651ecb4a0b321c661'}\n::: {.cell-output-display}\n![](technical_files/figure-html/technical-054-1.png){width=45%}\n:::\n\n::: {.cell-output-display}\n![](technical_files/figure-html/technical-054-2.png){width=45%}\n:::\n:::\n\n\n#### Generalized Sampling: The [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) Class\n\nIt may sometimes be desirable to configure parameter sampling in more detail.\n[paradox](https://paradox.mlr-org.com) uses the [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) abstract base class for sampling, which has many different sub-classes that can be parameterized and combined to control the sampling process.\nIt is even possible to create further sub-classes of the [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) class (or of any of *its* subclasses) for even more possibilities.\n\nEvery [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) object has a [`sample()`](https://www.rdocumentation.org/packages/base/topics/sample) function, which takes one argument, the number of instances to sample, and returns a [`Design`](https://paradox.mlr-org.com/reference/Design.html) object.\n\n##### 1D-Samplers\n\nThere is a variety of samplers that sample values for a single parameter.\nThese are [`Sampler1DUnif`](https://paradox.mlr-org.com/reference/Sampler1DUnif.html) (uniform sampling), [`Sampler1DCateg`](https://paradox.mlr-org.com/reference/Sampler1DCateg.html) (sampling for categorical parameters), [`Sampler1DNormal`](https://paradox.mlr-org.com/reference/Sampler1DNormal.html) (normally distributed sampling, truncated at parameter bounds), and [`Sampler1DRfun`](https://paradox.mlr-org.com/reference/Sampler1DRfun.html) (arbitrary 1D sampling, given a random-function).\nThese are initialized with a single [`Param`](https://paradox.mlr-org.com/reference/Param.html), and can then be used to sample values.\n\n\n::: {.cell hash='technical_cache/html/technical-055_388caa93188a6909326420b627fb548a'}\n\n```{.r .cell-code}\nsampA = Sampler1DCateg$new(parA)\nsampA$sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 5 rows:\n       A\n1: FALSE\n2:  TRUE\n3:  TRUE\n4:  TRUE\n5: FALSE\n```\n:::\n:::\n\n\n##### Hierarchical Sampler\n\nThe [`SamplerHierarchical`](https://paradox.mlr-org.com/reference/SamplerHierarchical.html) sampler is an auxiliary sampler that combines many 1D-Samplers to get a combined distribution.\nIts name \"hierarchical\" implies that it is able to respect parameter dependencies.\nThis suggests that parameters only get sampled when their dependencies are met.\n\nThe following example shows how this works: The `Int` parameter `B` depends on the `Lgl` parameter `A` being `TRUE`.\n`A` is sampled to be `TRUE` in about half the cases, in which case `B` takes a value between 0 and 10.\nIn the cases where `A` is `FALSE`, `B` is set to `NA`.\n\n\n::: {.cell hash='technical_cache/html/technical-056_1644ed07e2fd2cbef53d20a89ecedc10'}\n\n```{.r .cell-code}\npsSmall$add_dep(\"B\", \"A\", CondEqual$new(TRUE))\nsampH = SamplerHierarchical$new(psSmall,\n  list(Sampler1DCateg$new(parA),\n    Sampler1DUnif$new(parB),\n    Sampler1DUnif$new(parC))\n)\nsampled = sampH$sample(1000)\ntable(sampled$data[, c(\"A\", \"B\")], useNA = \"ifany\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       B\nA         0   1   2   3   4   5   6   7   8   9  10 <NA>\n  FALSE   0   0   0   0   0   0   0   0   0   0   0  510\n  TRUE   36  49  43  46  49  41  47  48  40  48  43    0\n```\n:::\n:::\n\n\n##### Joint Sampler\n\nAnother way of combining samplers is the [`SamplerJointIndep`](https://paradox.mlr-org.com/reference/SamplerJointIndep.html).\n[`SamplerJointIndep`](https://paradox.mlr-org.com/reference/SamplerJointIndep.html) also makes it possible to combine [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html)s that are not 1D.\nHowever, [`SamplerJointIndep`](https://paradox.mlr-org.com/reference/SamplerJointIndep.html) currently can not handle [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s with dependencies.\n\n\n::: {.cell hash='technical_cache/html/technical-057_fc8bc4e3329fe2851f1c37552d511ec6'}\n\n```{.r .cell-code}\nsampJ = SamplerJointIndep$new(\n  list(Sampler1DUnif$new(ParamDbl$new(\"x\", 0, 1)),\n    Sampler1DUnif$new(ParamDbl$new(\"y\", 0, 1)))\n)\nsampJ$sample(5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 5 rows:\n            x         y\n1: 0.23752465 0.8076742\n2: 0.07303362 0.0669248\n3: 0.02091371 0.3761876\n4: 0.95811981 0.3244582\n5: 0.25390980 0.5335375\n```\n:::\n:::\n\n\n##### SamplerUnif\n\nThe [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) used in [`generate_design_random()`](https://paradox.mlr-org.com/reference/generate_design_random.html) is the [`SamplerUnif`](https://paradox.mlr-org.com/reference/SamplerUnif.html) sampler, which corresponds to a `HierarchicalSampler` of [`Sampler1DUnif`](https://paradox.mlr-org.com/reference/Sampler1DUnif.html) for all parameters.\n\n### Parameter Transformation\n\nWhile the different [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html)s allow for a wide specification of parameter distributions, there are cases where the simplest way of getting a desired distribution is to sample parameters from a simple distribution (such as the uniform distribution) and then transform them.\nThis can be done by assigning a function to the `$trafo` slot of a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\nThe `$trafo` function is called with two parameters:\n\n* The list of parameter values to be transformed as `x`\n* The [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) itself as `param_set`\n\nThe `$trafo` function must return a list of transformed parameter values.\n\nThe transformation is performed when calling the `$transpose` function of the [`Design`](https://paradox.mlr-org.com/reference/Design.html) object returned by a [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) with the `trafo` ParamSet to `TRUE` (the default).\nThe following, for example, creates a parameter that is exponentially distributed:\n\n\n::: {.cell hash='technical_cache/html/technical-058_659d4b51cb263434e8c1ffa094e544b7'}\n\n```{.r .cell-code}\npsexp = ParamSet$new(list(ParamDbl$new(\"par\", 0, 1)))\npsexp$trafo = function(x, param_set) {\n  x$par = -log(x$par)\n  x\n}\ndesign = generate_design_random(psexp, 2)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 2 rows:\n         par\n1: 0.5486255\n2: 0.5870411\n```\n:::\n\n```{.r .cell-code}\ndesign$transpose()  # trafo is TRUE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$par\n[1] 0.6003392\n\n\n[[2]]\n[[2]]$par\n[1] 0.5326605\n```\n:::\n:::\n\n\nCompare this to `$transpose()` without transformation:\n\n\n::: {.cell hash='technical_cache/html/technical-059_f226a9ab6af1e52e3a39dce2936f54d6'}\n\n```{.r .cell-code}\ndesign$transpose(trafo = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$par\n[1] 0.5486255\n\n\n[[2]]\n[[2]]$par\n[1] 0.5870411\n```\n:::\n:::\n\n\n#### Transformation between Types\n\nUsually the design created with one [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) is then used to configure other objects that themselves have a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) which defines the values they take.\nThe [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s which can be used for random sampling, however, are restricted in some ways:\nThey must have finite bounds, and they may not contain \"untyped\" ([`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html)) parameters.\n`$trafo` provides the glue for these situations.\nThere is relatively little constraint on the trafo function's return value, so it is possible to return values that have different bounds or even types than the original [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\nIt is even possible to remove some parameters and add new ones.\n\nSuppose, for example, that a certain method requires a *function* as a parameter.\nLet's say a function that summarizes its data in a certain way.\nThe user can pass functions like `median()` or `mean()`, but could also pass quantiles or something completely different.\nThis method would probably use the following [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html):\n\n\n::: {.cell hash='technical_cache/html/technical-060_cf6ebbe7ab6b0f0100be4e9b612f9fef'}\n\n```{.r .cell-code}\nmethodPS = ParamSet$new(\n  list(\n    ParamUty$new(\"fun\",\n      custom_check = function(x) checkmate::checkFunction(x, nargs = 1))\n  )\n)\nprint(methodPS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n    id    class lower upper nlevels        default value\n1: fun ParamUty    NA    NA     Inf <NoDefault[3]>      \n```\n:::\n:::\n\n\nIf one wanted to sample this method, using one of four functions, a way to do this would be:\n\n\n::: {.cell hash='technical_cache/html/technical-061_23cd973148d0774aaca506f4f6b79daf'}\n\n```{.r .cell-code}\nsamplingPS = ParamSet$new(\n  list(\n    ParamFct$new(\"fun\", c(\"mean\", \"median\", \"min\", \"max\"))\n  )\n)\n\nsamplingPS$trafo = function(x, param_set) {\n  # x$fun is a `character(1)`,\n  # in particular one of 'mean', 'median', 'min', 'max'.\n  # We want to turn it into a function!\n  x$fun = get(x$fun, mode = \"function\")\n  x\n}\n```\n:::\n\n::: {.cell hash='technical_cache/html/technical-062_f61a8564ed25408411880af5af54a568'}\n\n```{.r .cell-code}\ndesign = generate_design_random(samplingPS, 2)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 2 rows:\n      fun\n1: median\n2: median\n```\n:::\n:::\n\n\nNote that the [`Design`](https://paradox.mlr-org.com/reference/Design.html) only contains the column \"`fun`\" as a `character` column.\nTo get a single value as a *function*, the `$transpose` function is used.\n\n\n::: {.cell hash='technical_cache/html/technical-063_8a3e2d02638c6dcda5446552a6c5f0ab'}\n\n```{.r .cell-code}\nxvals = design$transpose()\nprint(xvals[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fun\nfunction (x, na.rm = FALSE, ...) \nUseMethod(\"median\")\n<bytecode: 0x1261dce20>\n<environment: namespace:stats>\n```\n:::\n:::\n\n\nWe can now check that it fits the requirements set by `methodPS`, and that `fun` it is in fact a function:\n\n\n::: {.cell hash='technical_cache/html/technical-064_ea02e7217acae6ef74564099e7685fcb'}\n\n```{.r .cell-code}\nmethodPS$check(xvals[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"fun: Must have exactly 1 formal arguments, but has 2\"\n```\n:::\n\n```{.r .cell-code}\nxvals[[1]]$fun(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.5\n```\n:::\n:::\n\n\nImagine now that a different kind of parametrization of the function is desired:\nThe user wants to give a function that selects a certain quantile, where the quantile is set by a parameter.\nIn that case the `$transpose` function could generate a function in a different way.\nFor interpretability, the parameter is called \"`quantile`\" before transformation, and the \"`fun`\" parameter is generated on the fly.\n\n\n::: {.cell hash='technical_cache/html/technical-065_b46ad78b3d94ba43e9e00aa6c8c681e9'}\n\n```{.r .cell-code}\nsamplingPS2 = ParamSet$new(\n  list(\n    ParamDbl$new(\"quantile\", 0, 1)\n  )\n)\n\nsamplingPS2$trafo = function(x, param_set) {\n  # x$quantile is a `numeric(1)` between 0 and 1.\n  # We want to turn it into a function!\n  list(fun = function(input) quantile(input, x$quantile))\n}\n```\n:::\n\n::: {.cell hash='technical_cache/html/technical-066_efed19fb1b44a0363f90ab9b17407e71'}\n\n```{.r .cell-code}\ndesign = generate_design_random(samplingPS2, 2)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Design> with 2 rows:\n     quantile\n1: 0.03020900\n2: 0.09371297\n```\n:::\n:::\n\n\nThe [`Design`](https://paradox.mlr-org.com/reference/Design.html) now contains the column \"`quantile`\" that will be used by the `$transpose` function to create the `fun` parameter.\nWe also check that it fits the requirement set by `methodPS`, and that it is a function.\n\n\n::: {.cell hash='technical_cache/html/technical-067_477e151f0cd098cbab8d467ed9368f28'}\n\n```{.r .cell-code}\nxvals = design$transpose()\nprint(xvals[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$fun\nfunction(input) quantile(input, x$quantile)\n<environment: 0x136776a60>\n```\n:::\n\n```{.r .cell-code}\nmethodPS$check(xvals[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n\n```{.r .cell-code}\nxvals[[1]]$fun(1:10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 3.0209% \n1.271881 \n```\n:::\n:::\n\n\n## Logging {#logging}\n\nWe use the [lgr](https://cran.r-project.org/package=lgr) package for logging and progress output.\n\n### Changing [mlr3](https://mlr3.mlr-org.com) logging levels\n\nTo change the setting for [mlr3](https://mlr3.mlr-org.com) for the current session, you need to retrieve the logger (which is a [R6](https://cran.r-project.org/package=R6) object) from [lgr](https://cran.r-project.org/package=lgr), and then change the threshold of the like this:\n\n\n::: {.cell hash='technical_cache/html/technical-068_75f138220ea96e880da46af954a2814b'}\n\n```{.r .cell-code}\nrequireNamespace(\"lgr\")\n\nlogger = lgr::get_logger(\"mlr3\")\nlogger$set_threshold(\"<level>\")\n```\n:::\n\n\nThe default log level is `\"info\"`.\nAll available levels can be listed as follows:\n\n\n::: {.cell hash='technical_cache/html/technical-069_4bb9a28199d927a4fcb26bbfa8d987fd'}\n\n```{.r .cell-code}\ngetOption(\"lgr.log_levels\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfatal error  warn  info debug trace \n  100   200   300   400   500   600 \n```\n:::\n:::\n\n\nTo increase verbosity, set the log level to a higher value, e.g. to `\"debug\"` with:\n\n\n::: {.cell hash='technical_cache/html/technical-070_e6c93e8b1d3e20839dac0ed1191d9d0e'}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"debug\")\n```\n:::\n\n\nTo reduce the verbosity, reduce the log level to warn:\n\n\n::: {.cell hash='technical_cache/html/technical-071_0d62c2329abc49e44d40c163f2b223ea'}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\n```\n:::\n\n\n[lgr](https://cran.r-project.org/package=lgr) comes with a global option called `\"lgr.default_threshold\"` which can be set via `options()` to make your choice permanent across sessions.\n\nAlso note that the optimization packages such as [mlr3tuning](https://mlr3tuning.mlr-org.com)  [mlr3fselect](https://mlr3fselect.mlr-org.com) use the logger of their base package [bbotk](https://bbotk.mlr-org.com).\nTo disable the output from [mlr3](https://mlr3.mlr-org.com), but keep the output from [mlr3tuning](https://mlr3tuning.mlr-org.com), reduce the verbosity for the logger [mlr3](https://mlr3.mlr-org.com)\nand optionally change the logger [bbotk](https://bbotk.mlr-org.com) to the desired level.\n\n\n::: {.cell hash='technical_cache/html/technical-072_09218a43e8a6f68d5dd37b90878f30b4'}\n\n```{.r .cell-code}\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"info\")\n```\n:::\n\n\n### Redirecting output\n\nRedirecting output is already extensively covered in the documentation and vignette of [lgr](https://cran.r-project.org/package=lgr).\nHere is just a short example which adds an additional appender to log events into a temporary file in [JSON](https://en.wikipedia.org/wiki/JSON) format:\n\n\n::: {.cell hash='technical_cache/html/technical-073_6455049cd0240419b7c27b12b17fad2d'}\n\n```{.r .cell-code}\ntf = tempfile(\"mlr3log_\", fileext = \".json\")\n\n# get the logger as R6 object\nlogger = lgr::get_logger(\"mlr\")\n\n# add Json appender\nlogger$add_appender(lgr::AppenderJson$new(tf), name = \"json\")\n\n# signal a warning\nlogger$warn(\"this is a warning from mlr3\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWARN  [08:57:56.669] this is a warning from mlr3 \n```\n:::\n\n```{.r .cell-code}\n# print the contents of the file\ncat(readLines(tf))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{\"level\":300,\"timestamp\":\"2022-08-09 08:57:56\",\"logger\":\"mlr\",\"caller\":\"eval\",\"msg\":\"this is a warning from mlr3\"}\n```\n:::\n\n```{.r .cell-code}\n# remove the appender again\nlogger$remove_appender(\"json\")\n```\n:::\n\n\n### Immediate Log Feedback\n\n[mlr3](https://mlr3.mlr-org.com) uses the [future](#parallelization) package and [encapsulation](#encapsulation) to make evaluations fast, stable, and reproducible.\nHowever, this may lead to logs being delayed, out of order, or, in case of some errors, not present at all.\n\nWhen it is necessary to have immediate access to log messages, for example to investigate problems, one may therefore choose to disable [future](https://cran.r-project.org/package=future) and encapsulation.\nThis can be done by enabling the debug mode using `options(mlr.debug = TRUE)`; the `$encapsulate` slot of learners should also be set to `\"none\"` (default) or `\"evaluate\"`, but not `\"callr\"`.\nThis should only be done to investigate problems, however, and not for production use, because\n\n1. this disables parallelization, and\n2. this leads to different RNG behavior and therefore to results that are not reproducible when the debug mode is not set.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}