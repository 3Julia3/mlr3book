{
  "hash": "ced62577e9d2aeb1cc328165f5f67972",
  "result": {
    "markdown": "# Performance Evaluation and Comparison {#perf-eval-cmp }\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\nNow that we are familiar with the basics of how to create tasks and learners, how to fit models, let's have a look at some of the details, and in particular how [mlr3](https://mlr3.mlr-org.com) makes it easy to perform many common machine learning steps.\n\nWe will cover the following topics:\n\n**Performance Scoring**\n\n\n**Resampling**\n\n[Resampling](#resampling) is a methodology to create training and test splits.\nWe cover how to\n\n* access and select [resampling strategies](#resampling-settings),\n* instantiate the [split into training and test sets](#resampling-inst) by applying the resampling, and\n* execute the resampling to obtain [results](#resampling-exec).\n\nAdditional information on resampling can be found in the section about [nested resampling](#nested-resampling) and in the chapter on [model optimization](#optimization).\n\n**Benchmarking**\n\n[Benchmarking](#benchmarking) is used to compare the performance of different models, for example models trained with different learners, on different tasks, or with different resampling methods.\nThis is usually done to get an overview of how different methods perform across different tasks.\nWe cover how to\n\n* create a [benchmarking design](#bm-design),\n* [execute a design](#bm-exec) and aggregate results, and\n* [convert benchmarking objects](#bm-resamp) to other types of objects that can be used for different purposes.\n\n## ROC Curve and Thresholds {#binary-roc}\n\nAs we have seen before, binary classification is unique because of the presence of a positive and negative class and a threshold probability to distinguish between the two.\nROC Analysis, which stands for receiver operating characteristics, applies specifically to this case and allows a better picture of the trade-offs when choosing between the two classes.\n\nWe saw earlier that one can retrieve the confusion matrix of a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) by accessing the `$confusion` field.\nIn the following code chunk, we first retrieve the Sonar classification task and construct a classification tree learner.\nNext, we use the [`partition()`](https://mlr3.mlr-org.com/reference/partition.html) helper function to randomly split the rows of the Sonar task into two disjunct sets: a training set and a test set.\nWe train the learner on the training set and use the trained model to generate predictions on the test set.\nFinally, we retrieve the confusion matrix.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-001_1b6370174535815a16b6706b77b5b947'}\n\n```{.r .cell-code}\ntask = tsk(\"sonar\")\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\n\n# split into training and test\nsplits = partition(task, ratio = 0.8)\nprint(str(splits))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ train: int [1:167] 3 4 5 6 7 8 9 12 13 14 ...\n $ test : int [1:41] 1 2 10 11 24 28 35 37 46 48 ...\nNULL\n```\n:::\n\n```{.r .cell-code}\npred = learner$train(task, splits$train)$predict(task, splits$test)\npred$confusion\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        truth\nresponse  M  R\n       M 19  5\n       R  3 14\n```\n:::\n:::\n\n\n\nThe upper left quadrant denotes the number of times our model predicted the positive class and was correct about it.\nSimilarly, the lower right quadrant denotes the number of times our model predicted the negative class and was also correct about it.\nTogether, the elements on the diagonal are called True Positives (TP) and True Negatives (TN).\nThe upper right quadrant denotes the number of times we falsely predicted a positive label and is called False Positives (FP).\nThe lower left quadrant is called False Negatives (FN).\n\nWe can derive the following performance metrics from the confusion matrix:\n\n* **True Positive Rate (TPR)**: How many of the true positives did we predict as positive?\n* **True Negative Rate (TNR)**: How many of the true negatives did we predict as negative?\n* **Positive Predictive Value PPV**: If we predict positive how likely is it a true positive?\n* **Negative Predictive Value NPV**: If we predict negative how likely is it a true negative?\n\nIt is difficult to achieve a high TPR and low FPR simultaneously.\nWe can characterize the behavior of a binary classifier for different thresholds by plotting the TPR and FPR values -- this is the ROC curve.\nThe best classifier lies on the top-left corner -- the TPR is 1 and the FPR is 0.\nClassifiers on the diagonal produce labels randomly (possibly with different proportions).\nFor example, if each positive $x$ will be randomly classified with 25\\% as \"positive\", we get a TPR of 0.25.\nIf we assign each negative $x$ randomly to \"positive\" we get a FPR of 0.25.\nIn practice, we should never obtain a classifier clearly below the diagonal -- ROC curves for different labels are symmetric with respect to the diagonal, so a curve below the diagonal would indicate that the positive and negative class labels have been switched by the classifier.\n\nFor [mlr3](https://mlr3.mlr-org.com) prediction objects, the ROC curve can easily be created with [mlr3viz](https://mlr3viz.mlr-org.com) which relies on the [precrec](https://cran.r-project.org/package=precrec) to calculate and plot ROC curves:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-002_6b2c7c35b798692dc4f66735252ca52b'}\n\n```{.r .cell-code}\nlibrary(\"mlr3viz\")\n\n# TPR vs FPR / Sensitivity vs (1 - Specificity)\nautoplot(pred, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-002-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWe can also plot the precision-recall curve (PPV vs. TPR).\nThe main difference between ROC curves and precision-recall curves (PRC) is that the number of true-negative results is not used for making a PRC.\nPRCs are preferred over ROC curves for imbalanced populations.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-003_40ad1c146a74090991fd7c3f418c003f'}\n\n```{.r .cell-code}\n# Precision vs Recall\nautoplot(pred, type = \"prc\")\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-003-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n## Resampling {#sec-resampling}\n\nWhen evaluating the performance of a model, we are interested in its generalization performance -- how well will it perform on new data that has not been seen during training?\nWe can estimate the generalization performance by evaluating a model on a test set, as we have done above, that was created to contain only observations that are not contained in the training set.\nThere are many different strategies for partitioning a data set into training and test; in [mlr3](https://mlr3.mlr-org.com) we call these strategies \"resampling\".\n[mlr3](https://mlr3.mlr-org.com) includes the following predefined [resampling](#resampling) strategies:\n\n| Name | Identifier |\n|:---|:---|\n| [`cross validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_cv.html) | `\"cv\"` |\n| [`leave-one-out cross validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_loo.html) | `\"loo\"` |\n| [`repeated cross validation`](https://mlr3.mlr-org.com/reference/mlr_resamplings_repeated_cv.html) | `\"repeated_cv\"` |\n| [`bootstrapping`](https://mlr3.mlr-org.com/reference/mlr_resamplings_bootstrap.html)| `\"bootstrap\"` |\n| [`subsampling`](https://mlr3.mlr-org.com/reference/mlr_resamplings_subsampling.html) | `\"subsampling\"` |\n| [`holdout`](https://mlr3.mlr-org.com/reference/mlr_resamplings_holdout.html) | `\"holdout\"` |\n| [`in-sample resampling`](https://mlr3.mlr-org.com/reference/mlr_resamplings_insample.html)| `\"insample\"` |\n| [`custom resampling`](https://mlr3.mlr-org.com/reference/mlr_resamplings_custom.html) | `\"custom\"` |\n\n:::{.callout-note}\nIt is often desirable to repeatedly split the entire data in different ways to ensure that a \"lucky\" or \"unlucky\" split does not bias the generalization performance estimate.\nWithout resampling strategies like the ones we provide here, this is a tedious and error-prone process.\n:::\n\nThe following sections provide guidance on how to select a resampling strategy and how to use it.\n\nHere is a graphical illustration of the resampling process in general:\n\n\n\n::: {.cell layout-align=\"center\" hash='performance_cache/pdf/performance-004_1e989df903df4cd7cb1a615fdf91aeb7'}\n::: {.cell-output-display}\n![](images/ml_abstraction.pdf){fig-align='center'}\n:::\n:::\n\n\n\n### Settings {#resampling-settings}\n\nWe will use the [`penguins`](https://mlr3.mlr-org.com/reference/mlr_tasks_penguins.html) task and a simple classification tree from the [rpart](https://cran.r-project.org/package=rpart) package as an example here.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-005_6798896e880b83368a563ee556f3c0fa'}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\n\nWhen performing resampling with a dataset, we first need to define which approach to use.\n[mlr3](https://mlr3.mlr-org.com) resampling strategies and their parameters can be queried by looking at the `data.table` output of the [`mlr_resamplings`](https://mlr3.mlr-org.com/reference/mlr_resamplings.html) dictionary; this also lists the parameters that can be changed to affect the behavior of each strategy:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-006_2458654ded9b7b49653bcbe2e975529e'}\n\n```{.r .cell-code}\nas.data.table(mlr_resamplings)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           key                         label        params iters\n1:   bootstrap                     Bootstrap ratio,repeats    30\n2:      custom                 Custom Splits                  NA\n3:   custom_cv Custom Split Cross-Validation                  NA\n4:          cv              Cross-Validation         folds    10\n5:     holdout                       Holdout         ratio     1\n6:    insample           Insample Resampling                   1\n7:         loo                 Leave-One-Out                  NA\n8: repeated_cv     Repeated Cross-Validation folds,repeats   100\n9: subsampling                   Subsampling ratio,repeats    30\n```\n:::\n:::\n\n\n\n:::{.callout-tip}\nAdditional resampling methods for special use cases are available via extension packages, such as [mlr3spatiotemporal](https://github.com/mlr-org/mlr3spatiotemporal) for spatial data.\n:::\n\nWhat we showed in the [train/predict/score](#train-predict) part is the equivalent of holdout resampling, done manually, so let's consider this one first.\nWe can retrieve elements from the dictionary [`mlr_resamplings`](https://mlr3.mlr-org.com/reference/mlr_resamplings.html) with the convenience function [`rsmp()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html):\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-007_b1e3c3ec07451db157775ff04fa40a2d'}\n\n```{.r .cell-code}\nresampling = rsmp(\"holdout\")\nprint(resampling)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.6667\n```\n:::\n:::\n\n\n\n:::{.callout-note}\nNote that the `$is_instantiated` field is set to `FALSE`.\nThis means we did not apply the strategy to a dataset yet.\n:::\n\nBy default we get a .66/.33 split of the data into training and test.\nThere are two ways in which this ratio can be changed:\n\n1. Overwriting the slot in `$param_set$values` using a named list:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-008_c36dab7a07831933fb56b7e3998bcafe'}\n\n```{.r .cell-code}\nresampling$param_set$values = list(ratio = 0.8)\n```\n:::\n\n\n\n2. Specifying the resampling parameters directly during construction:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-009_91a7fbcfad2f0cc18283510e3042307f'}\n\n```{.r .cell-code}\nrsmp(\"holdout\", ratio = 0.8)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResamplingHoldout>: Holdout\n* Iterations: 1\n* Instantiated: FALSE\n* Parameters: ratio=0.8\n```\n:::\n:::\n\n\n\n### Instantiation {#resampling-inst}\n\nSo far, we have only chosen a resampling strategy; we now need to instantiate it with data.\n\nTo perform the splitting and obtain indices for the training and the test split, the resampling needs a [`Task`](https://mlr3.mlr-org.com/reference/Task.html).\nBy calling the method `instantiate()`, we split the row indices of the task into indices for training and test sets.\nThese resulting indices are stored in the [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) objects.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-010_e9f2f790b607967ccd3fb5f6b7fec4a0'}\n\n```{.r .cell-code}\nresampling$instantiate(task)\ntrain = resampling$train_set(1)\ntest = resampling$test_set(1)\nstr(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n int [1:275] 2 3 4 5 6 7 8 9 10 11 ...\n```\n:::\n\n```{.r .cell-code}\nstr(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n int [1:69] 1 13 14 15 17 20 28 32 42 44 ...\n```\n:::\n\n```{.r .cell-code}\n# are they disjunct?\nintersect(train, test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninteger(0)\n```\n:::\n\n```{.r .cell-code}\n# are all row ids either in train or test?\nsetequal(c(train, test), task$row_ids)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n\nNote that if you want to compare multiple [Learners](#learners) in a fair manner, using the same instantiated resampling for each learner is mandatory, such that each learner gets exactly the same training data and the performance of the trained model is evaluated in exactly the same test set.\nA way to greatly simplify the comparison of multiple learners is discussed in the [section on benchmarking](#benchmarking).\n\n\n### Execution {#resampling-exec}\n\nWith a [`Task`](https://mlr3.mlr-org.com/reference/Task.html), a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), and a [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations.\nFor this, we call the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function which returns a [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) object.\nWe can tell [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) to keep the fitted models (for example for later inspection) by setting the `store_models` option to `TRUE`.\n\n:::{.callout-note}\nThe [butcher](https://cran.r-project.org/package=butcher) package allows you to remove components of a fitted model that may no longer be needed.\nHowever, from the point of view of the mlr3 framework, it is impossible to find a good compromise here.\nOf course, as much balast as possible should be removed.\nHowever, if too much is removed, some graphics, outputs or third-party extensions may not work anymore as intended.\nOur conservative policy here is that the training data should not be part of the stored model.\nIf this is the case somewhere, please [open an issue](https://github.com/mlr-org/mlr3/issues).\n\nTo manually apply butcher, just replace the stored model in the learner:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-011_0038e04b9ce724517d133643bb4d003e'}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\")\nlearner$train(task)\n\nlearner$model = butcher::butcher(learner$model)\n```\n:::\n\n\n:::\n\nPer default, mlr3 discards fitted models after the prediction step to keep the [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) object small w.r.t. its memory consumption.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-012_a78635b64d86e6d558cdf8f52a0a155f'}\n\n```{.r .cell-code}\ntask = tsk(\"penguins\")\nlearner = lrn(\"classif.rpart\", maxdepth = 3, predict_type = \"prob\")\nresampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, learner, resampling, store_models = TRUE)\nprint(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 3 iterations\n* Task: penguins\n* Learner: classif.rpart\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n:::\n\n\n\nHere we use a three-fold cross-validation resampling, which trains and evaluates on three different training and test sets.\nThe returned [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html), stored as `rr`, provides various getters to access and aggregate the stored information.\nHere are a few examples:\n\n- Calculate the average performance across all resampling iterations, in terms of classification error:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-013_b9ba343a4cbfde63c572cf997915282e'}\n  \n  ```{.r .cell-code}\n  rr$aggregate(msr(\"classif.ce\"))\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  classif.ce \n  0.05819985 \n  ```\n  :::\n  :::\n\n\n\n  This is useful to check if one (or more) of the iterations are very different from the average.\n\n- Check for warnings or errors:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-014_d00c5a678eff1b53f3b361b32af4e935'}\n  \n  ```{.r .cell-code}\n  rr$warnings\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  Empty data.table (0 rows and 2 cols): iteration,msg\n  ```\n  :::\n  \n  ```{.r .cell-code}\n  rr$errors\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  Empty data.table (0 rows and 2 cols): iteration,msg\n  ```\n  :::\n  :::\n\n\n\n- Extract and inspect the resampling splits; this allows us to see in detail which observations were used for what purpose when:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-015_f51a46901f55143f59d6ab92d4b819c8'}\n  \n  ```{.r .cell-code}\n  rr$resampling\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  <ResamplingCV>: Cross-Validation\n  * Iterations: 3\n  * Instantiated: TRUE\n  * Parameters: folds=3\n  ```\n  :::\n  \n  ```{.r .cell-code}\n  rr$resampling$iters\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] 3\n  ```\n  :::\n  \n  ```{.r .cell-code}\n  str(rr$resampling$train_set(1))\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n   int [1:229] 9 12 16 17 19 24 25 29 30 34 ...\n  ```\n  :::\n  \n  ```{.r .cell-code}\n  str(rr$resampling$test_set(1))\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n   int [1:115] 1 5 8 10 15 18 21 22 23 26 ...\n  ```\n  :::\n  :::\n\n\n\n- Retrieve the model trained in a specific iteration and inspect it, for example, to investigate why the performance in this iteration was very different from the average:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-016_c5c168e2c0b6940b7deb93ff784f8029'}\n  \n  ```{.r .cell-code}\n  lrn = rr$learners[[1]]\n  lrn$model\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  n= 229 \n  \n  node), split, n, loss, yval, (yprob)\n        * denotes terminal node\n  \n  1) root 229 128 Adelie (0.44104803 0.22707424 0.33187773)  \n    2) bill_length< 42.35 94   1 Adelie (0.98936170 0.00000000 0.01063830) *\n    3) bill_length>=42.35 135  60 Gentoo (0.05925926 0.38518519 0.55555556)  \n      6) island=Dream,Torgersen 58   6 Chinstrap (0.10344828 0.89655172 0.00000000) *\n      7) island=Biscoe 77   2 Gentoo (0.02597403 0.00000000 0.97402597) *\n  ```\n  :::\n  :::\n\n\n\n- Extract the individual predictions:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-017_4331d69dbd35e9c0686724076014dcc6'}\n  \n  ```{.r .cell-code}\n  rr$prediction() # all predictions merged into a single Prediction object\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  <PredictionClassif> for 344 observations:\n      row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n            1    Adelie    Adelie  0.98936170     0.00000000  0.01063830\n            5    Adelie    Adelie  0.98936170     0.00000000  0.01063830\n            8    Adelie    Adelie  0.98936170     0.00000000  0.01063830\n  ---                                                                   \n          340 Chinstrap    Gentoo  0.01162791     0.01162791  0.97674419\n          342 Chinstrap Chinstrap  0.04761905     0.92857143  0.02380952\n          343 Chinstrap    Gentoo  0.01162791     0.01162791  0.97674419\n  ```\n  :::\n  \n  ```{.r .cell-code}\n  rr$predictions()[[1]] # predictions of first resampling iteration\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  <PredictionClassif> for 115 observations:\n      row_ids     truth  response prob.Adelie prob.Chinstrap prob.Gentoo\n            1    Adelie    Adelie   0.9893617      0.0000000   0.0106383\n            5    Adelie    Adelie   0.9893617      0.0000000   0.0106383\n            8    Adelie    Adelie   0.9893617      0.0000000   0.0106383\n  ---                                                                   \n          335 Chinstrap Chinstrap   0.1034483      0.8965517   0.0000000\n          336 Chinstrap Chinstrap   0.1034483      0.8965517   0.0000000\n          339 Chinstrap Chinstrap   0.1034483      0.8965517   0.0000000\n  ```\n  :::\n  :::\n\n\n\n- Filter the result, keep only specified resampling iterations:\n\n\n\n  ::: {.cell hash='performance_cache/pdf/performance-018_84a91d8499a4391f36118e731201de9f'}\n  \n  ```{.r .cell-code}\n  rr$filter(c(1, 3))\n  print(rr)\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  <ResampleResult> of 2 iterations\n  * Task: penguins\n  * Learner: classif.rpart\n  * Warnings: 0 in 0 iterations\n  * Errors: 0 in 0 iterations\n  ```\n  :::\n  :::\n\n\n\n### Custom resampling {#resamp-custom}\n\nSometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study.\nA manual resampling instance can be created using the `\"custom\"` template.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-019_d6b7b800eb13429efe31c6a1c35360b3'}\n\n```{.r .cell-code}\nresampling = rsmp(\"custom\")\nresampling$instantiate(task,\n  train = list(c(1:10, 51:60, 101:110)),\n  test = list(c(11:20, 61:70, 111:120))\n)\nresampling$iters\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\nresampling$train_set(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]   1   2   3   4   5   6   7   8   9  10  51  52  53  54  55  56  57  58  59\n[20]  60 101 102 103 104 105 106 107 108 109 110\n```\n:::\n\n```{.r .cell-code}\nresampling$test_set(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  11  12  13  14  15  16  17  18  19  20  61  62  63  64  65  66  67  68  69\n[20]  70 111 112 113 114 115 116 117 118 119 120\n```\n:::\n:::\n\n\n\n### Resampling with (predefined) groups\n\nIn some cases, it is desirable to keep observations together, i.e., either have all observations of the same group in the training set or have all observations of the same group in the test set.\nThis can be defined through the column role `\"group\"` during Task creation, i.e., a column in the data specifies the groups (see also the [help page](https://mlr3.mlr-org.com/reference/Resampling.html#grouping-blocking) on this column role).\n\nA potential use case for this is spatial modeling, where one wants to account for groups of observations during resampling.\n\n:::{.callout-tip appearance=\"simple\"}\nBesides the `\"group\"` column role, dedicated spatiotemporal resampling methods are available in [mlr3spatiotemporal](https://mlr3spatiotemporal.mlr-org.com).\nMore general information about this topic can be found in the [spatiotemporal resampling](special.html#spatiotemp-cv) section.\n:::\n\nIf you want the ensure that the observations in the training and test set are similarly distributed as in the complete task, one or multiple columns can be used for stratification via the column role `stratum`.\nSee also the [mlr3gallery post on this topic](https://mlr3gallery.mlr-org.com/posts/2020-03-30-stratification-blocking/) for a practical introduction.\n\n:::{.callout-note appearance=\"simple\"}\nIn the old [mlr](https://mlr.mlr-org.com) package, grouping was called \"blocking\".\n:::\n\n### Plotting Resample Results {#autoplot-resampleresult}\n\n[mlr3viz](https://mlr3viz.mlr-org.com) provides a [`autoplot()`](https://www.rdocumentation.org/packages/ggplot2/topics/autoplot) method for resampling results.\nAs an example, we create a binary classification task with two features, perform a resampling with a 10-fold cross-validation and visualize the results:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-020_8432335d67ac09a87c0a5d3bea6b476c'}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\ntask$select(c(\"glucose\", \"mass\"))\nlearner = lrn(\"classif.rpart\", predict_type = \"prob\")\nrr = resample(task, learner, rsmp(\"cv\"), store_models = TRUE)\nrr$score(msr(\"classif.auc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 task task_id                   learner    learner_id\n 1: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 2: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 3: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 4: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 5: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 6: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 7: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 8: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n 9: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n10: <TaskClassif[50]>    pima <LearnerClassifRpart[38]> classif.rpart\n5 variables not shown: [resampling, resampling_id, iteration, prediction, classif.auc]\n```\n:::\n\n```{.r .cell-code}\n# boxplot of AUC values across the 10 folds\nlibrary(\"mlr3viz\")\nautoplot(rr, measure = msr(\"classif.auc\"))\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-020-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# ROC curve, averaged over 10 folds\nautoplot(rr, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-020-2.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nWe can also plot the predictions of individual models:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-021_7f431f150bed59d367041503f87bb0ea'}\n\n```{.r .cell-code}\n# learner predictions for the first fold\nrr1 = rr$clone()$filter(1)\nautoplot(rr1, type = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing missing values (geom_point).\n```\n:::\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-021-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n:::{.callout-tip}\nAll available plot types are listed on the manual page of [`autoplot.ResampleResult()`](https://mlr3viz.mlr-org.com/reference/autoplot.ResampleResult.html).\n:::\n\n## Benchmarking {#benchmarking}\n\n\nComparing the performance of different learners on multiple tasks and/or different resampling schemes is a common task.\nThis operation is usually referred to as \"benchmarking\" in the field of machine learning.\nThe [mlr3](https://mlr3.mlr-org.com) package offers the [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) convenience function that takes care of most of the work of repeatedly training and evaluating models under the same conditions.\n\n### Design Creation {#bm-design}\n\nBenchmark experiments in [mlr3](https://mlr3.mlr-org.com) are specified through a design.\nSuch a design is essentially a table of scenarios to be evaluated; in particular unique combinations of [`Task`](https://mlr3.mlr-org.com/reference/Task.html), [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) and [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html) triplets.\n\nWe use the [`benchmark_grid()`](https://mlr3.mlr-org.com/reference/benchmark_grid.html) function to create an exhaustive design (that evaluates each learner on each task with each resampling) and instantiate the resampling properly, so that all learners are executed on the same train/test split for each tasks.\nWe set the learners to predict probabilities and also tell them to predict for the observations of the training set (by setting `predict_sets` to `c(\"train\", \"test\")`).\nAdditionally, we use [`tsks()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html), [`lrns()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html), and [`rsmps()`](https://mlr3.mlr-org.com/reference/mlr_sugar.html) to retrieve lists of [`Task`](https://mlr3.mlr-org.com/reference/Task.html), [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) and [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html).\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-022_149f3c611aec89635c9460d3cf80b2cc'}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\ndesign = benchmark_grid(\n  tasks = tsks(c(\"spam\", \"german_credit\", \"sonar\")),\n  learners = lrns(c(\"classif.ranger\", \"classif.rpart\", \"classif.featureless\"),\n    predict_type = \"prob\", predict_sets = c(\"train\", \"test\")),\n  resamplings = rsmps(\"cv\", folds = 3)\n)\nprint(design)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                task                         learner         resampling\n1: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n2: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n3: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n4: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n5: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n6: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n7: <TaskClassif[50]>      <LearnerClassifRanger[38]> <ResamplingCV[20]>\n8: <TaskClassif[50]>       <LearnerClassifRpart[38]> <ResamplingCV[20]>\n9: <TaskClassif[50]> <LearnerClassifFeatureless[38]> <ResamplingCV[20]>\n```\n:::\n:::\n\n\n\nThe created design table can be passed to [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) to start the computation.\nIt is also possible to create a custom design manually, for example, to exclude certain task-learner combinations.\n\n:::{.callout-caution}\nHowever, if you create a custom design with [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package), the train/test splits will be different for each row of the design if you do not [**manually instantiate**](#resampling-inst) the resampling before creating the design.\nSee the help page on [`benchmark_grid()`](https://mlr3.mlr-org.com/reference/benchmark_grid.html) for an example.\n:::\n\n\n### Execution Aggregation of Results {#bm-exec}\n\nAfter the [benchmark design](#bm-design) is ready, we can call [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) on it:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-023_d5face0fd2867504936e46c521c5aaad'}\n\n```{.r .cell-code}\nbmr = benchmark(design)\n```\n:::\n\n\n\n:::{.callout-tip}\nNote that we did not have to instantiate the resampling manually.\n[`benchmark_grid()`](https://mlr3.mlr-org.com/reference/benchmark_grid.html) took care of it for us: each resampling strategy is instantiated once for each task during the construction of the exhaustive grid.\nThis way, each learner operates on the same training and test sets which makes the results easier to compare.\n:::\n\nOnce the benchmarking is finished (and, depending on the size of your design, this can take quite some time), we can aggregate the performance with the `$aggregate()` method of the returned [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html).\nWe create two measures to calculate the area under the curve (AUC) for the training and the test set:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-024_9c82a651b9b5e002ec0b4b06ae954868'}\n\n```{.r .cell-code}\nmeasures = list(\n  msr(\"classif.auc\", predict_sets = \"train\", id = \"auc_train\"),\n  msr(\"classif.auc\", id = \"auc_test\")\n)\n\ntab = bmr$aggregate(measures)\nprint(tab[, .(task_id, learner_id, auc_train, auc_test)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         task_id          learner_id auc_train  auc_test\n1:          spam      classif.ranger 0.9994269 0.9859443\n2:          spam       classif.rpart 0.9152765 0.9061875\n3:          spam classif.featureless 0.5000000 0.5000000\n4: german_credit      classif.ranger 0.9984245 0.8063696\n5: german_credit       classif.rpart 0.8073881 0.7110071\n6: german_credit classif.featureless 0.5000000 0.5000000\n7:         sonar      classif.ranger 1.0000000 0.9165297\n8:         sonar       classif.rpart 0.8953818 0.7588270\n9:         sonar classif.featureless 0.5000000 0.5000000\n```\n:::\n:::\n\n\n\nWe can aggregate the results even further.\nFor example, we might be interested to know which learner performed best across all tasks.\nSimply aggregating the performances with the mean is usually not statistically sound.\nInstead, we calculate the rank statistic for each learner, grouped by task.\nThen the calculated ranks, grouped by the learner, are aggregated with the [data.table](https://cran.r-project.org/package=data.table) package.\nAs larger AUC scores are better, we multiply the values by $-1$ such that the best learner has a rank of $1$.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-025_a140e6cdea40a23df2585e53b0f2887b'}\n\n```{.r .cell-code}\nlibrary(\"data.table\")\n# group by levels of task_id, return columns:\n# - learner_id\n# - rank of col '-auc_train' (per level of learner_id)\n# - rank of col '-auc_test' (per level of learner_id)\nranks = tab[, .(learner_id, rank_train = rank(-auc_train), rank_test = rank(-auc_test)), by = task_id]\nprint(ranks)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         task_id          learner_id rank_train rank_test\n1:          spam      classif.ranger          1         1\n2:          spam       classif.rpart          2         2\n3:          spam classif.featureless          3         3\n4: german_credit      classif.ranger          1         1\n5: german_credit       classif.rpart          2         2\n6: german_credit classif.featureless          3         3\n7:         sonar      classif.ranger          1         1\n8:         sonar       classif.rpart          2         2\n9:         sonar classif.featureless          3         3\n```\n:::\n\n```{.r .cell-code}\n# group by levels of learner_id, return columns:\n# - mean rank of col 'rank_train' (per level of learner_id)\n# - mean rank of col 'rank_test' (per level of learner_id)\nranks = ranks[, .(mrank_train = mean(rank_train), mrank_test = mean(rank_test)), by = learner_id]\n\n# print the final table, ordered by mean rank of AUC test\nranks[order(mrank_test)]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            learner_id mrank_train mrank_test\n1:      classif.ranger           1          1\n2:       classif.rpart           2          2\n3: classif.featureless           3          3\n```\n:::\n:::\n\n\n\nUnsurprisingly, the featureless learner is worse overall.\nThe winner is the classification forest, which outperforms a single classification tree.\n\n\n### Plotting Benchmark Results {#autoplot-benchmarkresult}\n\nSimilar to [tasks](#autoplot-task), [predictions](#autoplot-prediction), or [resample results](#autoplot-resampleresult), [mlr3viz](https://mlr3viz.mlr-org.com) also provides a [`autoplot()`](https://www.rdocumentation.org/packages/ggplot2/topics/autoplot) method for benchmark results.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-026_5a3b8589a9f325ce54b8a011a4c4050a'}\n\n```{.r .cell-code}\nautoplot(bmr) + ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-026-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nSuch a plot gives a nice overview of the overall performance and how learners compare on different tasks in an intuitive way.\n\nWe can also plot ROC (receiver operating characteristics) curves.\nWe filter the [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html) to only contain a single [`Task`](https://mlr3.mlr-org.com/reference/Task.html), then we simply plot the result:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-027_700cb46438867c381e59e4c05b594455'}\n\n```{.r .cell-code}\nbmr_small = bmr$clone(deep = TRUE)$filter(task_id = \"german_credit\")\nautoplot(bmr_small, type = \"roc\")\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-027-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nAll available plot types are listed on the manual page of [`autoplot.BenchmarkResult()`](https://mlr3viz.mlr-org.com/reference/autoplot.BenchmarkResult.html).\n\n### Statistical Tests\n\nThe package [mlr3benchmark](https://mlr3benchmark.mlr-org.com) provides some infrastructure for applying statistical significance tests on [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html).\nCurrently, Friedman tests and pairwise Friedman-Nemenyi tests [@demsar2006] are supported for benchmarks with at least two tasks and at least two learners.\nThe results can be summarized in Critical Difference Plots.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-028_b5fe4fb9213afd749316480af435b5f1'}\n\n```{.r .cell-code}\nlibrary(\"mlr3benchmark\")\n\nbma = as.BenchmarkAggr(bmr, measures = msr(\"classif.auc\"))\nbma$friedman_posthoc()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n\tPairwise comparisons using Nemenyi-Wilcoxon-Wilcox all-pairs test for a two-way balanced complete block design\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\ndata: auc and learner_id and task_id\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n            ranger rpart\nrpart       0.438  -    \nfeatureless 0.038  0.438\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nP value adjustment method: single-step\n```\n:::\n\n```{.r .cell-code}\nautoplot(bma, type = \"cd\")\n```\n\n::: {.cell-output-display}\n![](performance_files/figure-pdf/performance-028-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n### Extracting ResampleResults {#bm-resamp}\n\nA [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html) object is essentially a collection of multiple [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) objects.\nAs these are stored in a column of the aggregated [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package), we can easily extract them:\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-029_3daba5e3395e2e2c4d2fead9c7714415'}\n\n```{.r .cell-code}\ntab = bmr$aggregate(measures)\nrr = tab[task_id == \"german_credit\" & learner_id == \"classif.ranger\"]$resample_result[[1]]\nprint(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ResampleResult> of 3 iterations\n* Task: german_credit\n* Learner: classif.ranger\n* Warnings: 0 in 0 iterations\n* Errors: 0 in 0 iterations\n```\n:::\n:::\n\n\n\nWe can now investigate this resampling and even single resampling iterations using one of the approaches shown in [the previous section on resampling](#resampling):\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-030_8e019e68f81c59b495f941b1d396fe64'}\n\n```{.r .cell-code}\nmeasure = msr(\"classif.auc\")\nrr$aggregate(measure)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.auc \n  0.8063696 \n```\n:::\n\n```{.r .cell-code}\n# get the iteration with worst AUC\nperf = rr$score(measure)\ni = which.min(perf$classif.auc)\n\n# get the corresponding learner and training set\nprint(rr$learners[[i]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<LearnerClassifRanger:classif.ranger>\n* Model: -\n* Parameters: num.threads=1\n* Packages: mlr3, mlr3learners, ranger\n* Predict Types:  response, [prob]\n* Feature Types: logical, integer, numeric, character, factor, ordered\n* Properties: hotstart_backward, importance, multiclass, oob_error,\n  twoclass, weights\n```\n:::\n\n```{.r .cell-code}\nhead(rr$resampling$train_set(i))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  2  5  7  8  9 10\n```\n:::\n:::\n\n\n\n### Converting and Merging\n\nA [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) can be converted to a [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html) with the function [`as_benchmark_result()`](https://mlr3.mlr-org.com/reference/as_benchmark_result.html).\nWe can also combine two [`BenchmarkResults`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html) into a larger result object, for example, for two related benchmarks that are computed on different machines.\n\n\n\n::: {.cell hash='performance_cache/pdf/performance-031_3077bacaf19e5097e582c78f44d649f7'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nresampling = rsmp(\"holdout\")$instantiate(task)\n\nrr1 = resample(task, lrn(\"classif.rpart\"), resampling)\nrr2 = resample(task, lrn(\"classif.featureless\"), resampling)\n\n# Cast both ResampleResults to BenchmarkResults\nbmr1 = as_benchmark_result(rr1)\nbmr2 = as_benchmark_result(rr2)\n\n# Merge 2nd BMR into the first BMR\nbmr = c(bmr1, bmr2)\nprint(bmr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BenchmarkResult> of 2 rows with 2 resampling runs\n nr task_id          learner_id resampling_id iters warnings errors\n  1    iris       classif.rpart       holdout     1        0      0\n  2    iris classif.featureless       holdout     1        0      0\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}