{
  "hash": "552636b7db7bc1fcf6f9db565768eb17",
  "result": {
    "markdown": "# Model Optimization {#optimization}\n\n::: {.cell}\n\n:::\n\n\n\nIn machine learning, when you are dissatisfied with the performance of a model, you might ask yourself how to best improve the model:\n\n* Can it be improved by tweaking the hyperparameters of the learner, i.e. the configuration options that affect its behavior?\n* Or, should you just use a completely different learner for this particular task?\n\nThis chapter might help answer this question.\n\n**Model Tuning**\n\nMachine learning algorithms have default values set for their hyperparameters.\nIn many cases, these hyperparameters need to be changed by the user to achieve optimal performance on the given dataset.\nWhile you can certainly search for hyperparameter settings that improve performance manually, we do not recommend this approach as it is tedious and rarely leads to the best performance.\nFortunately, the [mlr3](https://mlr3.mlr-org.com) ecosystem provides packages and tools for automated tuning.\nTo tune a machine learning algorithm, you have to specify\n\n1. the [search space](#tuning-optimization),\n1. the [optimization algorithm](#tuning-algorithms) (i.e. tuning method),\n1. an evaluation method (i.e., a resampling strategy), and\n1. a performance measure.\n\nIn the [tuning](#tuning) part, we will have a look at:\n\n* empirically sound [hyperparameter tuning](#tuning),\n* selecting the [optimizing algorithm](#tuning-optimization),\n* defining [search spaces concisely](#searchspace),\n* [triggering](#tuning-triggering) the tuning, and\n* [automating](#autotuner) tuning.\n\nWe will use the [mlr3tuning](https://mlr3tuning.mlr-org.com) package, which supports common tuning operations.\n\n**Feature Selection**\n\nTuning the hyperparameters is only one way of improving the performance of your model.\nThe second part of this chapter explains [feature selection](#fs), also known as variable or descriptor selection.\n[Feature selection](#fs) is the process of finding the feature subset that is most relevant with respect to the prediction or for which the learner fits a model with the highest performance.\nApart from improving model performance, there are additional reasons to perform feature selection:\n\n* enhance the interpretability of the model,\n* speed up model fitting, or\n* eliminate the need to collect lots of expensive features.\n\nHere, we mostly focus on feature selection as a means of improving model performance.\n\nThere are different approaches to identifying the relevant features.\nIn the [feature selection](#fs) part, we describe three methods:\n\n* [Filter](#fs-filter) algorithms select features independently of the learner by scoring the different features.\n* [Variable importance filters](#fs-var-imp-filters) select features that are important according to the model induced by a learner.\n* [Wrapper methods](#fs-wrapper) iteratively select features to optimize a performance measure, each time fitting and evaluating a model with a different subset of features.\n\nNote that filters operate independently of learners.\nVariable importance filters rely on the learner to extract information on feature importance from a trained model, for example, by inspecting a learned decision tree and returning the features that are used in the first few levels.\nThe obtained importance values can be used to subset the data, which can then be used to train a learner.\nWrapper methods can be used with any learner but need to train the learner potentially many times, making this the most expensive method.\n\n**Nested Resampling**\n\nFor hyperparameter tuning, a normal resampling (e.g. a cross-validation) is no longer sufficient to ensure an unbiased evaluation.\nConsider the following thought experiment to gain intuition for why this is the case.\nSuppose a learner has a hyperparameter that has no real effect on the fitted model, but only introduces random noise into the predictions.\nEvaluating different values for this hyperparameter, one will show the best performance (purely randomly).\nThis is the hyperparameter value that will be chosen as the best, although the hyperparameter has no real effect.\nTo discover this, another separate validation set is required -- it will reveal that the \"optimized\" setting really does not perform better than anything else.\n\nWe need a nested resampling to ensure unbiased estimates of the generalization error during hyperparameter optimization.\nWe discuss the following aspects in this part:\n\n* [Inner and outer resampling strategies](#nested-resampling) in nested resampling.\n* The [execution](#nested-resamp-exec) of nested resampling.\n* The [evaluation](#nested-resamp-eval) of resampling iterations.\n\n## Hyperparameter Tuning {#tuning}\n\n\n\n\n\nHyperparameters are the parameters of the learners that control how a model is fit to the data.\nThey are sometimes called second-level or second-order parameters of machine learning -- the parameters of the *models* are the first-order parameters and \"fit\" to the data during model training.\nThe hyperparameters of a learner can have a major impact on the performance of a learned model, but are often only optimized in an ad-hoc manner or not at all.\nThis process is often called model 'tuning'.\n\nHyperparameter tuning is supported via the [mlr3tuning](https://mlr3tuning.mlr-org.com) extension package.\nBelow you can find an illustration of the general process:\n\n\n::: {.cell hash='optimization_cache/html/optimization-002_b11da0582d004305b372558e254d35b5'}\n::: {.cell-output-display}\n![](images/tuning_process.svg)\n:::\n:::\n\n\nAt the heart of [mlr3tuning](https://mlr3tuning.mlr-org.com) are the R6 classes\n\n* [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html), [`TuningInstanceMultiCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html) to describe the tuning problem and store the results, and\n* [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) as the base class for implementations of tuning algorithms.\n\n### The `TuningInstance*` Classes {#tuning-optimization}\n\nWe will examine the optimization of a simple classification tree on the [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set as an introductory example here.\n\n\n::: {.cell hash='optimization_cache/html/optimization-003_3ed93181462a52827e85586cafa17d04'}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\ntask = tsk(\"pima\")\nprint(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskClassif:pima> (768 x 9): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n    triceps\n```\n:::\n:::\n\n\nWe use the [rpart](https://cran.r-project.org/package=rpart) classification tree and choose a subset of the hyperparameters we want to tune.\nThis is often referred to as the \"tuning space\".\nFirst, let's look at all the hyperparameters that are available.\nInformation on what they do can be found in [`the documentation of the learner`](https://www.rdocumentation.org/packages/rpart/topics/rpart.control).\n\n\n::: {.cell hash='optimization_cache/html/optimization-004_3a676a313aa26e63f388bff0ced7e9aa'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\nlearner$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n                id    class lower upper nlevels        default value\n 1:             cp ParamDbl     0     1     Inf           0.01      \n 2:     keep_model ParamLgl    NA    NA       2          FALSE      \n 3:     maxcompete ParamInt     0   Inf     Inf              4      \n 4:       maxdepth ParamInt     1    30      30             30      \n 5:   maxsurrogate ParamInt     0   Inf     Inf              5      \n 6:      minbucket ParamInt     1   Inf     Inf <NoDefault[3]>      \n 7:       minsplit ParamInt     1   Inf     Inf             20      \n 8: surrogatestyle ParamInt     0     1       2              0      \n 9:   usesurrogate ParamInt     0     2       3              2      \n10:           xval ParamInt     0   Inf     Inf             10     0\n```\n:::\n:::\n\n\nHere, we opt to tune two hyperparameters:\n\n* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.\n* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.\n\nThe tuning space needs to be bounded with lower and upper bounds for the values of the hyperparameters:\n\n\n::: {.cell hash='optimization_cache/html/optimization-005_c58c55de91e6aa2ca9743526f0b1d223'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cp = p_dbl(lower = 0.001, upper = 0.1),\n  minsplit = p_int(lower = 1, upper = 10)\n)\nsearch_space\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n         id    class lower upper nlevels        default value\n1:       cp ParamDbl 0.001   0.1     Inf <NoDefault[3]>      \n2: minsplit ParamInt 1.000  10.0      10 <NoDefault[3]>      \n```\n:::\n:::\n\n\nThe bounds are usually set based on experience.\n\nNext, we need to specify how to evaluate the performance of a trained model.\nFor this, we need to choose a [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and a [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html).\n\n\n::: {.cell hash='optimization_cache/html/optimization-006_a8b7bb15cbc1bc5c11765c20bc6dcee3'}\n\n```{.r .cell-code}\nhout = rsmp(\"holdout\")\nmeasure = msr(\"classif.ce\")\n```\n:::\n\n\nFinally, we have to specify the budget available for tuning.\nThis is a crucial step, as exhaustively evaluating all possible hyperparameter configurations is usually not feasible.\n[mlr3](https://mlr3.mlr-org.com) allows to specify complex termination criteria by selecting one of the available [`Terminators`](https://bbotk.mlr-org.com/reference/Terminator.html):\n\n* Terminate after a given time ([`TerminatorClockTime`](https://bbotk.mlr-org.com/reference/mlr_terminators_clock_time.html)).\n* Terminate after a given number of iterations ([`TerminatorEvals`](https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html)).\n* Terminate after a specific performance has been reached ([`TerminatorPerfReached`](https://bbotk.mlr-org.com/reference/mlr_terminators_perf_reached.html)).\n* Terminate when tuning does find a better configuration for a given number of iterations ([`TerminatorStagnation`](https://bbotk.mlr-org.com/reference/mlr_terminators_stagnation.html)).\n* A combination of the above in an *ALL* or *ANY* fashion ([`TerminatorCombo`](https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html)).\n\nFor this short introduction, we specify a budget of 20 iterations and then put everything together into a [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html):\n\n\n::: {.cell hash='optimization_cache/html/optimization-007_ebc3c8f775e0668b7b19adf9735e720a'}\n\n```{.r .cell-code}\nlibrary(\"mlr3tuning\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: paradox\n```\n:::\n\n```{.r .cell-code}\nevals20 = trm(\"evals\", n_evals = 20)\n\ninstance = TuningInstanceSingleCrit$new(\n  task = task,\n  learner = learner,\n  resampling = hout,\n  measure = measure,\n  search_space = search_space,\n  terminator = evals20\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.rpart_on_pima>\n* Search Space:\n         id    class lower upper nlevels\n1:       cp ParamDbl 0.001   0.1     Inf\n2: minsplit ParamInt 1.000  10.0      10\n* Terminator: <TerminatorEvals>\n```\n:::\n:::\n\n\nTo start the tuning, we still need to select how the optimization should take place.\nIn other words, we need to choose the **optimization algorithm** via the [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) class.\n\n### The `Tuner` Class {#tuning-algorithms}\n\nThe following algorithms are currently implemented in [mlr3tuning](https://mlr3tuning.mlr-org.com):\n\n* Grid Search ([`TunerGridSearch`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html))\n* Random Search ([`TunerRandomSearch`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_random_search.html)) [@bergstra2012]\n* Generalized Simulated Annealing ([`TunerGenSA`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_gensa.html))\n* Non-Linear Optimization ([`TunerNLoptr`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_nloptr.html))\n\nIf you're interested in learning more about these approaches, the [Wikipedia page on hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization) is a good place to start.\n\nIn this example, we will use a simple grid search with a grid resolution of 5.\n\n\n::: {.cell hash='optimization_cache/html/optimization-008_93832fcf3583eaeae2a0361fa3d4c623'}\n\n```{.r .cell-code}\ntuner = tnr(\"grid_search\", resolution = 5)\n```\n:::\n\n\nAs we have only numeric parameters, [`TunerGridSearch`](https://mlr3tuning.mlr-org.com/reference/mlr_tuners_grid_search.html) will create an equidistant grid between the respective upper and lower bounds.\nOur two-dimensional grid of resolution 5 consists of $5^2 = 25$ configurations.\nEach configuration is a distinct setting of hyperparameter values for the previously defined [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) which is then fitted to the task and evaluated using the provided [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html).\nAll configurations will be examined by the tuner (in a random order), until either all configurations are evaluated or the [`Terminator`](https://bbotk.mlr-org.com/reference/Terminator.html) signals that the budget is exhausted, i.e. here the tuner will stop after evaluating 20 of the 25 total configurations.\n\n### Triggering the Tuning {#tuning-triggering}\n\nTo start the tuning, we simply pass the [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html) to the `$optimize()` method of the initialized [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html).\nThe tuner proceeds as follows:\n\n1. The [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) proposes at least one hyperparameter configuration to evaluate (the [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html) may propose multiple points to be able to evaluate them in parallel, which can be controlled via the setting `batch_size`).\n1. For each configuration, the given [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) is fitted on the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and evaluated using the provided [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html).\n1  All evaluations are stored in the archive of the [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html).\n1. The [`Terminator`](https://bbotk.mlr-org.com/reference/Terminator.html) is queried if the budget is exhausted.\n1  If the budget is not exhausted, go back to 1), else terminate.\n1. Determine the configurations with the best observed performance from the archive.\n1. Store the best configurations as result in the tuning instance object.\n   The best hyperparameter settings (`$result_learner_param_vals`) and the corresponding measured performance (`$result_y`) can be retrieved from the tuning instance.\n\n\n::: {.cell hash='optimization_cache/html/optimization-009_8adccc3adb81c4ca3a3fdf17b7e1acb5'}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      cp minsplit learner_param_vals  x_domain classif.ce\n1: 0.001       10          <list[3]> <list[2]>  0.2734375\n```\n:::\n\n```{.r .cell-code}\ninstance$result_learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$xval\n[1] 0\n\n$cp\n[1] 0.001\n\n$minsplit\n[1] 10\n```\n:::\n\n```{.r .cell-code}\ninstance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2734375 \n```\n:::\n:::\n\n\nYou can investigate all of the evaluations that were performed; they are stored in the archive of the [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html) and can be accessed by using [`as.data.table()`](https://www.rdocumentation.org/packages/data.table/topics/as.data.table):\n\n\n::: {.cell hash='optimization_cache/html/optimization-010_b9e4553e8856b84d396e8ba247a5806d'}\n\n```{.r .cell-code}\nas.data.table(instance$archive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         cp minsplit classif.ce x_domain_cp x_domain_minsplit runtime_learners\n 1: 0.00100        5  0.2929688     0.00100                 5            0.011\n 2: 0.00100        3  0.3125000     0.00100                 3            0.008\n 3: 0.10000        1  0.2773438     0.10000                 1            0.005\n 4: 0.02575       10  0.2773438     0.02575                10            0.005\n 5: 0.10000        5  0.2773438     0.10000                 5            0.006\n 6: 0.10000        3  0.2773438     0.10000                 3            0.009\n 7: 0.00100       10  0.2734375     0.00100                10            0.006\n 8: 0.00100        1  0.3125000     0.00100                 1            0.008\n 9: 0.00100        8  0.2890625     0.00100                 8            0.007\n10: 0.07525        1  0.2773438     0.07525                 1            0.006\n11: 0.05050        8  0.2773438     0.05050                 8            0.006\n12: 0.05050        3  0.2773438     0.05050                 3            0.006\n13: 0.02575        1  0.2773438     0.02575                 1            0.006\n14: 0.02575        3  0.2773438     0.02575                 3            0.006\n15: 0.07525        3  0.2773438     0.07525                 3            0.005\n16: 0.02575        5  0.2773438     0.02575                 5            0.005\n17: 0.07525        8  0.2773438     0.07525                 8            0.006\n18: 0.05050       10  0.2773438     0.05050                10            0.007\n19: 0.07525        5  0.2773438     0.07525                 5            0.007\n20: 0.05050        1  0.2773438     0.05050                 1            0.009\n5 variables not shown: [timestamp, batch_nr, warnings, errors, resample_result]\n```\n:::\n:::\n\n\nAltogether, the grid search evaluated 20/25 different hyperparameter configurations in a random order before the [`Terminator`](https://bbotk.mlr-org.com/reference/Terminator.html) stopped the tuning.\nIn this example there were multiple configurations with the same best classification error, and without other criteria, the first one was returned.\nYou may want to choose the configuration with the lowest classification error as well as time to train the model or some other combination of criteria for hyper parameter selection.\nYou can do this with r ref(`\"TuningInstanceMultiCrit\")`, see [Tuning with Multiple Performance Measures](#mult-measures-tuning).\n\nThe associated resampling iterations can be accessed in the `\"BenchmarkResult\")` of the tuning instance:\n\n\n::: {.cell hash='optimization_cache/html/optimization-011_5833f1f8722ba674924b7c8a51b2f5c8'}\n\n```{.r .cell-code}\ninstance$archive$benchmark_result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<BenchmarkResult> of 20 rows with 20 resampling runs\n nr task_id    learner_id resampling_id iters warnings errors\n  1    pima classif.rpart       holdout     1        0      0\n  2    pima classif.rpart       holdout     1        0      0\n  3    pima classif.rpart       holdout     1        0      0\n  4    pima classif.rpart       holdout     1        0      0\n  5    pima classif.rpart       holdout     1        0      0\n  6    pima classif.rpart       holdout     1        0      0\n  7    pima classif.rpart       holdout     1        0      0\n  8    pima classif.rpart       holdout     1        0      0\n  9    pima classif.rpart       holdout     1        0      0\n 10    pima classif.rpart       holdout     1        0      0\n 11    pima classif.rpart       holdout     1        0      0\n 12    pima classif.rpart       holdout     1        0      0\n 13    pima classif.rpart       holdout     1        0      0\n 14    pima classif.rpart       holdout     1        0      0\n 15    pima classif.rpart       holdout     1        0      0\n 16    pima classif.rpart       holdout     1        0      0\n 17    pima classif.rpart       holdout     1        0      0\n 18    pima classif.rpart       holdout     1        0      0\n 19    pima classif.rpart       holdout     1        0      0\n 20    pima classif.rpart       holdout     1        0      0\n```\n:::\n:::\n\n\nThe `uhash` column links the resampling iterations to the evaluated configurations stored in `instance$archive$data`.\nThis allows e.g. to score the included [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html)s on a different performance measure.\n\n\n::: {.cell hash='optimization_cache/html/optimization-012_5d95aee247cda67e6be358283431fa81'}\n\n```{.r .cell-code}\ninstance$archive$benchmark_result$score(msr(\"classif.acc\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                   uhash nr              task task_id\n 1: aec70b5f-aca9-4579-8f96-b91ad1dca93a  1 <TaskClassif[50]>    pima\n 2: c3dad574-b33c-4e4c-8e93-84fb88f84727  2 <TaskClassif[50]>    pima\n 3: 1ba0ce7d-862a-4c51-839b-bfa3f056d58c  3 <TaskClassif[50]>    pima\n 4: 45d48c24-6519-45e5-81c1-d33c908a4794  4 <TaskClassif[50]>    pima\n 5: 8ca10ae3-f3f5-4ab9-8a19-ec980e0eeb53  5 <TaskClassif[50]>    pima\n 6: 81445cab-282a-42ac-827f-5b7c9a1f2e5d  6 <TaskClassif[50]>    pima\n 7: 01a29392-bd09-4569-ae59-f2f110058122  7 <TaskClassif[50]>    pima\n 8: 7ea8e595-0197-42f3-8a23-8cad75896236  8 <TaskClassif[50]>    pima\n 9: b93e8d71-8545-486b-ba92-0f8671238af6  9 <TaskClassif[50]>    pima\n10: 4ddd4ce0-e359-4181-9a2a-9369afbd8326 10 <TaskClassif[50]>    pima\n11: fb42588a-2e8c-46b8-8565-3e5aaddb7577 11 <TaskClassif[50]>    pima\n12: 89f16456-5d61-4165-b0f8-d3c01b253859 12 <TaskClassif[50]>    pima\n13: ad102a85-6c43-42d0-9566-76a4efbb6407 13 <TaskClassif[50]>    pima\n14: d6cbd21c-0762-4fe1-8ee2-9e7341dfebfe 14 <TaskClassif[50]>    pima\n15: 1307e957-7bea-48b4-b3e8-54b8847559f0 15 <TaskClassif[50]>    pima\n16: 9b4ba620-416b-4941-9ef2-724fe50e76a3 16 <TaskClassif[50]>    pima\n17: 3bca357a-09cf-488d-8091-ab5f506b125f 17 <TaskClassif[50]>    pima\n18: a7a19424-4062-4adb-a281-5cba01306f12 18 <TaskClassif[50]>    pima\n19: fa22830f-dc53-4308-8821-68416b3f9316 19 <TaskClassif[50]>    pima\n20: 64970fa9-fff6-47aa-8432-33eedf0d8e69 20 <TaskClassif[50]>    pima\n7 variables not shown: [learner, learner_id, resampling, resampling_id, iteration, prediction, classif.acc]\n```\n:::\n:::\n\n\nNow we can take the optimized hyperparameters, set them for the previously-created [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), and train it on the full dataset.\n\n\n::: {.cell hash='optimization_cache/html/optimization-013_c32aa559b99bdfa634c92e9c1c992c84'}\n\n```{.r .cell-code}\nlearner$param_set$values = instance$result_learner_param_vals\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to make a prediction on new, external data.\nNote that predicting on observations present in the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) should be avoided because the model has seen these observations already during tuning and training and therefore performance values would be statistically biased -- the resulting performance measure would be over-optimistic.\nTo get statistically unbiased performance estimates for a given task, [nested resampling](#nested-resampling) is required.\n\n### Tuning with Multiple Performance Measures {#mult-measures-tuning}\n\nWhen tuning, you might want to use multiple criteria to find the best configuration of hyperparameters.\nFor example, you might want the configuration with the lowest classification error and lowest time to train the model.\nThe full list of performance measures can be found [here](https://mlr3.mlr-org.com/reference/mlr_measures.html).\n\nContinuing the above example and tuning the same hyperparameters:\n\n* The complexity hyperparameter `cp` that controls when the learner considers introducing another branch.\n* The `minsplit` hyperparameter that controls how many observations must be present in a leaf for another split to be attempted.\n\nThe tuning process is identical to the previous example, however, this time we will specify two [`performance measures`](https://mlr3.mlr-org.com/reference/Measure.html), classification error and time to train the model (`time_train`).\n\n\n::: {.cell hash='optimization_cache/html/optimization-014_71f89dbb2f48168f5eded1142f54048e'}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.ce\", \"time_train\"))\n```\n:::\n\n\nInstead of creating a new [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html) with a single measure, we create a new [`TuningInstanceMultiCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html) with the two measures we are interested in here.\nOtherwise, it is the same as above.\n\n\n::: {.cell hash='optimization_cache/html/optimization-015_67798b7aad54b444f93235e95b267876'}\n\n```{.r .cell-code}\nlibrary(\"mlr3tuning\")\n\nevals20 = trm(\"evals\", n_evals = 20)\n\ninstance = TuningInstanceMultiCrit$new(\n  task = task,\n  learner = learner,\n  resampling = hout,\n  measures = measures,\n  search_space = search_space,\n  terminator = evals20\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TuningInstanceMultiCrit>\n* State:  Not optimized\n* Objective: <ObjectiveTuning:classif.rpart_on_pima>\n* Search Space:\n         id    class lower upper nlevels\n1:       cp ParamDbl 0.001   0.1     Inf\n2: minsplit ParamInt 1.000  10.0      10\n* Terminator: <TerminatorEvals>\n```\n:::\n:::\n\n\nAfter triggering the tuning, we will have the configuration with the best classification error and time to train the model.\n\n\n::: {.cell hash='optimization_cache/html/optimization-016_8dd1eca4287a6e56193f77df4a1e0968'}\n\n```{.r .cell-code}\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         cp minsplit learner_param_vals  x_domain classif.ce time_train\n 1: 0.10000        8          <list[3]> <list[2]>  0.2421875          0\n 2: 0.07525        1          <list[3]> <list[2]>  0.2421875          0\n 3: 0.05050        5          <list[3]> <list[2]>  0.2421875          0\n 4: 0.10000        5          <list[3]> <list[2]>  0.2421875          0\n 5: 0.07525        3          <list[3]> <list[2]>  0.2421875          0\n 6: 0.07525       10          <list[3]> <list[2]>  0.2421875          0\n 7: 0.10000        3          <list[3]> <list[2]>  0.2421875          0\n 8: 0.10000        1          <list[3]> <list[2]>  0.2421875          0\n 9: 0.05050       10          <list[3]> <list[2]>  0.2421875          0\n10: 0.10000       10          <list[3]> <list[2]>  0.2421875          0\n11: 0.05050        3          <list[3]> <list[2]>  0.2421875          0\n12: 0.07525        8          <list[3]> <list[2]>  0.2421875          0\n13: 0.05050        8          <list[3]> <list[2]>  0.2421875          0\n```\n:::\n\n```{.r .cell-code}\ninstance$result_learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$xval\n[1] 0\n\n[[1]]$cp\n[1] 0.1\n\n[[1]]$minsplit\n[1] 8\n\n\n[[2]]\n[[2]]$xval\n[1] 0\n\n[[2]]$cp\n[1] 0.07525\n\n[[2]]$minsplit\n[1] 1\n\n\n[[3]]\n[[3]]$xval\n[1] 0\n\n[[3]]$cp\n[1] 0.0505\n\n[[3]]$minsplit\n[1] 5\n\n\n[[4]]\n[[4]]$xval\n[1] 0\n\n[[4]]$cp\n[1] 0.1\n\n[[4]]$minsplit\n[1] 5\n\n\n[[5]]\n[[5]]$xval\n[1] 0\n\n[[5]]$cp\n[1] 0.07525\n\n[[5]]$minsplit\n[1] 3\n\n\n[[6]]\n[[6]]$xval\n[1] 0\n\n[[6]]$cp\n[1] 0.07525\n\n[[6]]$minsplit\n[1] 10\n\n\n[[7]]\n[[7]]$xval\n[1] 0\n\n[[7]]$cp\n[1] 0.1\n\n[[7]]$minsplit\n[1] 3\n\n\n[[8]]\n[[8]]$xval\n[1] 0\n\n[[8]]$cp\n[1] 0.1\n\n[[8]]$minsplit\n[1] 1\n\n\n[[9]]\n[[9]]$xval\n[1] 0\n\n[[9]]$cp\n[1] 0.0505\n\n[[9]]$minsplit\n[1] 10\n\n\n[[10]]\n[[10]]$xval\n[1] 0\n\n[[10]]$cp\n[1] 0.1\n\n[[10]]$minsplit\n[1] 10\n\n\n[[11]]\n[[11]]$xval\n[1] 0\n\n[[11]]$cp\n[1] 0.0505\n\n[[11]]$minsplit\n[1] 3\n\n\n[[12]]\n[[12]]$xval\n[1] 0\n\n[[12]]$cp\n[1] 0.07525\n\n[[12]]$minsplit\n[1] 8\n\n\n[[13]]\n[[13]]$xval\n[1] 0\n\n[[13]]$cp\n[1] 0.0505\n\n[[13]]$minsplit\n[1] 8\n```\n:::\n\n```{.r .cell-code}\ninstance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    classif.ce time_train\n 1:  0.2421875          0\n 2:  0.2421875          0\n 3:  0.2421875          0\n 4:  0.2421875          0\n 5:  0.2421875          0\n 6:  0.2421875          0\n 7:  0.2421875          0\n 8:  0.2421875          0\n 9:  0.2421875          0\n10:  0.2421875          0\n11:  0.2421875          0\n12:  0.2421875          0\n13:  0.2421875          0\n```\n:::\n:::\n\n\n### Automating the Tuning {#autotuner}\n\nWe can automate this entire process in [mlr3](https://mlr3.mlr-org.com) so that learners are tuned transparently, without the need to extract information on the best hyperparameter settings at the end.\nThe [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) wraps a learner and augments it with an automatic tuning process for a given set of hyperparameters.\nBecause the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) itself inherits from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) base class, it can be used like any other learner.\nIn keeping with our example above, we create a classification learner that tunes itself automatically.\nThis classification tree learner tunes the parameters `cp` and `minsplit` using an inner resampling (holdout).\nWe create a terminator which allows 10 evaluations, and use a simple random search as tuning algorithm:\n\n\n::: {.cell hash='optimization_cache/html/optimization-017_579fb6ab491d2abd0f585f3c6ead9471'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\nsearch_space = ps(\n  cp = p_dbl(lower = 0.001, upper = 0.1),\n  minsplit = p_int(lower = 1, upper = 10)\n)\nterminator = trm(\"evals\", n_evals = 10)\ntuner = tnr(\"random_search\")\n\nat = AutoTuner$new(\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  search_space = search_space,\n  terminator = terminator,\n  tuner = tuner\n)\nat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<AutoTuner:classif.rpart.tuned>\n* Model: list\n* Search Space:\n<ParamSet>\n         id    class lower upper nlevels        default value\n1:       cp ParamDbl 0.001   0.1     Inf <NoDefault[3]>      \n2: minsplit ParamInt 1.000  10.0      10 <NoDefault[3]>      \n* Packages: mlr3, mlr3tuning, rpart\n* Predict Type: response\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n:::\n:::\n\n\nWe can now use the learner like any other learner, calling the `$train()` and `$predict()` method. The differnce to a normal learner is that `$train()` runs the tuning, which will take longer than a normal training process.\n\n\n::: {.cell hash='optimization_cache/html/optimization-018_0678f5b9c101d9fe440ce69c3f49dca3'}\n\n```{.r .cell-code}\nat$train(task)\n```\n:::\n\n\nWe can also pass it to [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) and [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html), just like any other learner.\nThis would result in a [nested resampling](#nested-resampling).\n\n## Tuning Search Spaces {#searchspace}\n\nWhen running an optimization, it is important to inform the tuning algorithm about what hyperparameters are valid.\nHere the names, types, and valid ranges of each hyperparameter are important.\nAll this information is communicated with objects of the class [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), which is defined in [paradox](https://paradox.mlr-org.com).\nWhile it is possible to create [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)-objects using its `$new`-constructor, it is much shorter and readable to use the [`ps`](https://paradox.mlr-org.com/reference/ps.html)-shortcut, which will be presented here.\nFor an in-depth description of [paradox](https://paradox.mlr-org.com) and its classes, see the [[paradox](https://paradox.mlr-org.com) chapter](#paradox).\n\nNote, that [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) objects exist in two contexts.\nFirst, [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)-objects are used to define the space of valid parameter settings for a learner (and other objects).\nSecond, they are used to define a search space for tuning.\nWe are mainly interested in the latter.\nFor example we can consider the `minsplit` parameter of the [`classif.rpart Learner`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html).\nThe [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) associated with the learner has a lower but *no* upper bound.\nHowever, for tuning the value, a lower *and* upper bound must be given because tuning search spaces need to be bounded.\nFor [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) or [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) objects, typically \"unbounded\" [`ParamSets`](https://paradox.mlr-org.com/reference/ParamSet.html) are used.\nHere, however, we will mainly focus on creating \"bounded\" [`ParamSets`](https://paradox.mlr-org.com/reference/ParamSet.html) that can be used for tuning.\nSee the [in-depth [paradox](https://paradox.mlr-org.com) chapter](#paradox) for more details on using [`ParamSets`](https://paradox.mlr-org.com/reference/ParamSet.html) to  define parameter ranges for use-cases besides tuning.\n\n### Creating [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s\n\nAn empty `\"ParamSet\")` -- not yet very useful -- can be constructed using just the `\"ps\")` call:\n\n\n::: {.cell hash='optimization_cache/html/optimization-019_c2a970d1415cb44c864064d8c5cd54ae'}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nsearch_space = ps()\nprint(search_space)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\nEmpty.\n```\n:::\n:::\n\n\n[`ps`](https://paradox.mlr-org.com/reference/ps.html) takes named [`Domain`](https://paradox.mlr-org.com/reference/Domain.html) arguments that are turned into parameters.\nA possible search space for the `\"classif.svm\"` learner could for example be:\n\n\n::: {.cell hash='optimization_cache/html/optimization-020_7d314b8d1ef253c9e9c143c1dcb67fa2'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(lower = 0.1, upper = 10),\n  kernel = p_fct(levels = c(\"polynomial\", \"radial\"))\n)\nprint(search_space)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels        default value\n1:   cost ParamDbl   0.1    10     Inf <NoDefault[3]>      \n2: kernel ParamFct    NA    NA       2 <NoDefault[3]>      \n```\n:::\n:::\n\n\nThere are five domain constructors that produce a parameters when given to [`ps`](https://paradox.mlr-org.com/reference/ps.html):\n\n| Constructor               | Description                          | Is bounded?                        | Underlying Class    |\n| :-----------------------: | :----------------------------------: | :--------------------------------: | :-----------------: |\n| [`p_dbl`](https://paradox.mlr-org.com/reference/Domain.html)                   | Real valued parameter (\"double\")     | When `upper` and `lower` are given | [`ParamDbl`](https://paradox.mlr-org.com/reference/ParamDbl.html) |\n| [`p_int`](https://paradox.mlr-org.com/reference/Domain.html)                  | Integer parameter                    | When `upper` and `lower` are given | [`ParamInt`](https://paradox.mlr-org.com/reference/ParamInt.html) |\n| [`p_fct`](https://paradox.mlr-org.com/reference/Domain.html)                   | Discrete valued parameter (\"factor\") | Always                             | [`ParamFct`](https://paradox.mlr-org.com/reference/ParamFct.html) |\n| [`p_lgl`](https://paradox.mlr-org.com/reference/Domain.html)                 | Logical / Boolean parameter          | Always                             | [`ParamLgl`](https://paradox.mlr-org.com/reference/ParamLgl.html) |\n| [`p_uty`](https://paradox.mlr-org.com/reference/Domain.html)                   | Untyped parameter                    | Never                              | [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html) |\n\nThese domain constructors each take some of the following arguments:\n\n* **`lower`**, **`upper`**: lower and upper bound of numerical parameters ([`p_dbl`](https://paradox.mlr-org.com/reference/Domain.html) and [`p_int`](https://paradox.mlr-org.com/reference/Domain.html)). These need to be given to get bounded parameter spaces valid for tuning.\n* **`levels`**: Allowed categorical values for `p_fct` parameters.\n  Required argument for [`p_fct`](https://paradox.mlr-org.com/reference/Domain.html).\n  See [below](#autolevel) for more details on this parameter.\n* **`trafo`**: transformation function, see [below](#searchspace-trafo).\n* **`depends`**: dependencies, see [below](#searchspace-depends).\n* **`tags`**: Further information about a parameter, used for example by the [`hyperband`](#hyperband) tuner.\n* **`default`**: Value corresponding to default behavior when the parameter is not given.\n  Not used for tuning search spaces.\n* **`special_vals`**: Valid values besides the normally accepted values for a parameter.\n  Not used for tuning search spaces.\n* **`custom_check`**: Function that checks whether a value given to [`p_uty`](https://paradox.mlr-org.com/reference/Domain.html) is valid.\n  Not used for tuning search spaces.\n\nThe `lower` and `upper` parameters are always in the first and second position respectively, except for [`p_fct`](https://paradox.mlr-org.com/reference/Domain.html) where `levels` is in the first position.\nIt is preferred to omit the labels (ex: upper = 0.1 becomes just 0.1). This way of defining a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) is more concise than the equivalent definition above.\nPreferred:\n\n\n::: {.cell hash='optimization_cache/html/optimization-021_fbc6c9dd9630722bef09ce347e6f09be'}\n\n```{.r .cell-code}\nsearch_space = ps(cost = p_dbl(0.1, 10), kernel = p_fct(c(\"polynomial\", \"radial\")))\n```\n:::\n\n\n### Transformations (`trafo`) {#searchspace-trafo}\n\nWe can use the [paradox](https://paradox.mlr-org.com) function [`generate_design_grid`](https://paradox.mlr-org.com/reference/generate_design_grid.html) to look at the values that would be evaluated by grid search.\n(We are using [`rbindlist()`](https://www.rdocumentation.org/packages/data.table/topics/rbindlist) here because the result of `$transpose()` is a list that is harder to read.\nIf we didn't use `$transpose()`, on the other hand, the transformations that we investigate here are not applied.) In `generate_design_grid(search_space, 3)`, `search_space` is the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) argument and 3 is the specified resolution in the parameter space.\nThe resolution for categorical parameters is ignored; these parameters always produce a grid over all of their valid levels.\nFor numerical parameters the endpoints of the params are always included in the grid, so if there were 3 levels for the kernel instead of 2 there would be 9 rows, or if the resolution was 4 in this example there would be 8 rows in the resulting table.\n\n\n::: {.cell hash='optimization_cache/html/optimization-022_4a2d9ec691d8bcbb633eb89f1814fa7d'}\n\n```{.r .cell-code}\nlibrary(\"data.table\")\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    cost     kernel\n1:  0.10 polynomial\n2:  0.10     radial\n3:  5.05 polynomial\n4:  5.05     radial\n5: 10.00 polynomial\n6: 10.00     radial\n```\n:::\n:::\n\n\nWe notice that the `cost` parameter is taken on a linear scale.\nWe assume, however, that the difference of cost between `0.1` and `1` should have a similar effect as the difference between `1` and `10`.\nTherefore it makes more sense to tune it on a *logarithmic scale*.\nThis is done by using a **transformation** (`trafo`).\nThis is a function that is applied to a parameter after it has been sampled by the tuner.\nWe can tune `cost` on a logarithmic scale by sampling on the linear scale `[-1, 1]` and computing `10^x` from that value.\n\n::: {.cell hash='optimization_cache/html/optimization-023_9916f3fc3d7dd2c1ce58bd482282f27a'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  1.0 polynomial\n4:  1.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n```\n:::\n:::\n\n\nIt is even possible to attach another transformation to the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) as a whole that gets executed after individual parameter's transformations were performed.\nIt is given through the `.extra_trafo` argument and should be a function with parameters `x` and `param_set` that takes a list of parameter values in `x` and returns a modified list.\nThis transformation can access all parameter values of an evaluation and modify them with interactions.\nIt is even possible to add or remove parameters.\n(The following is a bit of a silly example.)\n\n\n::: {.cell hash='optimization_cache/html/optimization-024_5c2d1ef19f024761cd894612cf4f53d9'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  .extra_trafo = function(x, param_set) {\n    if (x$kernel == \"polynomial\") {\n      x$cost = x$cost * 2\n    }\n    x\n  }\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cost     kernel\n1:  0.2 polynomial\n2:  0.1     radial\n3:  2.0 polynomial\n4:  1.0     radial\n5: 20.0 polynomial\n6: 10.0     radial\n```\n:::\n:::\n\n\nThe available types of search space parameters are limited: continuous, integer, discrete, and logical scalars.\nThere are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions.\nThese can not be defined in a search space [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), and they are often given as [`ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html) in the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)'s [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\nWhen trying to tune over these hyperparameters, it is necessary to perform a Transformation that changes the type of a parameter.\n\nAn example is the `class.weights` parameter of the [Support Vector Machine](https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/) (SVM), which takes a named vector of class weights with one entry for each target class.\nThe trafo that would tune `class.weights` for the [`mlr_tasks_spam`](https://mlr3.mlr-org.com/reference/mlr_tasks_spam.html), `'tsk(\"spam\")` dataset could be:\n\n\n::: {.cell hash='optimization_cache/html/optimization-025_7f91e5babb8f9f12ec334d987db827f3'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  class.weights = p_dbl(0.1, 0.9, trafo = function(x) c(spam = x, nonspam = 1 - x))\n)\ngenerate_design_grid(search_space, 3)$transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$class.weights\n   spam nonspam \n    0.1     0.9 \n\n\n[[2]]\n[[2]]$class.weights\n   spam nonspam \n    0.5     0.5 \n\n\n[[3]]\n[[3]]$class.weights\n   spam nonspam \n    0.9     0.1 \n```\n:::\n:::\n\n\n(We are omitting [`rbindlist()`](https://www.rdocumentation.org/packages/data.table/topics/rbindlist) in this example because it breaks the vector valued return elements.)\n\n### Automatic Factor Level Transformation {#autolevel}\n\nA common use-case is the necessity to specify a list of values that should all be tried (or sampled from).\nIt may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried.\nOr it may be that a choice of special numeric values should be tried.\nFor this, the [`p_fct`](https://paradox.mlr-org.com/reference/Domain.html) constructor's `level` argument may be a value that is not a `character` vector, but something else.\nIf, for example, only the values `0.1`, `3`, and `10` should be tried for the `cost` parameter, even when doing random search, then the following search space would achieve that:\n\n\n::: {.cell hash='optimization_cache/html/optimization-026_7374647b8c6f130d8c119b6a92ecb80d'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n```\n:::\n:::\n\n\nThis is equivalent to the following:\n\n::: {.cell hash='optimization_cache/html/optimization-027_e3c336a9d3d5f2a9d9b5ca4b11d88764'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_fct(c(\"0.1\", \"3\", \"10\"),\n    trafo = function(x) list(`0.1` = 0.1, `3` = 3, `10` = 10)[[x]]),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   cost     kernel\n1:  0.1 polynomial\n2:  0.1     radial\n3:  3.0 polynomial\n4:  3.0     radial\n5: 10.0 polynomial\n6: 10.0     radial\n```\n:::\n:::\n\n\nNote: Though the resolution is 3 here, in this case it doesn't matter because both `cost` and `kernel` are factors (the resolution for categorical variables is ignored, these parameters always produce a grid over all their valid levels).\n\nThis may seem silly, but makes sense when considering that factorial tuning parameters are always `character` values:\n\n\n::: {.cell hash='optimization_cache/html/optimization-028_5ffd19fd55c5e38e604f1ce1295e048f'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_fct(c(0.1, 3, 10)),\n  kernel = p_fct(c(\"polynomial\", \"radial\"))\n)\ntypeof(search_space$params$cost$levels)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"character\"\n```\n:::\n:::\n\n\nBe aware that this results in an \"unordered\" hyperparameter, however.\nTuning algorithms that make use of ordering information of parameters, like genetic algorithms or model based optimization, will perform worse when this is done.\nFor these algorithms, it may make more sense to define a [`p_dbl`](https://paradox.mlr-org.com/reference/Domain.html) or [`p_int`](https://paradox.mlr-org.com/reference/Domain.html) with a more fitting trafo.\n\nThe `class.weights` case from above can also be implemented like this, if there are only a few candidates of `class.weights` vectors that should be tried.\nNote that the `levels` argument of [`p_fct`](https://paradox.mlr-org.com/reference/Domain.html) must be named if there is no easy way for `as.character()` to create names:\n\n\n::: {.cell hash='optimization_cache/html/optimization-029_0efdf9b52697d9b3205c1e8ec4b68872'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  class.weights = p_fct(\n    list(\n      candidate_a = c(spam = 0.5, nonspam = 0.5),\n      candidate_b = c(spam = 0.3, nonspam = 0.7)\n    )\n  )\n)\ngenerate_design_grid(search_space)$transpose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$class.weights\n   spam nonspam \n    0.5     0.5 \n\n\n[[2]]\n[[2]]$class.weights\n   spam nonspam \n    0.3     0.7 \n```\n:::\n:::\n\n\n### Parameter Dependencies (`depends`) {#searchspace-depends}\n\nSome parameters are only relevant when another parameter has a certain value, or one of several values.\nThe [Support Vector Machine](https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/) (SVM), for example, has the `degree` parameter that is only valid when `kernel` is `\"polynomial\"`.\nThis can be specified using the `depends` argument.\nIt is an expression that must involve other parameters and be of the form `<param> == <scalar>`, `<param> %in% <vector>`, or multiple of these chained by `&&`.\nTo tune the `degree` parameter, one would need to do the following:\n\n\n::: {.cell hash='optimization_cache/html/optimization-030_8cc655703060fcb5380c9ea2b6b8fd2c'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  cost = p_dbl(-1, 1, trafo = function(x) 10^x),\n  kernel = p_fct(c(\"polynomial\", \"radial\")),\n  degree = p_int(1, 3, depends = kernel == \"polynomial\")\n)\nrbindlist(generate_design_grid(search_space, 3)$transpose(), fill = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    cost     kernel degree\n 1:  0.1 polynomial      1\n 2:  0.1 polynomial      2\n 3:  0.1 polynomial      3\n 4:  0.1     radial     NA\n 5:  1.0 polynomial      1\n 6:  1.0 polynomial      2\n 7:  1.0 polynomial      3\n 8:  1.0     radial     NA\n 9: 10.0 polynomial      1\n10: 10.0 polynomial      2\n11: 10.0 polynomial      3\n12: 10.0     radial     NA\n```\n:::\n:::\n\n\n### Creating Tuning ParamSets from other ParamSets {#tune-token}\n\nHaving to define a tuning [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) for a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) that already has parameter set information may seem unnecessarily tedious, and there is indeed a way to create tuning [`ParamSets`](https://paradox.mlr-org.com/reference/ParamSet.html) from a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)'s [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), making use of as much information as already available.\n\nThis is done by setting values of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html)'s [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) to so-called [`TuneToken`](https://paradox.mlr-org.com/reference/to_tune.html)s, constructed with a [`to_tune`](https://paradox.mlr-org.com/reference/to_tune.html) call.\nThis can be done in the same way that other hyperparameters are set to specific values.\nIt can be understood as the hyperparameters being tagged for later tuning.\nThe resulting [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) used for tuning can be retrieved using the `$search_space()` method.\n\n\n::: {.cell hash='optimization_cache/html/optimization-031_7680f89dec08723edd0deccd211115ac'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.svm\")\nlearner$param_set$values$kernel = \"polynomial\" # for example\nlearner$param_set$values$degree = to_tune(lower = 1, upper = 3)\n\nprint(learner$param_set$search_space())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels        default value\n1: degree ParamInt     1     3       3 <NoDefault[3]>      \n```\n:::\n\n```{.r .cell-code}\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   degree\n1:      1\n2:      2\n3:      3\n```\n:::\n:::\n\n\nIt is possible to omit `lower` here, because it can be inferred from the lower bound of the `degree` parameter itself.\nFor other parameters, that are already bounded, it is possible to not give any bounds at all, because their ranges are already bounded.\nAn example is the logical `shrinking` hyperparameter:\n\n::: {.cell hash='optimization_cache/html/optimization-032_9fcb0d612ee7b2f85024c84eca625a76'}\n\n```{.r .cell-code}\nlearner$param_set$values$shrinking = to_tune()\n\nprint(learner$param_set$search_space())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n          id    class lower upper nlevels        default value\n1:    degree ParamInt     1     3       3 <NoDefault[3]>      \n2: shrinking ParamLgl    NA    NA       2           TRUE      \n```\n:::\n\n```{.r .cell-code}\nrbindlist(generate_design_grid(\n  learner$param_set$search_space(), 3)$transpose()\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   degree shrinking\n1:      1      TRUE\n2:      1     FALSE\n3:      2      TRUE\n4:      2     FALSE\n5:      3      TRUE\n6:      3     FALSE\n```\n:::\n:::\n\n\n`\"to_tune\")` can also be constructed with a [`Domain`](https://paradox.mlr-org.com/reference/Domain.html) object, i.e. something constructed with a `p_***` call.\nThis way it is possible to tune continuous parameters with discrete values, or to give trafos or dependencies.\nOne could, for example, tune the `cost` as above on three given special values, and introduce a dependency of `shrinking` on it.\nNotice that a short form for `to_tune(<levels>)` is a short form of `to_tune(p_fct(<levels>))`.\n\n:::{.callout-note}\nWhen introducing the dependency, we need to use the `degree` value from *before* the implicit trafo, which is the name or `as.character()` of the respective value, here `\"val2\"`!\n:::\n\n\n::: {.cell hash='optimization_cache/html/optimization-033_705a09c540ce9bf68c32274c7e5e3922'}\n\n```{.r .cell-code}\nlearner$param_set$values$type = \"C-classification\" # needs to be set because of a bug in paradox\nlearner$param_set$values$cost = to_tune(c(val1 = 0.3, val2 = 0.7))\nlearner$param_set$values$shrinking = to_tune(p_lgl(depends = cost == \"val2\"))\n\nprint(learner$param_set$search_space())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n          id    class lower upper nlevels        default parents value\n1:      cost ParamFct    NA    NA       2 <NoDefault[3]>              \n2:    degree ParamInt     1     3       3 <NoDefault[3]>              \n3: shrinking ParamLgl    NA    NA       2 <NoDefault[3]>    cost      \nTrafo is set.\n```\n:::\n\n```{.r .cell-code}\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   degree cost shrinking\n1:      1  0.3        NA\n2:      1  0.7      TRUE\n3:      1  0.7     FALSE\n4:      2  0.3        NA\n5:      2  0.7      TRUE\n6:      2  0.7     FALSE\n7:      3  0.3        NA\n8:      3  0.7      TRUE\n9:      3  0.7     FALSE\n```\n:::\n:::\n\n\nThe `\"search_space()` picks up dependencies fromt the underlying [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) automatically.\nSo if the `kernel` is tuned, then `degree` automatically gets the dependency on it, without us having to specify that.\n(Here we reset `cost` and `shrinking` to `NULL` for the sake of clarity of the generated output.)\n\n\n::: {.cell hash='optimization_cache/html/optimization-034_d0191761af1a2ab094b637604d36fd6c'}\n\n```{.r .cell-code}\nlearner$param_set$values$cost = NULL\nlearner$param_set$values$shrinking = NULL\nlearner$param_set$values$kernel = to_tune(c(\"polynomial\", \"radial\"))\n\nprint(learner$param_set$search_space())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels        default parents value\n1: degree ParamInt     1     3       3 <NoDefault[3]>  kernel      \n2: kernel ParamFct    NA    NA       2 <NoDefault[3]>              \n```\n:::\n\n```{.r .cell-code}\nrbindlist(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), fill = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       kernel degree\n1: polynomial      1\n2: polynomial      2\n3: polynomial      3\n4:     radial     NA\n```\n:::\n:::\n\n\nIt is even possible to define whole [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html)s that get tuned over for a single parameter.\nThis may be especially useful for vector hyperparameters that should be searched along multiple dimensions.\nThis [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) must, however, have an `.extra_trafo` that returns a list with a single element, because it corresponds to a single hyperparameter that is being tuned.\nSuppose the `class.weights` hyperparameter should be tuned along two dimensions:\n\n\n::: {.cell hash='optimization_cache/html/optimization-035_c5828daf9f3da8db2ce32c24855863a1'}\n\n```{.r .cell-code}\nlearner$param_set$values$class.weights = to_tune(\n  ps(spam = p_dbl(0.1, 0.9), nonspam = p_dbl(0.1, 0.9),\n    .extra_trafo = function(x, param_set) list(c(spam = x$spam, nonspam = x$nonspam))\n))\nhead(generate_design_grid(learner$param_set$search_space(), 3)$transpose(), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[[1]]$kernel\n[1] \"polynomial\"\n\n[[1]]$degree\n[1] 1\n\n[[1]]$class.weights\n   spam nonspam \n    0.1     0.1 \n\n\n[[2]]\n[[2]]$kernel\n[1] \"polynomial\"\n\n[[2]]$degree\n[1] 1\n\n[[2]]$class.weights\n   spam nonspam \n    0.1     0.5 \n\n\n[[3]]\n[[3]]$kernel\n[1] \"polynomial\"\n\n[[3]]$degree\n[1] 1\n\n[[3]]$class.weights\n   spam nonspam \n    0.1     0.9 \n```\n:::\n:::\n\n\n## Nested Resampling {#nested-resampling}\n\nEvaluating a machine learning model often requires an additional layer of resampling when hyperparameters or features have to be selected.\nNested resampling separates these model selection steps from the process estimating the performance of the model.\nIf the same data is used for the model selection steps and the evaluation of the model itself, the resulting performance estimate of the model might be severely biased.\nOne reason for this bias is that the repeated evaluation of the model on the test data could leak information about its structure into the model, this results in over-optimistic performance estimates.\nKeep in mind that nested resampling is a statistical procedure to estimate the predictive performance of the model trained on the full dataset.\nNested resampling is not a procedure to select optimal hyperparameters.\nThe resampling produces many hyperparameter configurations which should be not used to construct a final model [@Simon2007].\n\n\n::: {.cell hash='optimization_cache/html/optimization-036_251b88968176d077bef43beba488dbfe'}\n::: {.cell-output-display}\n![](images/nested_resampling.png){width=98%}\n:::\n:::\n\n\nThe graphic above illustrates nested resampling for hyperparameter tuning with 3-fold cross-validation in the outer resampling and 4-fold cross-validation in the inner resampling.\n\nThe nested resampling process:\n\n1. Uses a 3-fold cross-validation to get different testing and training data sets (outer resampling).\n1. Within the training data uses a 4-fold cross-validation to get different inner testing and training data sets (inner resampling).\n1. Tunes the hyperparameters using the inner data splits.\n1. Fits the learner on the outer training data set using the tuned hyperparameter configuration obtained with the inner resampling.\n1. Evaluates the performance of the learner on the outer testing data.\n1. 2-5 is repeated for each of the three folds (outer resampling).\n1. The three performance values are aggregated for an unbiased performance estimate.\n\nSee also [this article](https://machinelearningmastery.com/k-fold-cross-validation/) for more explanations.\n\n### Execution {#nested-resamp-exec}\n\nThe previous [section](#tuning) examined the optimization of a simple classification tree on the [`mlr_tasks_pima`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html).\nWe continue the example and estimate the predictive performance of the model with nested resampling.\n\nWe use a 4-fold cross-validation in the inner resampling loop.\nThe [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) executes the hyperparameter tuning and is stopped after 5 evaluations.\nThe hyperparameter configurations are proposed by grid search.\n\n\n::: {.cell hash='optimization_cache/html/optimization-037_e73e6947e12c3235e890cecf11d64a0d'}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\nlearner = lrn(\"classif.rpart\")\nresampling = rsmp(\"cv\", folds = 4)\nmeasure = msr(\"classif.ce\")\nsearch_space = ps(cp = p_dbl(lower = 0.001, upper = 0.1))\nterminator = trm(\"evals\", n_evals = 5)\ntuner = tnr(\"grid_search\", resolution = 10)\n\nat = AutoTuner$new(learner, resampling, measure, terminator, tuner, search_space)\n```\n:::\n\n\nA 3-fold cross-validation is used in the outer resampling loop.\nOn each of the three outer train sets hyperparameter tuning is done and we receive three optimized hyperparameter configurations.\nTo execute the nested resampling, we pass the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) to the [`resample()`](https://mlr3.mlr-org.com/reference/resample.html) function.\nWe have to set `store_models = TRUE` because we need the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) models to investigate the inner tuning.\n\n\n::: {.cell hash='optimization_cache/html/optimization-038_24b33a3d045f8ce9217e0926f30e6abc'}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\nouter_resampling = rsmp(\"cv\", folds = 3)\n\nrr = resample(task, at, outer_resampling, store_models = TRUE)\n```\n:::\n\n\nYou can freely combine different inner and outer resampling strategies.\nNested resampling is not restricted to hyperparameter tuning.\nYou can swap the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) for a [`AutoFSelector`](https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html) and estimate the performance of a model which is fitted on an optimized feature subset.\n\n### Evaluation {#nested-resamp-eval}\n\nWith the created [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) we can now inspect the executed resampling iterations more closely.\nSee the section on [Resampling](#resampling) for more detailed information about [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html) objects.\n\nWe check the inner tuning results for stable hyperparameters.\nThis means that the selected hyperparameters should not vary too much.\nWe might observe unstable models in this example because the small data set and the low number of resampling iterations might introduces too much randomness.\nUsually, we aim for the selection of stable hyperparameters for all outer training sets.\n\n\n::: {.cell hash='optimization_cache/html/optimization-039_e58af4ff7fcda4054b594e8d417d031c'}\n\n```{.r .cell-code}\nextract_inner_tuning_results(rr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   iteration    cp classif.ce learner_param_vals  x_domain task_id\n1:         1 0.056  0.2734375          <list[2]> <list[1]>    pima\n2:         2 0.034  0.2460938          <list[2]> <list[1]>    pima\n3:         3 0.001  0.2519531          <list[2]> <list[1]>    pima\n2 variables not shown: [learner_id, resampling_id]\n```\n:::\n:::\n\n\nNext, we want to compare the predictive performances estimated on the outer resampling to the inner resampling.\nSignificantly lower predictive performances on the outer resampling indicate that the models with the optimized hyperparameters overfit the data.\n\n\n::: {.cell hash='optimization_cache/html/optimization-040_bd4e6fa4231b70c2baefee33ab005448'}\n\n```{.r .cell-code}\nrr$score()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                task task_id         learner          learner_id\n1: <TaskClassif[50]>    pima <AutoTuner[42]> classif.rpart.tuned\n2: <TaskClassif[50]>    pima <AutoTuner[42]> classif.rpart.tuned\n3: <TaskClassif[50]>    pima <AutoTuner[42]> classif.rpart.tuned\n5 variables not shown: [resampling, resampling_id, iteration, prediction, classif.ce]\n```\n:::\n:::\n\n\nThe aggregated performance of all outer resampling iterations is essentially the unbiased performance of the model with optimal hyperparameter found by grid search.\n\n\n::: {.cell hash='optimization_cache/html/optimization-041_38550ebbbb16dec72d621834e0d84c11'}\n\n```{.r .cell-code}\nrr$aggregate()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.2838542 \n```\n:::\n:::\n\n\nNote that nested resampling is computationally expensive.\nFor this reason we use relatively small number of hyperparameter configurations and a low number of resampling iterations in this example.\nIn practice, you normally have to increase both.\nAs this is computationally intensive you might want to have a look at the section on [Parallelization](#parallelization).\n\n### Final Model {#nested-final-model}\n\nWe can use the [`AutoTuner`](https://mlr3tuning.mlr-org.com/reference/AutoTuner.html) to tune the hyperparameters of our learner and fit the final model on the full data set.\n\n\n::: {.cell hash='optimization_cache/html/optimization-042_1c64fc40fc6f67a38f3755ca0fd1f38a'}\n\n```{.r .cell-code}\nat$train(task)\n```\n:::\n\n\nThe trained model can now be used to make predictions on new data.\n\n:::{.callout-warning}\nA common mistake is to report the performance estimated on the resampling sets on which the tuning was performed (`at$tuning_result$classif.ce`) as the model's performance.\n:::\n\nInstead, the performance estimated with nested resampling should be reported as the actual performance of the model.\n\n## Tuning with Hyperband {#hyperband}\n\nBesides the more traditional tuning methods, the ecosystem around [mlr3](https://mlr3.mlr-org.com) offers another procedure for hyperparameter optimization called Hyperband implemented in the [mlr3hyperband](https://mlr3hyperband.mlr-org.com) package.\n\nHyperband is a budget-oriented procedure, weeding out suboptimal performing configurations early on during a partially sequential training process, increasing tuning efficiency as a consequence.\nFor this, a combination of incremental resource allocation and early stopping is used: As optimization progresses, computational resources are increased for more promising configurations, while less promising ones are terminated early.\n\nTo give an introductory analogy, imagine two horse trainers are given eight untrained horses.\nBoth trainers want to win the upcoming race, but they are only given 32 units of food.\nGiven that each horse can be fed up to 8 units food (\"maximum budget\" per horse), there is not enough food for all the horses.\nIt is critical to identify the most promising horses early, and give them enough food to improve.\nSo, the trainers need to develop a strategy to split up the food in the best possible way.\nThe first trainer is very optimistic and wants to explore the full capabilities of a horse, because he does not want to pass a judgment on a horse's performance unless it has been fully trained.\nSo, he divides his budget by the maximum amount he can give to a horse (lets say eight, so $32 / 8 = 4$) and randomly picks four horses - his budget simply is not enough to fully train more.\nThose four horses are then trained to their full capabilities, while the rest is set free.\nThis way, the trainer is confident about choosing the best out of the four trained horses, but he might have overlooked the horse with the highest potential since he only focused on half of them.\nThe other trainer is more creative and develops a different strategy.\nHe thinks, if a horse is not performing well at the beginning, it will also not improve after further training.\nBased on this assumption, he decides to give one unit of food to each horse and observes how they develop.\nAfter the initial food is consumed, he checks their performance and kicks the slowest half out of his training regime.\nThen, he increases the available food for the remaining, further trains them until the food is consumed again, only to kick out the worst half once more.\nHe repeats this until the one remaining horse gets the rest of the food.\nThis means only one horse is fully trained, but on the flip side, he was able to start training with all eight horses.\n\nOn race day, all the horses are put on the starting line.\nBut which trainer will have the winning horse?\nThe one, who tried to train a maximum amount of horses to their fullest?\nOr the other one, who made assumptions about the training progress of his horses?\nHow the training phases may possibly look like is visualized in @fig-04-optimization-hyperband-01.\n\n\n::: {.cell layout-align=\"center\" hash='optimization_cache/html/fig-04-optimization-hyperband-01_ce10fd2282547883b05b474127bb9676'}\n::: {.cell-output-display}\n![Visulization of how the training processes may look like. The left plot corresponds to the non-selective trainer, while the right one to the selective trainer.](images/horse_training1.png){#fig-04-optimization-hyperband-01 fig-align='center' width=99%}\n:::\n:::\n\n\nHyperband works very similar in some ways, but also different in others.\nIt is not embodied by one of the trainers in our analogy, but more by the person, who would pay them.\nHyperband consists of several brackets, each bracket corresponding to a trainer, and we do not care about horses but about hyperparameter configurations of a machine learning algorithm.\nThe budget is not in terms of food, but in terms of a hyperparameter of the learner that scales in some way with the computational effort.\nAn example is the number of epochs we train a neural network, or the number of iterations in boosting.\nFurthermore, there are not only two brackets (or trainers), but several, each placed at a unique spot between fully explorative of later training stages and extremely selective, equal to higher exploration of early training stages.\nThe level of selection aggressiveness is handled by a user-defined parameter called $\\eta$.\nSo, $1/\\eta$ is the fraction of remaining configurations after a bracket removes his worst performing ones, but $\\eta$ is also the factor by that the budget is increased for the next stage.\nBecause there is a different maximum budget per configuration that makes sense in different scenarios, the user also has to set this as the $R$ parameter.\nNo further parameters are required for Hyperband -- the full required budget across all brackets is indirectly given by\n$$\n(\\lfloor \\log_{\\eta}{R} \\rfloor + 1)^2 * R\n$$\n\n[@Li2016].\nTo give an idea how a full bracket layout might look like for a specific $R$ and $\\eta$, a quick overview is given in the following table.\n\n\n::: {.cell hash='optimization_cache/html/optimization-044_20aa556da1ed0737f049b23880b5d01c'}\n::: {.cell-output-display}\n<table class=\"kable_wrapper\">\n<caption>Hyperband layout for $\\eta = 2$ and $R = 8$, consisting of four brackets with $n$ as the amount of active configurations.</caption>\n<tbody>\n  <tr>\n   <td> \n\n| stage| budget|  n|\n|-----:|------:|--:|\n|     1|      1|  8|\n|     2|      2|  4|\n|     3|      4|  2|\n|     4|      8|  1|\n\n </td>\n   <td> \n\n| stage| budget|  n|\n|-----:|------:|--:|\n|     1|      2|  6|\n|     2|      4|  3|\n|     3|      8|  1|\n\n </td>\n   <td> \n\n| stage| budget|  n|\n|-----:|------:|--:|\n|     1|      4|  4|\n|     2|      8|  2|\n\n </td>\n   <td> \n\n| stage| budget|  n|\n|-----:|------:|--:|\n|     1|      8|  4|\n\n </td>\n  </tr>\n</tbody>\n</table>\n:::\n:::\n\n\nOf course, early termination based on a performance criterion may be disadvantageous if it is done too aggressively in certain scenarios.\nA learner to jumping radically in its estimated performance during the training phase may get the best configurations canceled too early, simply because they do not improve quickly enough compared to others.\nIn other words, it is often unclear beforehand if having an high amount of configurations $n$, that gets aggressively discarded early, is better than having a high budget $B$ per configuration.\nThe arising tradeoff, that has to be made, is called the \"$n$ versus $B/n$ problem\".\nTo create a balance between selection based on early training performance versus exploration of training performances in later training stages, $\\lfloor \\log_{\\eta}{R} \\rfloor + 1$ brackets are constructed with an associated set of varying sized configurations.\nThus, some brackets contain more configurations, with a small initial budget.\nIn these, a lot are discarded after having been trained for only a short amount of time, corresponding to the selective trainer in our horse analogy.\nOthers are constructed with fewer configurations, where discarding only takes place after a significant amount of budget was consumed.\nThe last bracket usually never discards anything, but also starts with only very few configurations -- this is equivalent to the trainer explorative of later stages.\nThe former corresponds high $n$, while the latter high $B/n$.\nEven though different brackets are initialized with a different amount of configurations and different initial budget sizes, each bracket is assigned (approximately) the same budget $(\\lfloor \\log_{\\eta}{R} \\rfloor + 1) * R$.\n\nThe configurations at the start of each bracket are initialized by random, often uniform sampling.\nNote that currently all configurations are trained completely from the beginning, so no online updates of models from stage to stage is happening.\n\nTo identify the budget for evaluating Hyperband, the user has to specify explicitly which hyperparameter of the learner influences the budget by extending a single hyperparameter in the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) with an argument (`tags = \"budget\"`), like in the following snippet:\n\n\n::: {.cell hash='optimization_cache/html/optimization-045_a703203f40f1594dfab76fa6fd46346b'}\n\n```{.r .cell-code}\nlibrary(\"mlr3verse\")\n\n# Hyperparameter subset of XGBoost\nsearch_space = ps(\n  nrounds = p_int(lower = 1, upper = 16, tags = \"budget\"),\n  booster = p_fct(levels = c(\"gbtree\", \"gblinear\", \"dart\"))\n)\n```\n:::\n\n\nThanks to the broad ecosystem of the [mlr3verse](https://mlr3verse.mlr-org.com) a learner does not require a natural budget parameter.\nA typical case of this would be decision trees.\nBy using subsampling as preprocessing with [mlr3pipelines](https://mlr3pipelines.mlr-org.com), we can work around a lacking budget parameter.\n\n\n::: {.cell hash='optimization_cache/html/optimization-046_d99d4cc6dd3d70b20c83c188e2271587'}\n\n```{.r .cell-code}\nset.seed(123)\n\n# extend \"classif.rpart\" with \"subsampling\" as preprocessing step\nll = po(\"subsample\") %>>% lrn(\"classif.rpart\")\n\n# extend hyperparameters of \"classif.rpart\" with subsampling fraction as budget\nsearch_space = ps(\n  classif.rpart.cp = p_dbl(lower = 0.001, upper = 0.1),\n  classif.rpart.minsplit = p_int(lower = 1, upper = 10),\n  subsample.frac = p_dbl(lower = 0.1, upper = 1, tags = \"budget\")\n)\n```\n:::\n\n\nWe can now plug the new learner with the extended hyperparameter set into a [`TuningInstanceSingleCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceSingleCrit.html) the same way as usual.\nNaturally, Hyperband terminates once all of its brackets are evaluated, so a [`Terminator`](https://bbotk.mlr-org.com/reference/Terminator.html) in the tuning instance acts as an upper bound and should be only set to a low value if one is unsure of how long Hyperband will take to finish under the given settings.\n\n\n::: {.cell hash='optimization_cache/html/optimization-047_b25fd0d38ebef767e8c910159fa902e5'}\n\n```{.r .cell-code}\ninstance = TuningInstanceSingleCrit$new(\n  task = tsk(\"iris\"),\n  learner = ll,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\n```\n:::\n\n\nNow, we initialize a new instance of the [`mlr3hyperband::mlr_tuners_hyperband`](https://mlr3hyperband.mlr-org.com/reference/mlr_tuners_hyperband.html) class and start tuning with it.\n\n\n::: {.cell hash='optimization_cache/html/optimization-048_a6acf307c51f1076372bdcf241cbb1e6'}\n\n```{.r .cell-code}\nlibrary(\"mlr3hyperband\")\ntuner = tnr(\"hyperband\", eta = 3)\n\n# reduce logging output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals\n1:       0.07348139                      5      0.1111111          <list[6]>\n2 variables not shown: [x_domain, classif.ce]\n```\n:::\n:::\n\n\nTo receive the results of each sampled configuration, we simply run the following snippet.\n\n\n::: {.cell hash='optimization_cache/html/optimization-049_686f3ff0a868e54700c2d473634fe58a'}\n\n```{.r .cell-code}\nas.data.table(instance$archive)[, c(\n  \"subsample.frac\",\n  \"classif.rpart.cp\",\n  \"classif.rpart.minsplit\",\n  \"classif.ce\"\n), with = FALSE]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce\n 1:      0.1111111       0.02532664                      3       0.04\n 2:      0.1111111       0.07348139                      5       0.02\n 3:      0.1111111       0.08489786                      3       0.02\n 4:      0.1111111       0.05025520                      6       0.02\n 5:      0.1111111       0.03940299                      4       0.02\n 6:      0.1111111       0.02539845                      7       0.42\n 7:      0.1111111       0.01199855                      4       0.14\n 8:      0.1111111       0.03960945                      4       0.02\n 9:      0.1111111       0.05762160                      6       0.02\n10:      0.3333333       0.04186187                      8       0.02\n11:      0.3333333       0.02730298                      7       0.08\n12:      0.3333333       0.06336733                      7       0.04\n13:      0.3333333       0.01919902                      4       0.06\n14:      0.3333333       0.08650077                      6       0.02\n15:      0.3333333       0.07348139                      5       0.04\n16:      0.3333333       0.08489786                      3       0.02\n17:      0.3333333       0.05025520                      6       0.06\n18:      1.0000000       0.08413701                      3       0.04\n19:      1.0000000       0.03193237                      6       0.04\n20:      1.0000000       0.07112074                      5       0.04\n21:      1.0000000       0.08489786                      3       0.04\n22:      1.0000000       0.04186187                      8       0.04\n    subsample.frac classif.rpart.cp classif.rpart.minsplit classif.ce\n```\n:::\n:::\n\n\nYou can access the best found configuration through the instance object.\n\n\n::: {.cell hash='optimization_cache/html/optimization-050_8ca25aec9d04bf351e1a83b55927fd9f'}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   classif.rpart.cp classif.rpart.minsplit subsample.frac learner_param_vals\n1:       0.07348139                      5      0.1111111          <list[6]>\n2 variables not shown: [x_domain, classif.ce]\n```\n:::\n\n```{.r .cell-code}\ninstance$result_learner_param_vals\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$subsample.frac\n[1] 0.1111111\n\n$subsample.stratify\n[1] FALSE\n\n$subsample.replace\n[1] FALSE\n\n$classif.rpart.xval\n[1] 0\n\n$classif.rpart.cp\n[1] 0.07348139\n\n$classif.rpart.minsplit\n[1] 5\n```\n:::\n\n```{.r .cell-code}\ninstance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n      0.02 \n```\n:::\n:::\n\n\nIn the traditional way, Hyperband uses uniform sampling to receive a configuration sample at the start of each bracket.\nBut it is also possible to define a custom [`Sampler`](https://paradox.mlr-org.com/reference/Sampler.html) for each hyperparameter.\n\n\n::: {.cell hash='optimization_cache/html/optimization-051_551af01e0793be4ccb1d0c337298c974'}\n\n```{.r .cell-code}\nsearch_space = ps(\n  nrounds = p_int(lower = 1, upper = 16, tags = \"budget\"),\n  eta = p_dbl(lower = 0, upper = 1),\n  booster = p_fct(levels = c(\"gbtree\", \"gblinear\", \"dart\"))\n)\n\ninstance = TuningInstanceSingleCrit$new(\n  task = tsk(\"iris\"),\n  learner = lrn(\"classif.xgboost\"),\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\n\n# beta distribution with alpha = 2 and beta = 5\n# categorical distribution with custom probabilities\nsampler = SamplerJointIndep$new(list(\n  Sampler1DRfun$new(search_space$params$eta, function(n) rbeta(n, 2, 5)),\n  Sampler1DCateg$new(search_space$params$booster, prob = c(0.2, 0.3, 0.5))\n))\n```\n:::\n\n\nThen, the defined sampler has to be given as an argument during instance creation.\nAfterwards, the usual tuning can proceed.\n\n\n::: {.cell hash='optimization_cache/html/optimization-052_be20656e36299dba3d88a3c0a7c1cc8e'}\n\n```{.r .cell-code}\ntuner = tnr(\"hyperband\", eta = 2, sampler = sampler)\nset.seed(123)\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nrounds       eta booster learner_param_vals  x_domain classif.ce\n1:       1 0.2414701    dart          <list[5]> <list[3]>       0.04\n```\n:::\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nrounds       eta booster learner_param_vals  x_domain classif.ce\n1:       1 0.2414701    dart          <list[5]> <list[3]>       0.04\n```\n:::\n:::\n\n\nFurthermore, we extended the original algorithm, to make it also possible to use [mlr3hyperband](https://mlr3hyperband.mlr-org.com) for multi-objective optimization.\nTo do this, simply specify more measures in the [`TuningInstanceMultiCrit`](https://mlr3tuning.mlr-org.com/reference/TuningInstanceMultiCrit.html) and run the rest as usual.\n\n\n::: {.cell hash='optimization_cache/html/optimization-053_4e9e0c70625b80c41cba5cea9b24106f'}\n\n```{.r .cell-code}\ninstance = TuningInstanceMultiCrit$new(\n  task = tsk(\"pima\"),\n  learner = lrn(\"classif.xgboost\"),\n  resampling = rsmp(\"holdout\"),\n  measures = msrs(c(\"classif.tpr\", \"classif.fpr\")),\n  terminator = trm(\"none\"), # hyperband terminates itself\n  search_space = search_space\n)\n\ntuner = tnr(\"hyperband\", eta = 4)\ntuner$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    nrounds       eta  booster learner_param_vals  x_domain classif.tpr\n 1:       1 0.5654654 gblinear          <list[5]> <list[3]>  0.00000000\n 2:       1 0.8296239 gblinear          <list[5]> <list[3]>  0.00000000\n 3:       1 0.3914988 gblinear          <list[5]> <list[3]>  0.00000000\n 4:       1 0.2736227 gblinear          <list[5]> <list[3]>  0.00000000\n 5:       1 0.5938669 gblinear          <list[5]> <list[3]>  0.00000000\n 6:       1 0.1601848 gblinear          <list[5]> <list[3]>  0.00000000\n 7:       1 0.8477392 gblinear          <list[5]> <list[3]>  0.00000000\n 8:       1 0.7736921 gblinear          <list[5]> <list[3]>  0.00000000\n 9:       1 0.2954001 gblinear          <list[5]> <list[3]>  0.00000000\n10:       4 0.2688034 gblinear          <list[5]> <list[3]>  0.00000000\n11:       4 0.5551772 gblinear          <list[5]> <list[3]>  0.06172840\n12:       4 0.2312309   gbtree          <list[5]> <list[3]>  0.72839506\n13:       4 0.5432329 gblinear          <list[5]> <list[3]>  0.06172840\n14:       4 0.4475701 gblinear          <list[5]> <list[3]>  0.03703704\n15:       4 0.1302881 gblinear          <list[5]> <list[3]>  0.00000000\n16:       4 0.5654654 gblinear          <list[5]> <list[3]>  0.07407407\n17:       4 0.7736921 gblinear          <list[5]> <list[3]>  0.14814815\n18:      16 0.2882662   gbtree          <list[5]> <list[3]>  0.71604938\n19:      16 0.7275336 gblinear          <list[5]> <list[3]>  0.45679012\n20:      16 0.2688034 gblinear          <list[5]> <list[3]>  0.20987654\n1 variable not shown: [classif.fpr]\n```\n:::\n:::\n\n\nNow the result is not a single best configuration but an estimated Pareto front.\nAll red points are not dominated by another parameter configuration regarding their *fpr* and *tpr* performance measures.\n\n\n::: {.cell hash='optimization_cache/html/optimization-054_0b82c34d512dc5cc58bdf294cd00f9e9'}\n\n```{.r .cell-code}\ninstance$result\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    nrounds       eta  booster learner_param_vals  x_domain classif.tpr\n 1:       1 0.5654654 gblinear          <list[5]> <list[3]>  0.00000000\n 2:       1 0.8296239 gblinear          <list[5]> <list[3]>  0.00000000\n 3:       1 0.3914988 gblinear          <list[5]> <list[3]>  0.00000000\n 4:       1 0.2736227 gblinear          <list[5]> <list[3]>  0.00000000\n 5:       1 0.5938669 gblinear          <list[5]> <list[3]>  0.00000000\n 6:       1 0.1601848 gblinear          <list[5]> <list[3]>  0.00000000\n 7:       1 0.8477392 gblinear          <list[5]> <list[3]>  0.00000000\n 8:       1 0.7736921 gblinear          <list[5]> <list[3]>  0.00000000\n 9:       1 0.2954001 gblinear          <list[5]> <list[3]>  0.00000000\n10:       4 0.2688034 gblinear          <list[5]> <list[3]>  0.00000000\n11:       4 0.5551772 gblinear          <list[5]> <list[3]>  0.06172840\n12:       4 0.2312309   gbtree          <list[5]> <list[3]>  0.72839506\n13:       4 0.5432329 gblinear          <list[5]> <list[3]>  0.06172840\n14:       4 0.4475701 gblinear          <list[5]> <list[3]>  0.03703704\n15:       4 0.1302881 gblinear          <list[5]> <list[3]>  0.00000000\n16:       4 0.5654654 gblinear          <list[5]> <list[3]>  0.07407407\n17:       4 0.7736921 gblinear          <list[5]> <list[3]>  0.14814815\n18:      16 0.2882662   gbtree          <list[5]> <list[3]>  0.71604938\n19:      16 0.7275336 gblinear          <list[5]> <list[3]>  0.45679012\n20:      16 0.2688034 gblinear          <list[5]> <list[3]>  0.20987654\n1 variable not shown: [classif.fpr]\n```\n:::\n\n```{.r .cell-code}\nplot(classif.tpr ~ classif.fpr, instance$archive$data)\npoints(classif.tpr ~ classif.fpr, instance$result, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](optimization_files/figure-html/optimization-054-1.png){width=672}\n:::\n:::\n\n\n## Feature Selection {#fs}\n\nOften, data sets include a large number of features.\nThe technique of extracting a subset of relevant features is called \"feature selection\".\n\nThe objective of feature selection is to fit the sparse dependent of a model on a subset of available data features in the most suitable manner.\nFeature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance.\nDifferent approaches exist to identify the relevant features.\nTwo different approaches are emphasized in the literature:\none is called [Filtering](#fs-filter) and the other approach is often referred to as feature subset selection or [wrapper methods](#fs-wrapper).\n\nWhat are the differences [@guyon2003;@chandrashekar2014]?\n\n* **Filtering**:\n  An external algorithm computes a rank of the features (e.g. based on the correlation to the response).\n  Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables.\n  The selected features will then be used to fit a model (with optional hyperparameters selected by tuning).\n  This calculation is usually cheaper than \"feature subset selection\" in terms of computation time.\n  All filters are connected via package [mlr3filters](https://mlr3filters.mlr-org.com).\n* **Wrapper Methods**:\n  Here, no ranking of features is done.\n  Instead, an optimization algorithm selects a subset of the features, evaluates the set by calculating the resampled predictive performance, and then\n  proposes a new set of features (or terminates).\n  A simple example is the sequential forward selection.\n  This method is usually computationally very intensive as a lot of models are fitted.\n  Also, strictly speaking, all these models would need to be tuned before the performance is estimated.\n  This would require an additional nested level in a CV setting.\n  After undertaken all of these steps, the final set of selected features is again fitted (with optional hyperparameters selected by tuning).\n  Wrapper methods are implemented in the [mlr3fselect](https://mlr3fselect.mlr-org.com) package.\n* **Embedded Methods**:\n  Many learners internally select a subset of the features which they find helpful for prediction.\n  These subsets can usually be queried, as the following example demonstrates:\n\n  ::: {.cell hash='optimization_cache/html/optimization-055_f6a171ffee36f0303a659e24a8ea0aeb'}\n  \n  ```{.r .cell-code}\n  library(\"mlr3verse\")\n  \n  task = tsk(\"iris\")\n  learner = lrn(\"classif.rpart\")\n  \n  # ensure that the learner selects features\n  stopifnot(\"selected_features\" %in% learner$properties)\n  \n  # fit a simple classification tree\n  learner = learner$train(task)\n  \n  # extract all features used in the classification tree:\n  learner$selected_features()\n  ```\n  \n  ::: {.cell-output .cell-output-stdout}\n  ```\n  [1] \"Petal.Length\" \"Petal.Width\" \n  ```\n  :::\n  :::\n\n\nThere are also ensemble filters built upon the idea of stacking single filter methods. These are not yet implemented.\n\n### Filters {#fs-filter}\n\nFilter methods assign an importance value to each feature.\nBased on these values the features can be ranked.\nThereafter, we are able to select a feature subset.\nThere is a list of all implemented filter methods in the [Appendix](#appendix).\n\n#### Calculating filter values {#fs-calc}\n\nCurrently, only classification and regression tasks are supported.\n\nThe first step it to create a new R object using the class of the desired filter method.\nSimilar to other instances in [mlr3](https://mlr3.mlr-org.com), these are registered in a dictionary ([`mlr_filters`](https://mlr3filters.mlr-org.com/reference/mlr_filters.html)) with an associated shortcut function [`flt()`](https://mlr3filters.mlr-org.com/reference/flt.html).\nEach object of class `Filter` has a `.$calculate()` method which computes the filter values and ranks them in a descending order.\n\n\n::: {.cell hash='optimization_cache/html/optimization-056_7caef85b51019001087efe81b3b3fab9'}\n\n```{.r .cell-code}\nfilter = flt(\"jmim\")\n\ntask = tsk(\"iris\")\nfilter$calculate(task)\n\nas.data.table(filter)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        feature     score\n1:  Petal.Width 1.0000000\n2: Sepal.Length 0.6666667\n3: Petal.Length 0.3333333\n4:  Sepal.Width 0.0000000\n```\n:::\n:::\n\n\nSome filters support changing specific hyperparameters.\nThis is similar to setting hyperparameters of a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) using `.$param_set$values`:\n\n\n::: {.cell hash='optimization_cache/html/optimization-057_1e715f197be497390418418fd1fe054e'}\n\n```{.r .cell-code}\nfilter_cor = flt(\"correlation\")\nfilter_cor$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels    default value\n1:    use ParamFct    NA    NA       5 everything      \n2: method ParamFct    NA    NA       3    pearson      \n```\n:::\n\n```{.r .cell-code}\n# change parameter 'method'\nfilter_cor$param_set$values = list(method = \"spearman\")\nfilter_cor$param_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet>\n       id    class lower upper nlevels    default    value\n1:    use ParamFct    NA    NA       5 everything         \n2: method ParamFct    NA    NA       3    pearson spearman\n```\n:::\n:::\n\n\n#### Variable Importance Filters {#fs-var-imp-filters}\n\nAll [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) with the property \"importance\" come with integrated feature selection methods.\n\nYou can find a list of all learners with this property in the [Appendix](#fs-filter-embedded-list).\n\nFor some learners the desired filter method needs to be set during learner creation.\nFor example, learner [`classif.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html) comes with multiple integrated methods, c.f. the help page of [`ranger::ranger()`](https://www.rdocumentation.org/packages/ranger/topics/ranger).\nTo use method \"impurity\", you need to set the filter method during construction.\n\n\n::: {.cell hash='optimization_cache/html/optimization-058_f2d3ecd56f79a34901f46d9d123eed07'}\n\n```{.r .cell-code}\nlrn = lrn(\"classif.ranger\", importance = \"impurity\")\n```\n:::\n\n\nNow you can use the [`FilterImportance`](https://mlr3filters.mlr-org.com/reference/mlr_filters_importance.html) filter class for algorithm-embedded methods:\n\n\n::: {.cell hash='optimization_cache/html/optimization-059_3a5ae1234d7cde2c5a0d0ea748bff13f'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\")\nfilter = flt(\"importance\", learner = lrn)\nfilter$calculate(task)\nhead(as.data.table(filter), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        feature     score\n1:  Petal.Width 43.967169\n2: Petal.Length 43.485892\n3: Sepal.Length  9.664467\n```\n:::\n:::\n\n\n### Wrapper Methods {#fs-wrapper}\n\nWrapper feature selection is supported via the [mlr3fselect](https://mlr3fselect.mlr-org.com) extension package.\nAt the heart of [mlr3fselect](https://mlr3fselect.mlr-org.com) are the R6 classes:\n\n* [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html), `\"r ref(FSelectInstanceMultiCrit\")`: These two classes describe the feature selection problem and store the results.\n* [`FSelector`](https://mlr3fselect.mlr-org.com/reference/FSelector.html): This class is the base class for implementations of feature selection algorithms.\n\n#### The `FSelectInstance` Classes {#fs-wrapper-optimization}\n\nThe following sub-section examines the feature selection on the [`Pima`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html) data set which is used to predict whether or not a patient has diabetes.\n\n\n::: {.cell hash='optimization_cache/html/optimization-060_9edc4ddef3a1227610a45f580edca1ea'}\n\n```{.r .cell-code}\ntask = tsk(\"pima\")\nprint(task)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<TaskClassif:pima> (768 x 9): Pima Indian Diabetes\n* Target: diabetes\n* Properties: twoclass\n* Features (8):\n  - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure,\n    triceps\n```\n:::\n:::\n\n\nWe use the classification tree from [rpart](https://cran.r-project.org/package=rpart).\n\n\n::: {.cell hash='optimization_cache/html/optimization-061_f09d675e4e4315e838bad5fb86f78410'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\n```\n:::\n\n\nNext, we need to specify how to evaluate the performance of the feature subsets.\nFor this, we need to choose a [`resampling strategy`](https://mlr3.mlr-org.com/reference/Resampling.html) and a [`performance measure`](https://mlr3.mlr-org.com/reference/Measure.html).\n\n\n::: {.cell hash='optimization_cache/html/optimization-062_c7168b9d83112c3952caa650235b1e20'}\n\n```{.r .cell-code}\nhout = rsmp(\"holdout\")\nmeasure = msr(\"classif.ce\")\n```\n:::\n\n\nFinally, one has to choose the available budget for the feature selection.\nThis is done by selecting one of the available [`Terminators`](https://bbotk.mlr-org.com/reference/Terminator.html):\n\n* Terminate after a given time ([`TerminatorClockTime`](https://bbotk.mlr-org.com/reference/mlr_terminators_clock_time.html))\n* Terminate after a given amount of iterations ([`TerminatorEvals`](https://bbotk.mlr-org.com/reference/mlr_terminators_evals.html))\n* Terminate after a specific performance is reached ([`TerminatorPerfReached`](https://bbotk.mlr-org.com/reference/mlr_terminators_perf_reached.html))\n* Terminate when feature selection does not improve ([`TerminatorStagnation`](https://bbotk.mlr-org.com/reference/mlr_terminators_stagnation.html))\n* A combination of the above in an *ALL* or *ANY* fashion ([`TerminatorCombo`](https://bbotk.mlr-org.com/reference/mlr_terminators_combo.html))\n\nFor this short introduction, we specify a budget of 20 evaluations and then put everything together into a [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html):\n\n\n::: {.cell hash='optimization_cache/html/optimization-063_eae47ce003bb9e557bde341aade47cd7'}\n\n```{.r .cell-code}\nevals20 = trm(\"evals\", n_evals = 20)\n\ninstance = FSelectInstanceSingleCrit$new(\n  task = task,\n  learner = learner,\n  resampling = hout,\n  measure = measure,\n  terminator = evals20\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<FSelectInstanceSingleCrit>\n* State:  Not optimized\n* Objective: <ObjectiveFSelect:classif.rpart_on_pima>\n* Search Space:\n         id    class lower upper nlevels\n1:      age ParamLgl    NA    NA       2\n2:  glucose ParamLgl    NA    NA       2\n3:  insulin ParamLgl    NA    NA       2\n4:     mass ParamLgl    NA    NA       2\n5: pedigree ParamLgl    NA    NA       2\n6: pregnant ParamLgl    NA    NA       2\n7: pressure ParamLgl    NA    NA       2\n8:  triceps ParamLgl    NA    NA       2\n* Terminator: <TerminatorEvals>\n```\n:::\n:::\n\n\nTo start the feature selection, we still need to select an algorithm which are defined via the [`FSelector`](https://mlr3fselect.mlr-org.com/reference/FSelector.html) class\n\n#### The `FSelector` Class\n\nThe following algorithms are currently implemented in [mlr3fselect](https://mlr3fselect.mlr-org.com):\n\n* Random Search ([`FSelectorRandomSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_random_search.html))\n* Exhaustive Search ([`FSelectorExhaustiveSearch`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_exhaustive_search.html))\n* Sequential Search ([`FSelectorSequential`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_sequential.html))\n* Recursive Feature Elimination ([`FSelectorRFE`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_rfe.html))\n* Design Points ([`FSelectorDesignPoints`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors_design_points.html))\n\nIn this example, we will use a simple random search and retrieve it from the dictionary [`mlr_fselectors`](https://mlr3fselect.mlr-org.com/reference/mlr_fselectors.html) with the [`fs()`](https://mlr3fselect.mlr-org.com/reference/fs.html) function:\n\n\n::: {.cell hash='optimization_cache/html/optimization-064_5da25a6b88035c641ceb4177fecfb4f4'}\n\n```{.r .cell-code}\nfselector = fs(\"random_search\")\n```\n:::\n\n\n#### Triggering the Tuning {#wrapper-selection-triggering}\n\nTo start the feature selection, we simply pass the [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html) to the `$optimize()` method of the initialized [`FSelector`](https://mlr3fselect.mlr-org.com/reference/FSelector.html).\nThe algorithm proceeds as follows\n\n1. The [`FSelector`](https://mlr3fselect.mlr-org.com/reference/FSelector.html) proposes at least one feature subset and may propose multiple subsets to improve parallelization, which can be controlled via the setting `batch_size`).\n1. For each feature subset, the given [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) is fitted on the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) using the provided [`Resampling`](https://mlr3.mlr-org.com/reference/Resampling.html).\n1  All evaluations are stored in the archive of the [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html).\n1. The [`Terminator`](https://bbotk.mlr-org.com/reference/Terminator.html) is queried if the budget is exhausted.\n1  If the budget is not exhausted, restart with 1) until it is.\n1. Determine the feature subset with the best observed performance.\n1. Store the best feature subset as the result in the instance object.\nThe best feature subset (`$result_feature_set`) and the corresponding measured performance (`$result_y`) can be accessed from the instance.\n\n\n::: {.cell hash='optimization_cache/html/optimization-065_d97d0fc256db55b81445e0b2b29569fb'}\n\n```{.r .cell-code}\n# reduce logging output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfselector$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age glucose insulin mass pedigree pregnant pressure triceps\n1: TRUE    TRUE    TRUE TRUE    FALSE    FALSE     TRUE   FALSE\n2 variables not shown: [features, classif.ce]\n```\n:::\n\n```{.r .cell-code}\ninstance$result_feature_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n```\n:::\n\n```{.r .cell-code}\ninstance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclassif.ce \n 0.1953125 \n```\n:::\n:::\n\n\nOne can investigate all resamplings which were undertaken, as they are stored in the archive of the [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html) and can be accessed by using [`as.data.table()`](https://www.rdocumentation.org/packages/data.table/topics/as.data.table):\n\n\n::: {.cell hash='optimization_cache/html/optimization-066_547f5f9f557c120f247d0e64d091d263'}\n\n```{.r .cell-code}\nas.data.table(instance$archive)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age glucose insulin  mass pedigree pregnant pressure triceps classif.ce\n 1: FALSE   FALSE   FALSE FALSE    FALSE     TRUE     TRUE    TRUE  0.3281250\n 2: FALSE   FALSE    TRUE  TRUE    FALSE    FALSE     TRUE    TRUE  0.3242188\n 3:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE    FALSE   FALSE  0.2187500\n 4: FALSE   FALSE    TRUE FALSE    FALSE    FALSE    FALSE    TRUE  0.3710938\n 5: FALSE    TRUE    TRUE  TRUE    FALSE     TRUE    FALSE   FALSE  0.2226562\n 6:  TRUE   FALSE   FALSE FALSE    FALSE    FALSE    FALSE    TRUE  0.3085938\n 7:  TRUE   FALSE    TRUE  TRUE    FALSE     TRUE     TRUE   FALSE  0.2578125\n 8:  TRUE   FALSE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE  0.2851562\n 9: FALSE   FALSE   FALSE FALSE     TRUE    FALSE    FALSE   FALSE  0.3671875\n10:  TRUE    TRUE    TRUE FALSE     TRUE    FALSE    FALSE    TRUE  0.2851562\n11:  TRUE    TRUE   FALSE FALSE     TRUE     TRUE     TRUE    TRUE  0.2500000\n12:  TRUE    TRUE   FALSE FALSE     TRUE    FALSE    FALSE   FALSE  0.2539062\n13:  TRUE    TRUE    TRUE  TRUE     TRUE     TRUE    FALSE    TRUE  0.2343750\n14:  TRUE   FALSE    TRUE  TRUE     TRUE     TRUE     TRUE    TRUE  0.2851562\n15:  TRUE   FALSE    TRUE FALSE    FALSE     TRUE    FALSE    TRUE  0.2851562\n16:  TRUE   FALSE   FALSE FALSE    FALSE    FALSE     TRUE    TRUE  0.3710938\n17:  TRUE    TRUE   FALSE FALSE    FALSE     TRUE    FALSE    TRUE  0.2382812\n18:  TRUE    TRUE    TRUE  TRUE    FALSE    FALSE     TRUE   FALSE  0.1953125\n19: FALSE   FALSE   FALSE FALSE    FALSE    FALSE    FALSE    TRUE  0.3750000\n20:  TRUE    TRUE    TRUE FALSE     TRUE     TRUE     TRUE    TRUE  0.2460938\n4 variables not shown: [runtime_learners, timestamp, batch_nr, resample_result]\n```\n:::\n:::\n\n\nThe associated resampling iterations can be accessed in the [`BenchmarkResult`](https://mlr3.mlr-org.com/reference/BenchmarkResult.html):\n\n\n::: {.cell hash='optimization_cache/html/optimization-067_9862a7c8e4cb5f3980066334964c3dc1'}\n\n```{.r .cell-code}\ninstance$archive$benchmark_result$data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNULL\n```\n:::\n:::\n\n\nThe `uhash` column links the resampling iterations to the evaluated feature subsets stored in `instance$archive$data()`.\nThis allows e.g. to score the included [`ResampleResult`](https://mlr3.mlr-org.com/reference/ResampleResult.html)s on a different measure.\n\nNow the optimized feature subset can be used to subset the task and fit the model on all observations.\n\n\n::: {.cell hash='optimization_cache/html/optimization-068_6931778efe371d05cd81fca362d3e258'}\n\n```{.r .cell-code}\ntask$select(instance$result_feature_set)\nlearner$train(task)\n```\n:::\n\n\nThe trained model can now be used to make a prediction on external data.\nNote that predicting on observations present in the [`Task`](https://mlr3.mlr-org.com/reference/Task.html), should be avoided.\nThe model has seen these observations already during feature selection and therefore results would be statistically biased.\nHence, the resulting performance measure would be over-optimistic.\nInstead, to get statistically unbiased performance estimates for the current task, [nested resampling](#nested-resampling) is required.\n\n#### Filtering with Multiple Performance Measures {#mult-measures-filtering}\n\nWhen filtering, you might want to use multiple criteria to evaluate the performance of the feature subsets. For example, you might want the subset with the lowest classification error and lowest time to train the model. The full list of performance measures can be found [here](https://mlr3.mlr-org.com/reference/mlr_measures.html).\n\nWe will expand the previous example and perform feature selection on the same dataset, [`Pima Indian Diabetes`](https://mlr3.mlr-org.com/reference/mlr_tasks_pima.html), however, this time we will use [`FSelectInstanceMultiCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceMultiCrit.html) to select the subset of features that has the lowest classification error and the lowest time to train the model.\n\nThe filtering process with multiple criteria is very similar to filtering with a single criterion.\n\n\n::: {.cell hash='optimization_cache/html/optimization-069_d8f9671e1646e783d2c844b7c9e07623'}\n\n```{.r .cell-code}\nmeasures = msrs(c(\"classif.ce\", \"time_train\"))\n```\n:::\n\n\nInstead of creating a new [`FSelectInstanceSingleCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceSingleCrit.html) with a single measure, we create a new [`FSelectInstanceMultiCrit`](https://mlr3fselect.mlr-org.com/reference/FSelectInstanceMultiCrit.html) with the two measures we are interested in here.\nOtherwise, it is the same as above.\n\n\n::: {.cell hash='optimization_cache/html/optimization-070_6ddc1bbc18233b124053a0eb54d2fe7b'}\n\n```{.r .cell-code}\nlibrary(\"mlr3filters\")\n\nevals20 = trm(\"evals\", n_evals = 20)\ninstance = FSelectInstanceMultiCrit$new(\ntask = task,\nlearner = learner,\nresampling = hout,\nmeasures = measures,\nterminator = evals20\n)\ninstance\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<FSelectInstanceMultiCrit>\n* State:  Not optimized\n* Objective: <ObjectiveFSelect:classif.rpart_on_pima>\n* Search Space:\n         id    class lower upper nlevels\n1:      age ParamLgl    NA    NA       2\n2:  glucose ParamLgl    NA    NA       2\n3:  insulin ParamLgl    NA    NA       2\n4:     mass ParamLgl    NA    NA       2\n5: pressure ParamLgl    NA    NA       2\n* Terminator: <TerminatorEvals>\n```\n:::\n:::\n\n\nAfter triggering the filtering, we will have the subset of features with the best classification error and time to train the model.\n\n\n::: {.cell hash='optimization_cache/html/optimization-071_3eab25c2459614dd6f9c3f848bbb4271'}\n\n```{.r .cell-code}\n# reduce logging output\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\nfselector$optimize(instance)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    age glucose insulin mass pressure                          features\n1: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n2: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n3: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n4: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n5: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n6: TRUE    TRUE   FALSE TRUE     TRUE         age,glucose,mass,pressure\n7: TRUE    TRUE    TRUE TRUE     TRUE age,glucose,insulin,mass,pressure\n2 variables not shown: [classif.ce, time_train]\n```\n:::\n:::\n\n::: {.cell hash='optimization_cache/html/optimization-072_8afe63e8a2731a9fb2601b041bd189d2'}\n\n```{.r .cell-code}\ninstance$result_feature_set\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n\n[[2]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n\n[[3]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n\n[[4]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n\n[[5]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n\n[[6]]\n[1] \"age\"      \"glucose\"  \"mass\"     \"pressure\"\n\n[[7]]\n[1] \"age\"      \"glucose\"  \"insulin\"  \"mass\"     \"pressure\"\n```\n:::\n:::\n\n::: {.cell hash='optimization_cache/html/optimization-073_3d83f6cc91f77f530ae20a83dc17574e'}\n\n```{.r .cell-code}\ninstance$result_y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   classif.ce time_train\n1:       0.25          0\n2:       0.25          0\n3:       0.25          0\n4:       0.25          0\n5:       0.25          0\n6:       0.25          0\n7:       0.25          0\n```\n:::\n:::\n\n\n#### Automating the Feature Selection {#autofselect}\n\nThe [`AutoFSelector`](https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html) wraps a learner and augments it with an automatic feature selection for a given task.\nBecause the [`AutoFSelector`](https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html) itself inherits from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) base class, it can be used like any other learner.\nAnalogously to the previous subsection, a new classification tree learner is created.\nThis classification tree learner automatically starts a feature selection on the given task using an inner resampling (holdout).\nWe create a terminator which allows 10 evaluations, and uses a simple random search as feature selection algorithm:\n\n\n::: {.cell hash='optimization_cache/html/optimization-074_3380f6f9857501e51e3e985a3b17cee8'}\n\n```{.r .cell-code}\nlearner = lrn(\"classif.rpart\")\nterminator = trm(\"evals\", n_evals = 10)\nfselector = fs(\"random_search\")\n\nat = AutoFSelector$new(\n  learner = learner,\n  resampling = rsmp(\"holdout\"),\n  measure = msr(\"classif.ce\"),\n  terminator = terminator,\n  fselector = fselector\n)\nat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<AutoFSelector:classif.rpart.fselector>\n* Model: -\n* Parameters: xval=0\n* Packages: mlr3, mlr3fselect, rpart\n* Predict Types:  [response], prob\n* Feature Types: logical, integer, numeric, factor, ordered\n* Properties: importance, missings, multiclass, selected_features,\n  twoclass, weights\n```\n:::\n:::\n\n\nWe can now use the learner like any other learner, calling the `$train()` and `$predict()` method.\nThis time however, we pass it to [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html) to compare the optimized feature subset to the complete feature set.\nThis way, the [`AutoFSelector`](https://mlr3fselect.mlr-org.com/reference/AutoFSelector.html) will do its resampling for feature selection on the training set of the respective split of the outer resampling.\nThe learner then undertakes predictions using the test set of the outer resampling.\nThis yields unbiased performance measures, as the observations in the test set have not been used during feature selection or fitting of the respective learner.\nThis is called [nested resampling](#nested-resampling).\n\nTo compare the optimized feature subset with the complete feature set, we can use [`benchmark()`](https://mlr3.mlr-org.com/reference/benchmark.html):\n\n\n::: {.cell hash='optimization_cache/html/optimization-075_4cff8bcf3558c990b7637fa9a4a3e702'}\n\n```{.r .cell-code}\ngrid = benchmark_grid(\n  task = tsk(\"pima\"),\n  learner = list(at, lrn(\"classif.rpart\")),\n  resampling = rsmp(\"cv\", folds = 3)\n)\n\nbmr = benchmark(grid, store_models = TRUE)\nbmr$aggregate(msrs(c(\"classif.ce\", \"time_train\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   nr      resample_result task_id              learner_id resampling_id iters\n1:  1 <ResampleResult[21]>    pima classif.rpart.fselector            cv     3\n2:  2 <ResampleResult[21]>    pima           classif.rpart            cv     3\n2 variables not shown: [classif.ce, time_train]\n```\n:::\n:::\n\n\nNote that we do not expect any significant differences since we only evaluated a small fraction of the possible feature subsets.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}