{
  "hash": "b3da1ab9b6f994d5a2817143d55f1b4c",
  "result": {
    "markdown": "# Extending {#extending }\n\n\n::: {.cell}\n\n:::\n\n\n\n\nThis chapter gives instructions on how to extend [mlr3](https://mlr3.mlr-org.com) and its extension packages with custom objects.\n\nThe approach is always the same:\n\n1. determine the base class you want to inherit from,\n1. extend the class with your custom functionality,\n1. test your implementation\n1. (optionally) add new object to the respective [`Dictionary`](https://mlr3misc.mlr-org.com/reference/Dictionary.html).\n\nThe chapter [Create a new learner](#extending-learners) illustrates the steps needed to create a custom learner in [mlr3](https://mlr3.mlr-org.com).\n\n## Adding new Learners {#sec-extending-learners}\n\n\n\n\n\n\n\nAlthough many learners are already included in the [mlr3](https://mlr3.mlr-org.com) ecosystem, there might be a situation in which your algorithm of choice is not implemented.\nHere, we show how to create a custom mlr3learner step-by-step using [`mlr3extralearners::create_learner`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html).\nIf you intend to add a learner to mlr3extralearners, **it is strongly recommended** to **first** open a [learner request issue](https://github.com/mlr-org/mlr3extralearners/issues/new?assignees=&labels=new+learner&template=learner-request-template.md&title=%5BLRNRQ%5D+Add+%3Calgorithm%3E+from+package+%3Cpackage%3E) to inform the [mlr3](https://mlr3.mlr-org.com) team about your idea.\nThis allows to discuss the implementation details and potential peculiarities of the learner before putting actual work in.\n\nThis section gives insights on how a mlr3learner is constructed and how to troubleshoot issues.\nSee the [Learner FAQ subsection](#learner-faq) for help.\n\n**Summary of steps for adding a new learner**\n\n1. Check that the learner does not already exist [here](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html).\n1. [Install](#setup) [mlr3extralearners](https://mlr3extralearners.mlr-org.com) and if you want to create a PR, also [fork and clone](#setup) it.\n1. [Run](#create-learner) [`mlr3extralearners::create_learner()`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html).\n1. Add the learner's [`ParamSet`](#param-set).\n1. Manually add [`.train`](#learner-train) and [`.predict`](#learner-predict) private methods to the learner.\n1. Implement supported [optional extractors](#optional-extractors) and [hotstarting](#hotstarting) if applicable.\n1. Fill out the missing parts in the learner's description.\n   To add references you first need to create an entry in bibentries.R\n1. Check that [unit tests](#learner-test) and [parameter tests](#learner-test) pass (these are automatically created).\n1. Run [cleaning functions](#cleaning).\n1. Open a [pull request](https://github.com/mlr-org/mlr3extralearners/pulls) with the \"new learner\" template.\n\n:::{.callout-warning}\nDo not copy/paste the code shown in this section.\nUse [`create_learner()`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html) to start.\n:::\n\n### Setting-up mlr3extralearners {#setup}\n\nIn order to use [`mlr3extralearners::create_learner`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html) you must have mlr3extralearners installed.\nNote that mlr3extralearners is not on CRAN and has to be installed from GitHub.\nTo ensure that you have the latest version, run `remotes::install_github(\"mlr-org/mlr3extralearners\")` before proceeding.\n\nIf you want to create a pull request to mlr3extralearners you also need to\n\n1. [Fork](https://docs.github.com/en/free-pro-team@latest/github/getting-started-with-github/fork-a-repo) the [repository](https://github.com/mlr-org/mlr3extralearners)\n2. [Clone](https://docs.github.com/en/free-pro-team@latest/github/creating-cloning-and-archiving-repositories/cloning-a-repository) a local copy of your forked repository.\n\n### Calling create_learner {#create-learner}\n\nThe learner `classif.rpart` will be used as a running example throughout this section.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-002_b2a2777525b6673ee9004c0f55a92040'}\n\n```{.r .cell-code}\nlibrary(\"mlr3extralearners\")\ncreate_learner(\n  path = \"path/to/a/folder\",\n  classname = \"Rpart\",\n  type = \"classif\",\n  key = \"rpart\",\n  algorithm = \"Decision Tree\",\n  package = \"rpart\",\n  caller = \"rpart\",\n  feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n  predict_types = c(\"response\", \"prob\"),\n  properties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\"),\n  gh_name = \"RaphaelS1\",\n  label = \"Regression and Partition Tree\",\n  data_formats = \"data.table\"\n)\n```\n:::\n\n\n\nThe full documentation for the function arguments is in [`mlr3extralearners::create_learner`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html), in this example we are doing the following:\n\n1. `path = \"path/to/a/folder\"` - This determines where the templates are being generated.\nIf the path is the root of an R package, the learner is created in the ./R directory and the test files in ./tests/testthat.\nOtherwise, all files are being created in the folder pointed to by path.\nAlready existing files will not be modified.\n1. `classname = \"Rpart\"` - Set the R6 class name to LearnerClassifRpart (classif is below)\n1. `algorithm = \"Decision Tree\"` - Create the title as \"Classification Decision Tree Learner\", where \"Classification\" is determined automatically from `type` and \"Learner\" is added for all learners.\n1. `type = \"classif\"` - Setting the learner as a classification learner, automatically filling the title, class name, id (`\"classif.rpart\"`) and task type.\n1. `key = \"rpart\"` - Used with `type` to create the unique ID of the learner, `\"classif.rpart\"`.\n1. `package = \"rpart\"` - Setting the package from which the learner is implemented, this fills in things like the training function (along with `caller`) and the `man` field.\n1. `caller = \"rpart\"` - This tells the `.train` function, and the description which function is called to run the algorithm, with `package` this automatically fills `rpart::rpart`.\n1. `feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\")` - Sets the type of features that can be handled by the learner. See [meta information](#learner-meta-information).\n1. `predict_types = c(\"response\", \"prob\"),` - Sets the available prediction types as response (pointwise prediction) and prob (probabilities). See [meta information](#learner-meta-information).\n1. `properties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\")` - Sets the properties the learner supports.\nBy including `\"importance\"` a public method called `importance` will be created that must be manually filled.\nSee [meta information](#learner-meta-information) and [optional extractors](#optional-extractors).\n1. `gh_name = \"RaphaelS1\"` - Fills the '\\@author' tag with my GitHub handle, this is required as it identifies the maintainer of the learner.\n\nThe sections below show an exemplary execution of [`mlr3extralearners::create_learner`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html).\n\n### `learner_package_type_key.R`\n\nThe first generated file which must be updated after running [`create_learner()`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html) is the one following the structure `learner_<package>_<type>_<key>.R`; in this example `learner_rpart_classif_rpart.R`.\n\nFor our example, the resulting script looks like this:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-003_9dbe5b0b40363dfcd832c609270479e8'}\n\n```{.r .cell-code}\n#' @title Classification Decision Tree Learner\n#' @author RaphaelS1\n#' @name mlr_learners_classif.rpart\n#'\n#' @description\n#' FIXME: BRIEF DESCRIPTION OF THE LEARNER.\n#' Calls [rpart::rpart()] from FIXME: (CRAN VS NO CRAN): \\CRANpkg{rpart} | 'rpart'.\n#'\n#' @section Custom mlr3 parameters:\n#' FIXME: DEVIATIONS FROM UPSTREAM PARAMETERS. DELETE IF NOT APPLICABLE.\n#'\n#' @section Custom mlr3 defaults:\n#' FIXME: DEVIATIONS FROM UPSTREAM DEFAULTS. DELETE IF NOT APPLICABLE.\n#'\n#' @section Installation:\n#' FIXME: CUSTOM INSTALLATION INSTRUCTIONS. DELETE IF NOT APPLICABLE.\n#'\n#' @templateVar id classif.rpart\n#' @template learner\n#'\n#' @references\n#' `r format_bib(FIXME: ONE OR MORE REFERENCES FROM bibentries.R)`\n#'\n#' @template seealso_learner\n#' @template example\n#' @export\nLearnerClassifRpart = R6Class(\"LearnerClassifRpart\",\n  inherit = LearnerClassif,\n  public = list(\n    #' @description\n    #' Creates a new instance of this [R6][R6::R6Class] class.\n    initialize = function() {\n      # FIXME: MANUALLY ADD PARAMETERS BELOW AND THEN DELETE THIS LINE\n      param_set = ps()\n\n      # FIXME: MANUALLY UPDATE PARAM VALUES BELOW IF APPLICABLE THEN DELETE THIS LINE.\n      param_set$values = list()\n\n      super$initialize(\n        id = \"classif.rpart\",\n        packages = \"rpart\",\n        feature_types = c(\"logical\", \"integer\", \"numeric\", \"factor\", \"ordered\"),\n        predict_types = c(\"response\", \"prob\"),\n        param_set = param_set,\n        properties = c(\"importance\", \"missings\", \"multiclass\", \"twoclass\", \"weights\"),\n        man = \"mlr3extralearners::mlr_learners_classif.rpart\",\n        label = \"Regression and Partition Tree\"\n      )\n    },\n    # FIXME: ADD IMPORTANCE METHOD IF APPLICABLE AND DELETE OTHERWISE\n    # SEE mlr3extralearners::LearnerRegrRandomForest FOR AN EXAMPLE\n    #' @description\n    #' The importance scores are extracted from the slot FIXME:.\n    #' @return Named `numeric()`.\n    importance = function() {\n      pars = self$param_set$get_values(tags = \"importance\")\n      # FIXME: Implement importance\n    }\n  ),\n  private = list(\n    .train = function(task) {\n      # get parameters for training\n      pars = self$param_set$get_values(tags = \"train\")\n\n      # FIXME: IF LEARNER DOES NOT HAVE 'weights' PROPERTY THEN DELETE THESE LINES.\n      if (\"weights\" %in% task$properties) {\n        # Add weights to learner\n      }\n\n      # FIXME: CREATE OBJECTS FOR THE TRAIN CALL\n      # AT LEAST \"data\" AND \"formula\" ARE REQUIRED\n      formula = task$formula()\n      data = task$data()\n\n      # FIXME: HERE IS SPACE FOR SOME CUSTOM ADJUSTMENTS BEFORE PROCEEDING TO THE\n      # TRAIN CALL. CHECK OTHER LEARNERS FOR WHAT CAN BE DONE HERE\n      # USE THE mlr3misc::invoke FUNCTION (IT'S SIMILAR TO do.call())\n\n      invoke(\n        rpart::rpart,\n        formula = formula,\n        data = data,\n        .args = pars\n      )\n    },\n    .predict = function(task) {\n      # get parameters with tag \"predict\"\n      pars = self$param_set$get_values(tags = \"predict\")\n\n      # get newdata and ensure same ordering in train and predict\n      newdata = ordered_features(task, self)\n\n      # Calculate predictions for the selected predict type.\n      type = self$predict_type\n\n      pred = invoke(predict, self$model, newdata = newdata, type = type, .args = pars)\n\n      # FIXME: ADD PREDICTIONS TO LIST BELOW\n      list()\n    }\n  )\n)\n\n.extralrns_dict$add(\"classif.rpart\", LearnerClassifRpart)\n```\n:::\n\n\n\nNow we have to do the following (the description will be addressed later):\n\n1. Add the learner's parameters to the [ParamSet](#param-set).\n1. Optionally [change default values](#param-set) for the parameters.\n1. Fill in the private [`.train`](#learner-train) method, which takes a (filtered) [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and returns a model.\n1. Fill in the private [`.predict`](#learner-predict) method, which operates on the model in `self$model` (stored during `$train()`) and a (differently subsetted) [`Task`](https://mlr3.mlr-org.com/reference/Task.html) to return a named list of predictions.\n1. As we included \"importance\" in `properties`, we have to add the public method `importance()` which returns a named numeric vectors with the decreasingly sorted importance scores (values) for the different features (names).\n\n### Meta-information {#learner-meta-information}\n\nIn the constructor (`initialize()`) the constructor of the super class (e.g. [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html)) is called with meta information about the learner which should be constructed.\nThis includes:\n\n* `id`: The ID of the new learner. Usually consists of `<type>.<key>`, for example: `\"classif.rpart\"`.\n* `packages`: The upstream package name(s) of the implemented learner.\n* `param_set`: A set of hyperparameters and their descriptions provided as a [`paradox::ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\n  For each hyperparameter the appropriate class needs to be chosen. When using the [`paradox::ps`](https://paradox.mlr-org.com/reference/ps.html) shortcut, a short constructor of the form `p_***` can be used:\n  * [`paradox::ParamLgl`](https://paradox.mlr-org.com/reference/ParamLgl.html) / [`paradox::p_lgl`](https://paradox.mlr-org.com/reference/Domain.html) for scalar logical hyperparameters.\n  * [`paradox::ParamInt`](https://paradox.mlr-org.com/reference/ParamInt.html) / [`paradox::p_int`](https://paradox.mlr-org.com/reference/Domain.html) for scalar integer hyperparameters.\n  * [`paradox::ParamDbl`](https://paradox.mlr-org.com/reference/ParamDbl.html) / [`paradox::p_dbl`](https://paradox.mlr-org.com/reference/Domain.html) for scalar numeric hyperparameters.\n  * [`paradox::ParamFct`](https://paradox.mlr-org.com/reference/ParamFct.html) / [`paradox::p_fct`](https://paradox.mlr-org.com/reference/Domain.html) for scalar factor hyperparameters (this includes characters).\n  * [`paradox::ParamUty`](https://paradox.mlr-org.com/reference/ParamUty.html) / [`paradox::p_uty`](https://paradox.mlr-org.com/reference/Domain.html) for everything else (e.g. vector paramters or list parameters).\n* `predict_types`: Set of predict types the learner supports.\n  These differ depending on the type of the learner. See [`mlr_reflections$learner_predict_types`](https://mlr3.mlr-org.com/reference/mlr_reflections.html) for the full list of predict types supported by [mlr3](https://mlr3.mlr-org.com).\n  * [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html)\n    * `response`: Only predicts a class label for each observation.\n    * `prob`: Also predicts the posterior probability for each class for each observation.\n  * [`LearnerRegr`](https://mlr3.mlr-org.com/reference/LearnerRegr.html)\n    * `response`: Only predicts a numeric response for each observation.\n    * `se`: Also predicts the standard error for each value of response.\n  * [`LearnerSurv`](https://mlr3proba.mlr-org.com/reference/LearnerSurv.html)\n    * `lp` - Linear predictor calculated as the fitted coefficients multiplied by the test data.\n    * `distr` - Predicted survival distribution, either discrete or continuous.\n      Implemented in [distr6](https://cran.r-project.org/package=distr6).\n    * `crank` - Continuous risk ranking.\n    * `response` - Predicted survival time.\n  * [`LearnerDens`](https://mlr3proba.mlr-org.com/reference/LearnerDens.html)\n    * `pdf`- Predicts the probability density function.\n    * `cdf` - Predicts the cumulative distribution function.\n    * `distr` - Predicts a distribution as implemented in [distr6](https://cran.r-project.org/package=distr6).\n  * [`LearnerClust`](https://mlr3cluster.mlr-org.com/reference/LearnerClust.html)\n    * `partition` - Assigns the observation to a partition.\n    * `prob` - Returns a probability for each partition.\n* `feature_types`: Set of feature types the learner is able to handle.\n  See [`mlr_reflections$task_feature_types`](https://mlr3.mlr-org.com/reference/mlr_reflections.html) for feature types supported by [mlr3](https://mlr3.mlr-org.com).\n* `properties`: Set of properties of the learner. See [`mlr_reflections$learner_properties`](https://mlr3.mlr-org.com/reference/mlr_reflections.html) for the full list of learner properties supported by [mlr3](https://mlr3.mlr-org.com).\n  The list of properties includes:\n  * `\"twoclass\"`: The learner works on binary classification problems.\n  * `\"multiclass\"`: The learner works on multi-class classification problems.\n  * `\"missings\"`: The learner can natively handle missing values.\n  * `\"weights\"`: The learner can work on tasks which have observation weights / case weights.\n  * `\"importance\"`: The learner supports extracting importance values for features.\n  * `\"selected_features\"`: The learner supports extracting the features which were selected by the model.\n  * `\"oob_error\"`: The learner supports extracting the out of bag error.\n  * `\"loglik\"`: The learner supports extracting the log-likelihood of the learner.\n  * `\"hotstart_forward\"`: The learner allows to continue training the model e.g. by adding more trees to a random forest.\n  * `\"hotstart_backward\"`: The learner allows to \"undo\" some of the training, e.g. by removing some trees from a model.\n* `man`: The roxygen identifier of the learner.\n  This is used within the `$help()` method of the super class to open the help page of the learner.\n* `label`: The label of the learner.\n  This should briefly describe the learner (similar to the description's title) and is for example used for printing.\n\n### ParamSet {#param-set}\n\nThe `ParamSet` is the set of hyperparameters used in model training and predicting, this is given as a [`paradox::ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html).\nThe set consists of a list of hyperparameters, where each has a specific class for the hyperparameter type (see above).\nIn addition, each parameter has one or more tags, that determine in which method they are used.\n\nBeyond that there are other tags that serve specific purposes:\n\n* The tag `\"threads\"` should be used (if applicable) to tag the parameter that determines the number of threads used for the learner's internal parallelization.\n  This parameter can be set using [`set_threads`](https://mlr3.mlr-org.com/reference/set_threads.html).\n* The tag `\"required\"` should be used to tag parameters that must be provided for the algorithm to be executable.\n* In case [optional extractors](#optional-extractors) are available, the can (although this is rarely the case) also have parameters and can be tagged accordingly.\n* If [hotstarting](#hotstarting) is available, the fidelity parameter should be tagged with `\"hotstart\"`.\n\nFor `classif.rpart` the `param_set` can be defined as follows\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-004_892d088023ea711f447a188bd40c4b41'}\n\n```{.r .cell-code}\nparam_set = ps(\n  minsplit = p_int(lower = 1L, default = 20L, tags = \"train\"),\n  minbucket = p_int(lower = 1L, tags = \"train\"),\n  cp = p_dbl(lower = 0, upper = 1, default = 0.01, tags = \"train\"),\n  maxcompete = p_int(lower = 0L, default = 4L, tags = \"train\"),\n  maxsurrogate = p_int(lower = 0L, default = 5L, tags = \"train\"),\n  maxdepth = p_int(lower = 1L, upper = 30L, default = 30L, tags = \"train\"),\n  usesurrogate = p_int(lower = 0L, upper = 2L, default = 2L, tags = \"train\"),\n  surrogatestyle = p_int(lower = 0L, upper = 1L, default = 0L, tags = \"train\"),\n  xval = p_int(lower = 0L, default = 0L, tags = \"train\"),\n  keep_model = p_lgl(default = FALSE, tags = \"train\")\n)\nparam_set$values = list(xval = 0L)\n```\n:::\n\n\n\nWithin [mlr3](https://mlr3.mlr-org.com) packages we suggest to stick to the shorthand notation above for consistency, however the `param_set` can be written with the underlying R6 classes as shown here\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-005_f197c0d729da88b07183bf628578dd56'}\n\n```{.r .cell-code}\nparam_set = ParamSet$new(list(\n  ParamInt$new(id = \"minsplit\", default = 20L, lower = 1L, tags = \"train\"),\n  ParamInt$new(id = \"minbucket\", lower = 1L, tags = \"train\"),\n  ParamDbl$new(id = \"cp\", default = 0.01, lower = 0, upper = 1, tags = \"train\"),\n  ParamInt$new(id = \"maxcompete\", default = 4L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxsurrogate\", default = 5L, lower = 0L, tags = \"train\"),\n  ParamInt$new(id = \"maxdepth\", default = 30L, lower = 1L, upper = 30L, tags = \"train\"),\n  ParamInt$new(id = \"usesurrogate\", default = 2L, lower = 0L, upper = 2L, tags = \"train\"),\n  ParamInt$new(id = \"surrogatestyle\", default = 0L, lower = 0L, upper = 1L, tags = \"train\"),\n  ParamInt$new(id = \"xval\", default = 0L, lower = 0L, tags = \"train\"),\n  ParamLgl$new(id = \"keep_model\", default = FALSE, tags = \"train\")\n))\nparam_set$values = list(xval = 0L)\n```\n:::\n\n\n\nYou should read though the learner documentation to find the full list of available parameters. Just looking at some of these in this example:\n\n* `\"cp\"` is numeric, has a feasible range of `[0,1]` and defaults to `0.01`.\n  The parameter is used during `\"train\"`.\n* `\"xval\"` is integer has a lower bound of `0`, a default of `0` and the parameter is used during `\"train\"`.\n* `\"keep_model\"` is logical with a default of `FALSE` and is used during `\"train\"`.\n\nIn some rare cases you may want to change the default parameter values.\nYou can do this by changing the `param_set$values`.\nYou can see we have done this for `\"classif.rpart\"` where the default for `xval` is changed to `0`.\nNote that the default in the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) is recorded as our changed default (0), and not the original (10).\nIt is strongly recommended to only change the defaults if absolutely required, when this is the case add the following to the learner documentation:\n\n```r\n#' @section Custom mlr3 defaults:\n#' - `<parameter>`:\n#'   - Actual default: <value>\n#'   - Adjusted default: <value>\n#'   - Reason for change: <text>\n```\n\n### Train function {#learner-train}\n\nThe train function takes a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) as input and must return a model.\nLet's say we want to translate the following call of `rpart::rpart()` into code that can be used inside the `.train()` method.\n\nFirst, we write something down that works completely without [mlr3](https://mlr3.mlr-org.com):\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-006_1c641a616e9ef3f35c264fb822d85db6'}\n\n```{.r .cell-code}\ndata = iris\nmodel = rpart::rpart(Species ~ ., data = iris, xval = 0)\n```\n:::\n\n\n\nWe need to pass the formula notation `Species ~ .`, the data and the hyperparameters.\nTo get the hyperparameters, we call `self$param_set$get_values(tag = \"train\")` and thereby query all parameters that are using during `\"train\"`.\nThen, the dataset is extracted from the [`Task`](https://mlr3.mlr-org.com/reference/Task.html).\nBecause the learner has the property `\"weights\"`, we insert the weights of the task if there are any.\n\nLast, we call the upstream function `rpart::rpart()` with the data and pass all hyperparameters via argument `.args` using the [`mlr3misc::invoke()`](https://mlr3misc.mlr-org.com/reference/invoke.html) function.\nThe latter is simply an optimized version of `do.call()` that we use within the [mlr3](https://mlr3.mlr-org.com) ecosystem.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-007_1626a64e51e07e76e9d7e843256a8859'}\n\n```{.r .cell-code}\n.train = function(task) {\n  pars = self$param_set$get_values(tags = \"train\")\n  if (\"weights\" %in% task$properties) {\n    pars$weights = task$weights$weight\n  }\n  formula = task$formula()\n  data = task$data()\n  invoke(\n    rpart::rpart,\n    formula = formula,\n    data = data,\n    .args = pars\n  )\n}\n```\n:::\n\n\n\n### Predict function {#learner-predict}\n\nThe internal predict method `.predict()` also operates on a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) as well as on the fitted model that has been created by the `train()` call previously and has been stored in `self$model`.\n\nThe return value is a [`Prediction`](https://mlr3.mlr-org.com/reference/Prediction.html) object.\nWe proceed analogously to what we did in the previous section.\nWe start with a version without any [mlr3](https://mlr3.mlr-org.com) objects and continue to replace objects until we have reached the desired interface:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-008_56874a4a657ad7bdb1801e417002536d'}\n\n```{.r .cell-code}\n# inputs:\ntask = tsk(\"iris\")\nself = list(model = rpart::rpart(task$formula(), data = task$data()))\n\ndata = iris\nresponse = predict(self$model, newdata = data, type = \"class\")\nprob = predict(self$model, newdata = data, type = \"prob\")\n```\n:::\n\n\n\nThe `\"rpart::predict.rpart()\"` function predicts class labels if argument `type` is set to to `\"class\"`, and class probabilities if set to `\"prob\"`.\n\nNext, we transition from `data` to a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) again and construct a list with the return type requested by the user, this is stored in the `$predict_type` slot of a learner class. Note that the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) is automatically passed to the prediction object, so all you need to do is return the predictions! Make sure the list names are identical to the task predict types.\n\nThe final `.predict()` method is below, we could omit the `pars` line as there are no parameters with the `\"predict\"` tag but we keep it here to be consistent:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-009_0b79b6f007cb8ad3a7994f795311fefa'}\n\n```{.r .cell-code}\n.predict = function(task) {\n  pars = self$param_set$get_values(tags = \"predict\")\n  # get newdata and ensure same ordering in train and predict\n  newdata = ordered_features(task, self)\n  if (self$predict_type == \"response\") {\n    response = invoke(\n      predict,\n      self$model,\n      newdata = newdata,\n      type = \"class\",\n      .args = pars\n    )\n\n    return(list(response = response))\n  } else {\n    prob = invoke(\n      predict,\n      self$model,\n      newdata = newdata,\n      type = \"prob\",\n      .args = pars\n    )\n    return(list(prob = prob))\n  }\n}\n```\n:::\n\n\n\n:::{.callout-warning}\nYou cannot rely on the column order of the data returned by `task$data()` as the order of columns may be different from the order of the columns during `$.train`.\nThe `newdata` line ensures the ordering is the same by calling the same order as in train!\n:::\n\n### Optional Extractors {#optional-extractors}\n\nSpecific learner implementations are free to implement additional getters to ease the access of certain parts of the model in the inherited subclasses.\nThe blueprint for these methods is only included in the generated learner template, if the property is set when calling `create_learner()`.\nThe comments in the templates will include references to other learners that have this property and can be used as guiding examples.\nTo determine whether these methods are applicable, one has to determine whether the learner supports this method in principle **and** whether it is implemented in the upstream package.\n\nFor the following operations, extractors are standardized:\n\n* `importance(...)` - Returns the feature importance score as numeric vector.\n  The higher the score, the more important the variable.\n  The returned vector is named with feature names and sorted in decreasing order.\n  Note that the model might omit features it has not used at all. The learner must be tagged with property `\"importance\"`.\n* `selected_features(...)` - Returns a subset of selected features as character().\n  The learner must be tagged with property `\"selected_features\"`.\n* `oob_error(...)` - Returns the out-of-bag error of the model as `numeric(1)`.\n  The learner must be tagged with property `\"oob_error\"`.\n* `loglik(...)` - Extracts the log-likelihood (c.f. `stats::logLik()`).\n  The learner must be tagged with property `\"loglik\"`.\n\nIn this case, we only have to implement the `importance()` method.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-010_17a4678804e862ccf68943adbf44d839'}\n\n```{.r .cell-code}\nimportance = function() {\n  if (is.null(self$model)) {\n    stopf(\"No model stored\")\n  }\n\n  importance = sort(self$model$variable.importance, decreasing = TRUE)\n  if (is.null(importance)) {\n    importance = mlr3misc::set_names(numeric())\n  }\n  return(importance)\n}\n```\n:::\n\n\n\n### Hotstarting  {#hotstarting}\n\nSome learners support resuming or continuing from an already fitted model.\nWe assume that hotstarting is only possible if a single hyperparameter (also called the fidelity parameter, usually controlling the complexity or expensiveness) is altered and all other hyperparameters are identical.\nThe fidelity parameters should be tagged with `\"hotstart\"`.\nExamples are:\n\n* Random Forest: Starting from a model with n trees, a random forest with n + k trees can be obtained by adding k trees (`\"hotstart_forward\"`) and a random forest with n - k trees can be obtained by removing k trees (`\"hotstart_backward\"`).\n* Gradient Boosting: When having fitted a model with n iterations, we only need k iterations to obtain a model with n + k iterations. (`\"hotstart_forward\"`).\n\nFor more information see [`HotstartStack`](https://mlr3.mlr-org.com/reference/HotstartStack.html).\n\n### Control objects/functions of learners {#learner-control}\n\nSome learners rely on a \"control\" object/function such as `glmnet::glmnet.control()`.\nAccounting for such depends on how the underlying package works:\n\n* If the package forwards the control parameters via `...` and makes it possible to just pass control parameters as additional parameters directly to the train call, there is no need to distinguish both `\"train\"` and `\"control\"` parameters.\n* If the control parameters need to be passed via a separate argument, one can e.g. use\n  `formalArgs(glmnet::glmnet.control)` to get the names of the control parameters and then extract them from the `pars` like shown below.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-011_0190957ca7a466739b90af8febb15230'}\n\n```{.r .cell-code}\npars = self$param_set$get_values(tags = \"train\")\nii = names(pars) %in% formalArgs(glmnet::glmnet.control)\npars_ctrl = pars[ii]\npars_train = pars[!ii]\n\ninvoke([...], .args = pars_train, control = pars_ctrl)\n```\n:::\n\n\n\n### Adding the description\n\nOnce the learner is implemented - and is not only intended for personal use - it's description should be filled out.\nMost steps should be clear from the instructions given in the template.\nFor the section '\\@references', the entry first has to be added to the file `bibentries.R`, essentially by converting the bibtex file to a R `bibentry` function call.\n\n### Testing the learner {#learner-test}\n\nOnce your learner is created, you are ready to start testing if it works, there are three types of tests: [manual](#learner-test-manual), [unit](#learner-test-unit) and [parameter](#learner-test-parameter).\n\n#### Train and Predict {#learner-test-manual}\n\nFor a bare-bone check you can just try to run a simple `train()` call locally.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-012_5b8091e522ec6b4cbfa9393eaae15054'}\n\n```{.r .cell-code}\ntask = tsk(\"iris\") # assuming a Classif learner\nlrn = lrn(\"classif.rpart\")\nlrn$train(task)\np = lrn$predict(task)\np$confusion\n```\n:::\n\n\n\nIf it runs without erroring, that's a very good start!\n\n#### Autotest {#learner-test-unit}\n\nTo ensure that your learner is able to handle all kinds of different properties and feature types, we have written an \"autotest\" that checks the learner for different combinations of such.\n\nThe \"autotest\" setup is generated automatically by [`create_learner()`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html).\nIt will have a name with the form `test_package_type_key.R`, in our case this will actually be `test_rpart_classif_rpart.R`.\nThis will create the following script, for which no changes are required to pass (assuming the learner was correctly created):\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-013_6a6d9c5ccf1f7957c991d50b0f9d8632'}\n\n```{.r .cell-code}\ntest_that(\"autotest\", {\n  learner = lrn(\"classif.rpart\")\n  expect_learner(learner)\n  # note that you can skip tests using the exclude argument\n  result = run_autotest(learner)\n  expect_true(result, info = result$error)\n})\n```\n:::\n\n\n\nFor some learners that have required parameters, it is needed to set some values for required parameters after construction so that the learner can be run in the first place.\n\nYou can also exclude some specific test arrangements within the \"autotest\" via the argument `exclude` in the `run_autotest()` function.\nCurrently the `run_autotest()` function lives in [inst/testthat](https://github.com/mlr-org/mlr3/blob/f16326bf34bcac59c3b0a2fdbcf90dbebb3b4bbc/inst/testthat/helper_autotest.R) of the [mlr3](https://mlr3.mlr-org.com) and still lacks documentation.\nThis should change in the near future.\n\nTo finally run the test suite, call `devtools::test()` or hit `CTRL + Shift + T` if you are using RStudio.\n\n#### Checking Parameters {#learner-test-parameter}\n\nSome learners have a high number of parameters and it is easy to miss out on some during the creation of a new learner.\nIn addition, if the maintainer of the upstream package changes something with respect to the arguments of the algorithm, the learner is in danger to break.\nAlso, new arguments could be added upstream and manually checking for new additions all the time is tedious.\n\nTherefore we have written a \"Parameter Check\" that runs regularly for every learner.\nThis \"Parameter Check\" compares the parameters of the [mlr3](https://mlr3.mlr-org.com) ParamSet against all arguments available in the upstream function that is called during `$train()` and `$predict()`.\nAgain the file is automatically created by [`create_learner()`](https://mlr3extralearners.mlr-org.com/reference/create_learner.html).\nThis will be named like `test_paramtest_package_type_key.R`, so in our example `test_paramtest_rpart_classif_rpart.R`.\nWhen the `.train` function calls multiple functions (e.g. a control function as described above), a list of functions can be passed to the parameter test.\n\nThe test comes with an `exclude` argument that should be used to _exclude and explain_ why certain arguments of the upstream function are not within the ParamSet of the mlr3learner.\nThis will likely be required for all learners as common arguments like `x`, `target` or `data` are handled by the [mlr3](https://mlr3.mlr-org.com) interface and are therefore not included within the ParamSet.\n\nHowever, there might be more parameters that need to be excluded, for example:\n\n* Type dependent parameters, i.e. parameters that only apply for classification or regression learners.\n* Parameters that are actually deprecated by the upstream package and which were therefore not included in the [mlr3](https://mlr3.mlr-org.com) ParamSet.\n\nAll excluded parameters should have a comment justifying their exclusion.\n\nIn our example, the final paramtest script looks like:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-014_504dcca19867cce63c131d4661a3aaab'}\n\n```{.r .cell-code}\ntest_that(\"classif.rpart train\", {\n  learner = lrn(\"classif.rpart\")\n  # this can also be a list of functions\n  fun = rpart::rpart\n  exclude = c(\n    \"formula\", # handled internally\n    \"model\", # handled internally\n    \"data\", # handled internally\n    \"weights\", # handled by task\n    \"subset\", # handled by task\n    \"na.action\", # handled internally\n    \"method\", # handled internally\n    \"x\", # handled internally\n    \"y\", # handled internally\n    \"parms\", # handled internally\n    \"control\", # handled internally\n    \"cost\" # handled internally\n  )\n\n  paramtest = run_paramtest(learner, fun, exclude, tag = \"train\")\n  expect_paramtest(paramtest)\n})\n\ntest_that(\"classif.rpart predict\", {\n  learner = lrn(\"classif.rpart\")\n  fun = rpart:::predict.rpart\n  exclude = c(\n    \"object\", # handled internally\n    \"newdata\", # handled internally\n    \"type\", # handled internally\n    \"na.action\" # handled internally\n  )\n\n  paramtest = run_paramtest(learner, fun, exclude, tag = \"predict\")\n  expect_paramtest(paramtest)\n})\n```\n:::\n\n\n\n### Package Cleaning {#cleaning}\n\nOnce all tests are passing, run the following functions to ensure that the package remains clean and tidy\n\n1. `devtools::document(roclets = c('rd', 'collate', 'namespace'))`\n1. If you haven't done this before run: `remotes::install_github('mlr-org/styler.mlr')`\n1. `styler::style_pkg(style = styler.mlr::mlr_style)`\n1. `usethis::use_tidy_description()`\n1. `lintr::lint_package()`\n\nPlease fix any errors indicated by `lintr` before creating a pull request. Finally ensure that all `FIXME` are resolved and deleted in the generated files.\n\nYou are now ready to add your learner to the [mlr3](https://mlr3.mlr-org.com) ecosystem!\nSimply open a pull request to \\url{https://github.com/mlr-org/mlr3extralearners/pulls} with the new learner template and complete the checklist in there.\nCreating this request will trigger an automated workflow that checks whether various conditions (such as `rcmdcheck::rcmdcheck()`) are satisfied.\nOnce the pull request is approved and merged, your learner will automatically appear on the [package website](https://mlr3extralearners.mlr-org.com/).\n\n### Thanks and Maintenance\n\nThank you for contributing to the [mlr3](https://mlr3.mlr-org.com) ecosystem!\n\nWhen you created the learner you would have given your GitHub handle, meaning that you are now listed as the learner author and maintainer. This means that if the learner breaks it is your responsibility to fix the learner - you can view the status of your learner [here](https://mlr3extralearners.mlr-org.com/articles/learners/test_overview.html).\n\n\n### Learner FAQ {#learner-faq}\n\n**Question 1**\n\nHow to deal with Parameters which have no default?\n\n**Answer**\n\nIf the learner does not work without providing a value, set a reasonable default in `param_set$values`, add tag `\"required\"` to the parameter and document your default properly.\n\n**Question 2**\n\nWhere to add the package of the upstream package in the DESCRIPTION file?\n\nAdd it to the \"Suggests\" section.\n\n**Question 3**\n\nHow to handle arguments from external \"control\" functions such as `glmnet::glmnet_control()`?\n\n**Answer**\n\nSee [\"Control objects/functions of learners\"](#learner-control).\n\n**Question 4**\n\nHow to document if my learner uses a custom default value that differs to the default of the upstream package?\n\n**Answer**\n\nIf you set a custom default for the mlr3learner that does not cope with the one of the upstream package (think twice if this is really needed!), add this information to the help page of the respective learner.\n\nYou can use the following skeleton for this:\n\n```r\n#' @section Custom mlr3 defaults:\n#' * `<parameter>`:\n#'   * Actual default: <value>\n#'   * Adjusted default: <value>\n#'   * Reason for change: <text>\n```\n\n**Question 5**\n\nWhen should the `\"required\"` tag be used when defining Params and what is its purpose?\n\n**Answer**\n\nThe `\"required\"` tag should be used when the following conditions are met:\n\n* The upstream function cannot be run without setting this parameter, i.e. it would throw an error.\n* The parameter has no default in the upstream function.\n\nIn [mlr3](https://mlr3.mlr-org.com) we follow the principle that every learner should be constructable without setting custom parameters.\nTherefore, if a parameter has no default in the upstream function, a custom value is usually set for this parameter in the mlr3learner (remember to document such changes in the help page of the learner).\n\nEven though this practice ensures that no parameter is unset in an mlr3learner and partially removes the usefulness of the `\"required\"` tag, the tag is still useful in the following scenario:\n\nIf a user sets custom parameters after construction of the learner\n\n```r\nlrn = lrn(\"<id>\")\nlrn$param_set$values = list(\"<param>\" = <value>)\n```\n\nHere, all parameters besides the ones set in the list would be unset.\nSee `paradox::ParamSet` for more information.\nIf a parameter is tagged as `\"required\"` in the ParamSet, the call above would error and prompt the user that required parameters are missing.\n\n**Question 6**\n\nWhat is this error when I run `devtools::load_all()`\n\n```r\n> devtools::load_all(\".\")\nLoading mlr3extralearners\nWarning message:\n.onUnload failed in unloadNamespace() for 'mlr3extralearners', details:\n  call: vapply(hooks, function(x) environment(x)$pkgname, NA_character_)\n  error: values must be length 1,\n but FUN(X[[1]]) result is length 0\n```\n\n**Answer**\n\nThis is not an error but a warning and you can safely ignore it!\n\n## Adding new Measures {#extending-measures}\n\nIn this section we showcase how to implement a custom performance measure.\n\nA good starting point is writing down the loss function independently of [mlr3](https://mlr3.mlr-org.com) (we also did this in the [mlr3measures](https://mlr3measures.mlr-org.com) package).\nHere, we illustrate writing measure by implementing the root of the mean squared error for regression problems:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-015_32687a75487ed3b5b049112a31c6f16c'}\n\n```{.r .cell-code}\nroot_mse = function(truth, response) {\n  mse = mean((truth - response)^2)\n  sqrt(mse)\n}\n\nroot_mse(c(0, 0.5, 1), c(0.5, 0.5, 0.5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4082483\n```\n:::\n:::\n\n\n\nIn the next step, we embed the `root_mse()` function into a new [R6](https://cran.r-project.org/package=R6) class inheriting from base classes [`MeasureRegr`](https://mlr3.mlr-org.com/reference/MeasureRegr.html)/[`Measure`](https://mlr3.mlr-org.com/reference/Measure.html).\nFor classification measures, use [`MeasureClassif`](https://mlr3.mlr-org.com/reference/MeasureClassif.html).\nWe keep it simple here and only explain the most important parts of the [`Measure`](https://mlr3.mlr-org.com/reference/Measure.html) class:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-016_b23dfa5a98661305c642ebf37b3dc5e9'}\n\n```{.r .cell-code}\nMeasureRootMSE = R6::R6Class(\"MeasureRootMSE\",\n  inherit = mlr3::MeasureRegr,\n  public = list(\n    initialize = function() {\n      super$initialize(\n        # custom id for the measure\n        id = \"root_mse\",\n\n        # additional packages required to calculate this measure\n        packages = character(),\n\n        # properties, see below\n        properties = character(),\n\n        # required predict type of the learner\n        predict_type = \"response\",\n\n        # feasible range of values\n        range = c(0, Inf),\n\n        # minimize during tuning?\n        minimize = TRUE\n      )\n    }\n  ),\n\n  private = list(\n    # custom scoring function operating on the prediction object\n    .score = function(prediction, ...) {\n      root_mse = function(truth, response) {\n        mse = mean((truth - response)^2)\n        sqrt(mse)\n      }\n\n      root_mse(prediction$truth, prediction$response)\n    }\n  )\n)\n```\n:::\n\n\n\nThis class can be used as template for most performance measures.\nIf something is missing, you might want to consider having a deeper dive into the following arguments:\n\n* `properties`: If you tag you measure with the property `\"requires_task\"`, the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) is automatically passed to your `.score()` function (don't forget to add the argument [`Task`](https://mlr3.mlr-org.com/reference/Task.html) in the signature).\n  The same is possible with `\"requires_learner\"` if you need to operate on the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) and `\"requires_train_set\"` if you want to access the set of training indices in the score function.\n* `aggregator`: This function (defaulting to `mean()`) controls how multiple performance scores, i.e. from different resampling iterations, are aggregated into a single numeric value if `average` is set to micro averaging.\n  This is ignored for macro averaging.\n* `predict_sets`: Prediction sets (subset of `(\"train\", \"test\")`) to operate on.\n  Defaults to the \"test\" set.\n\nFinally, if you want to use your custom measure just like any other measure shipped with [mlr3](https://mlr3.mlr-org.com) and access it via the [`mlr_measures`](https://mlr3.mlr-org.com/reference/mlr_measures.html) dictionary, you can easily add it:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-017_e8e6bfe7db92a83cbb8d39ed6314f82d'}\n\n```{.r .cell-code}\nmlr3::mlr_measures$add(\"root_mse\", MeasureRootMSE)\n```\n:::\n\n\n\nTypically it is a good idea to put the measure together with the call to `mlr_measures$add()` in a new R file and just source it in your project.\n\n\n::: {.cell hash='extending_cache/pdf/extending-018_148c9d4a886abda7c66becdcfd844a2a'}\n\n```{.r .cell-code}\n## source(\"measure_root_mse.R\")\nmsr(\"root_mse\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<MeasureRootMSE:root_mse>\n* Packages: mlr3\n* Range: [0, Inf]\n* Minimize: TRUE\n* Average: macro\n* Parameters: list()\n* Properties: -\n* Predict type: response\n```\n:::\n:::\n\n\n\n\n\n## Adding new PipeOps {#extending-pipeops}\n\nThis section showcases how the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package can be extended to include custom [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s.\nTo run the following examples, we will need a [`Task`](https://mlr3.mlr-org.com/reference/Task.html); we are using the well-known \"Iris\" task:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-020_fcb55200c7721f0b8857fb2f9c68ea6f'}\n\n```{.r .cell-code}\nlibrary(\"mlr3\")\ntask = tsk(\"iris\")\ntask$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0\n```\n:::\n:::\n\n\n\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) is fundamentally built around [`R6`](https://r6.r-lib.org/).\nWhen planning to create custom [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) objects, it can only help to [familiarize yourself with it](https://adv-r.hadley.nz/r6.html).\n\nIn principle, all a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) must do is inherit from the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) R6 class and implement the `.train()` and `.predict()` functions.\nThere are, however, several auxiliary subclasses that can make the creation of *certain* operations much easier.\n\n### General Case Example: `PipeOpCopy` {#ext-pipeopcopy}\n\nA very simple yet useful [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) is `PipeOpCopy`, which takes a single input and creates a variable number of output channels, all of which receive a copy of the input data.\nIt is a simple example that showcases the important steps in defining a custom [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\nWe will show a simplified version here, **`PipeOpCopyTwo`**, that creates exactly two copies of its input data.\n\nThe following figure visualizes how our [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) is situated in the `Pipeline` and the significant in- and outputs.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-021_d76045aca29ca80c1a95c6ed185fb523'}\n::: {.cell-output-display}\n![](images/po_multi_viz.png){width=2.87in}\n:::\n:::\n\n\n\n#### First Steps: Inheriting from [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)\n\nThe first part of creating a custom [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) is inheriting from [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html).\nWe make a mental note that we need to implement a `.train()` and a `.predict()` function, and that we probably want to have an `initialize()` as well:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-022_e502820d197fa26591af05b5a7912496'}\n\n```{.r .cell-code}\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      ....\n    },\n  ),\n  private == list(\n    .train = function(inputs) {\n      ....\n    },\n\n    .predict = function(inputs) {\n      ....\n    }\n  )\n)\n```\n:::\n\n\n\nNote, that **private** methods, e.g. `.train` and `.predict` etc are prefixed with a `.`.\n\n#### Channel Definitions\n\nWe need to tell the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) the layout of its channels: How many there are, what their names are going to be, and what types are acceptable.\nThis is done on initialization of the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) (using a `super$initialize` call) by giving the `input` and `output` `data.table` objects.\nThese must have three columns: a `\"name\"` column giving the names of input and output channels, and a `\"train\"` and `\"predict\"` column naming the class of objects we expect during training and prediction as input / output.\nA special value for these classes is `\"*\"`, which indicates that any class will be accepted; our simple copy operator accepts any kind of input, so this will be useful. We have only one input, but two output channels.\n\nBy convention, we name a single channel `\"input\"` or `\"output\"`, and a group of channels [`\"input1\"`, `\"input2\"`, ...], unless there is a reason to give specific different names. Therefore, our `input` `data.table` will have a single row `<\"input\", \"*\", \"*\">`, and our `output` table will have two rows, `<\"output1\", \"*\", \"*\">` and `<\"output2\", \"*\", \"*\">`.\n\nAll of this is given to the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) creator. Our `initialize()` will thus look as follows:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-023_9a847d46b6a716e103e563b97da9302a'}\n\n```{.r .cell-code}\ninitialize = function(id = \"copy.two\") {\n  input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\")\n  # the following will create two rows and automatically fill the `train`\n  # and `predict` cols with \"*\"\n  output = data.table::data.table(\n    name = c(\"output1\", \"output2\"),\n    train = \"*\", predict = \"*\"\n  )\n  super$initialize(id,\n    input = input,\n    output = output\n  )\n}\n```\n:::\n\n\n\n#### Train and Predict\n\nBoth `.train()` and `.predict()` will receive a `list` as input and must give a `list` in return.\nAccording to our `input` and `output` definitions, we will always get a list with a single element as input, and will need to return a list with two elements. Because all we want to do is create two copies, we will just create the copies using `c(inputs, inputs)`.\n\nTwo things to consider:\n\n- The `.train()` function must always modify the `self$state` variable to something that is not `NULL` or `NO_OP`.\n  This is because the `$state` slot is used as a signal that [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)  has been trained on data, even if the state itself is not important to the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) (as in our case).\n  Therefore, our `.train()` will set `self$state = list()`.\n\n- It is not necessary to \"clone\" our input or make deep copies, because we don't modify the data.\n  However, if we were changing a reference-passed object, for example by changing data in a [`Task`](https://mlr3.mlr-org.com/reference/Task.html), we would have to make a deep copy first.\n  This is because a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) may never modify its input object by reference.\n\nOur `.train()` and `.predict()` functions are now:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-024_2c5b000b16a8c82506f470f9f034f9a4'}\n\n```{.r .cell-code}\n.train = function(inputs) {\n  self$state = list()\n  c(inputs, inputs)\n}\n```\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-025_d6c3c9b610d437f05ac950eff3ddabb8'}\n\n```{.r .cell-code}\n.predict = function(inputs) {\n  c(inputs, inputs)\n}\n```\n:::\n\n\n\n#### Putting it Together\n\nThe whole definition thus becomes\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-026_bec3dec5ccce537f8338a30fb2421fc0'}\n\n```{.r .cell-code}\nPipeOpCopyTwo = R6::R6Class(\"PipeOpCopyTwo\",\n  inherit = mlr3pipelines::PipeOp,\n  public = list(\n    initialize = function(id = \"copy.two\") {\n      super$initialize(id,\n        input = data.table::data.table(name = \"input\", train = \"*\", predict = \"*\"),\n        output = data.table::data.table(name = c(\"output1\", \"output2\"),\n                            train = \"*\", predict = \"*\")\n      )\n    }\n  ),\n  private = list(\n    .train = function(inputs) {\n      self$state = list()\n      c(inputs, inputs)\n    },\n\n    .predict = function(inputs) {\n      c(inputs, inputs)\n    }\n  )\n)\n```\n:::\n\n\n\nWe can create an instance of our [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html), put it in a graph, and see what happens when we train it on something:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-027_4a482ba4b4048574e0c25ce2f6356fb4'}\n\n```{.r .cell-code}\nlibrary(\"mlr3pipelines\")\npoct = PipeOpCopyTwo$new()\ngr = Graph$new()\ngr$add_pipeop(poct)\n\nprint(gr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGraph with 1 PipeOps:\n       ID         State sccssors prdcssors\n copy.two <<UNTRAINED>>                   \n```\n:::\n\n```{.r .cell-code}\nresult = gr$train(task)\n\nstr(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ copy.two.output1:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n $ copy.two.output2:Classes 'TaskClassif', 'TaskSupervised', 'Task', 'R6' <TaskClassif:iris> \n```\n:::\n:::\n\n\n\n### Special Case: Preprocessing {#ext-pipe-preproc}\n\nMany [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s perform an operation on exactly one [`Task`](https://mlr3.mlr-org.com/reference/Task.html), and return exactly one [`Task`](https://mlr3.mlr-org.com/reference/Task.html). They may even not care about the \"Target\" / \"Outcome\" variable of that task, and only do some modification of some input data.\nHowever, it is usually important to them that the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) on which they perform prediction has the same data columns as the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) on which they train.\nFor these cases, the auxiliary base class [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html) exists.\nIt inherits from [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) itself, and other [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s should use it if they fall in the kind of use-case named above.\n\nWhen inheriting from [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html), one must either implement the private methods `.train_task()` and `.predict_task()`, or the methods `.train_dt()`, `.predict_dt()`, depending on whether wants to operate on a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) object or on its data as `data.table`s.\nIn the second case, one can optionally also overload the `.select_cols()` method, which chooses which of the incoming [`Task`](https://mlr3.mlr-org.com/reference/Task.html)'s features are given to the `.train_dt()` / `.predict_dt()` functions.\n\nThe following will show two examples: `PipeOpDropNA`, which removes a [`Task`](https://mlr3.mlr-org.com/reference/Task.html)'s rows with missing values during training (and implements `.train_task()` and `.predict_task()`), and [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html), which scales a [`Task`](https://mlr3.mlr-org.com/reference/Task.html)'s numeric columns (and implements `.train_dt()`, `.predict_dt()`, and `.select_cols()`).\n\n#### Example: `PipeOpDropNA`\n\nDropping rows with missing values may be important when training a model that can not handle them.\n\nBecause [mlr3](https://mlr3.mlr-org.com) `\"Task\", text = \"Tasks\")` only contain a view to the underlying data, it is not necessary to modify data to remove rows with missing values.\nInstead, the rows can be removed using the [`Task`](https://mlr3.mlr-org.com/reference/Task.html)'s `$filter` method, which modifies the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) in-place.\nThis is done in the private method `.train_task()`.\nWe take care that we also set the `$state` slot to signal that the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) was trained.\n\nThe private method `.predict_task()` does not need to do anything; removing missing values during prediction is not as useful, since learners that cannot handle them will just ignore the respective rows.\nFurthermore, [mlr3](https://mlr3.mlr-org.com) expects a [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) to always return just as many predictions as it was given input rows, so a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) that removes [`Task`](https://mlr3.mlr-org.com/reference/Task.html) rows during training can not be used inside a [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html).\n\nWhen we inherit from [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html), it sets the `input` and `output` `data.table`s for us to only accept a single [`Task`](https://mlr3.mlr-org.com/reference/Task.html).\nThe only thing we do during `initialize()` is therefore to set an `id` (which can optionally be changed by the user).\n\nThe complete `PipeOpDropNA` can therefore be written as follows.\nNote that it inherits from [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html), unlike the `PipeOpCopyTwo` example from above:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-028_15a91f232079e4d6074dee92e581103b'}\n\n```{.r .cell-code}\nPipeOpDropNA = R6::R6Class(\"PipeOpDropNA\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"drop.na\") {\n      super$initialize(id)\n    }\n  ),\n\n  private = list(\n    .train_task = function(task) {\n      self$state = list()\n      featuredata = task$data(cols = task$feature_names)\n      exclude = apply(is.na(featuredata), 1, any)\n      task$filter(task$row_ids[!exclude])\n    },\n\n    .predict_task = function(task) {\n      # nothing to be done\n      task\n    }\n  )\n)\n```\n:::\n\n\n\nTo test this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html), we create a small task with missing values:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-029_6841b633bc89906acac20fa2b032a6f4'}\n\n```{.r .cell-code}\nsmalliris = iris[(1:5) * 30, ]\nsmalliris[1, 1] = NA\nsmalliris[2, 2] = NA\nsitask = as_task_classif(smalliris, target = \"Species\")\nprint(sitask$data())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:     setosa          1.6         0.2           NA         3.2\n2: versicolor          3.9         1.4          5.2          NA\n3: versicolor          4.0         1.3          5.5         2.5\n4:  virginica          5.0         1.5          6.0         2.2\n5:  virginica          5.1         1.8          5.9         3.0\n```\n:::\n:::\n\n\n\nWe test this by feeding it to a new [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) that uses `PipeOpDropNA`.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-030_def8430818b6ddd07b23fc080583c354'}\n\n```{.r .cell-code}\ngr = Graph$new()\ngr$add_pipeop(PipeOpDropNA$new())\n\nfiltered_task = gr$train(sitask)[[1]]\nprint(filtered_task$data())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1: versicolor          4.0         1.3          5.5         2.5\n2:  virginica          5.0         1.5          6.0         2.2\n3:  virginica          5.1         1.8          5.9         3.0\n```\n:::\n:::\n\n\n\n#### Example: `PipeOpScaleAlways`\n\nAn often-applied preprocessing step is to simply **center** and/or **scale** the data to mean $0$ and standard deviation $1$.\nThis fits the [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html) pattern quite well.\nBecause it always replaces all columns that it operates on, and does not require any information about the task's target, it only needs to overload the `.train_dt()` and `.predict_dt()` functions.\nThis saves some boilerplate-code from getting the correct feature columns out of the task, and replacing them after modification.\n\nBecause scaling only makes sense on numeric features, we want to instruct [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html) to give us only these numeric columns.\nWe do this by overloading the `.select_cols()` function: It is called by the class to determine which columns to pass to `.train_dt()` and `.predict_dt()`.\nIts input is the [`Task`](https://mlr3.mlr-org.com/reference/Task.html) that is being transformed, and it should return a `character` vector of all features to work with.\nWhen it is not overloaded, it uses all columns; instead, we will set it to only give us numeric columns.\nBecause the `levels()` of the data table given to `.train_dt()` and `.predict_dt()` may be different from the [`Task`](https://mlr3.mlr-org.com/reference/Task.html)'s levels, these functions must also take a `levels` argument that is a named list of column names indicating their levels.\nWhen working with numeric data, this argument can be ignored, but it should be used instead of `levels(dt[[column]])` for factorial or character columns.\n\nThis is the first [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) where we will be using the `$state` slot for something useful: We save the centering offset and scaling coefficient and use it in `$.predict()`!\n\nFor simplicity, we are not using hyperparameters and will always scale and center all data.\nCompare this `PipeOpScaleAlways` operator to the one defined inside the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package, [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html).\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-031_cc30661dd3456a31fcfd3760245ca8c0'}\n\n```{.r .cell-code}\nPipeOpScaleAlways = R6::R6Class(\"PipeOpScaleAlways\",\n  inherit = mlr3pipelines::PipeOpTaskPreproc,\n  public = list(\n    initialize = function(id = \"scale.always\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .train_dt = function(dt, levels, target) {\n      sc = scale(as.matrix(dt))\n      self$state = list(\n        center = attr(sc, \"scaled:center\"),\n        scale = attr(sc, \"scaled:scale\")\n      )\n      sc\n    },\n\n    .predict_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n```\n:::\n\n\n\n_(Note for the observant: If you check `PipeOpScale.R` from the [mlr3pipelines](https://mlr3pipelines.mlr-org.com) package, you will notice that is uses \"`get(\"type\")`\" and \"`get(\"id\")`\" instead of \"`type`\" and \"`id`\", because the static code checker on CRAN would otherwise complain about references to undefined variables. This is a \"problem\" with `data.table` and not exclusive to [mlr3pipelines](https://mlr3pipelines.mlr-org.com).)_\n\nWe can, again, create a new [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html) that uses this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) to test it.\nCompare the resulting data to the original \"iris\" [`Task`](https://mlr3.mlr-org.com/reference/Task.html) data printed at the beginning:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-032_4b1f4cbfc8185ee65af901dd37ba7738'}\n\n```{.r .cell-code}\ngr = Graph$new()\ngr$add_pipeop(PipeOpScaleAlways$new())\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n```\n:::\n:::\n\n\n\n### Special Case: Preprocessing with Simple Train\n\nIt is possible to make even further simplifications for many [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s that perform mostly the same operation during training and prediction.\nThe point of [`Task`](https://mlr3.mlr-org.com/reference/Task.html) preprocessing is often to modify the training data in mostly the same way as prediction data (but in a way that *may* depend on training data).\n\nConsider constant feature removal, for example: The goal is to remove features that have no variance, or only a single factor level.\nHowever, what features get removed must be decided during *training*, and may only depend on training data.\nFurthermore, the actual process of removing features is the same during training and prediction.\n\nA simplification to make is therefore to have a private method `.get_state(task)` which sets the `$state` slot during training, and a private method `.transform(task)`, which gets called both during training *and* prediction.\nThis is done in the [`PipeOpTaskPreprocSimple`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html) class.\nJust like [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html), one can inherit from this and overload these functions to get a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) that performs preprocessing with very little boilerplate code.\n\nJust like [`PipeOpTaskPreproc`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreproc.html), [`PipeOpTaskPreprocSimple`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html) offers the possibility to instead overload the `.get_state_dt(dt, levels)` and `.transform_dt(dt, levels)` methods (and optionally, again, the `.select_cols(task)` function) to operate on `data.table` feature data instead of the whole [`Task`](https://mlr3.mlr-org.com/reference/Task.html).\n\nEven some methods that do not use [`PipeOpTaskPreprocSimple`](https://mlr3pipelines.mlr-org.com/reference/PipeOpTaskPreprocSimple.html) *could* work in a similar way: The `PipeOpScaleAlways` example from above will be shown to also work with this paradigm.\n\n#### Example: `PipeOpDropConst`\n\nA typical example of a preprocessing operation that does almost the same operation during training and prediction is an operation that drops features depending on a criterion that is evaluated during training.\nOne simple example of this is dropping constant features.\nBecause the [mlr3](https://mlr3.mlr-org.com) [`Task`](https://mlr3.mlr-org.com/reference/Task.html) class offers a flexible view on underlying data, it is most efficient to drop columns from the task directly using its `$select()` function, so the `.get_state_dt(dt, levels)` / `.transform_dt(dt, levels)` functions will *not* get used; instead we overload the `.get_state(task)` and `.transform(task)` methods.\n\nThe `.get_state()` function's result is saved to the `$state` slot, so we want to return something that is useful for dropping features.\nWe choose to save the names of all the columns that have nonzero variance.\nFor brevity, we use `length(unique(column)) > 1` to check whether more than one distinct value is present; a more sophisticated version could have a tolerance parameter for numeric values that are very close to each other.\n\nThe `.transform()` method is evaluated both during training *and* prediction, and can rely on the `$state` slot being present.\nAll it does here is call the `Task$select` function with the columns we chose to keep.\n\nThe full [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) could be written as follows:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-033_12a9862488cf5e85ab2656c2b36379f2'}\n\n```{.r .cell-code}\nPipeOpDropConst = R6::R6Class(\"PipeOpDropConst\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"drop.const\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .get_state = function(task) {\n      data = task$data(cols = task$feature_names)\n      nonconst = sapply(data, function(column) length(unique(column)) > 1)\n      list(cnames = colnames(data)[nonconst])\n    },\n\n    .transform = function(task) {\n      task$select(self$state$cnames)\n    }\n  )\n)\n```\n:::\n\n\n\nThis can be tested using the first five rows of the \"Iris\" [`Task`](https://mlr3.mlr-org.com/reference/Task.html), for which one feature (`\"Petal.Width\"`) is constant:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-034_8d8539a6571b15798351bc168e24ee7d'}\n\n```{.r .cell-code}\nirishead = task$clone()$filter(1:5)\nirishead$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n1:  setosa          1.4         0.2          5.1         3.5\n2:  setosa          1.4         0.2          4.9         3.0\n3:  setosa          1.3         0.2          4.7         3.2\n4:  setosa          1.5         0.2          4.6         3.1\n5:  setosa          1.4         0.2          5.0         3.6\n```\n:::\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-035_da0f8190cc654ec02befcfff545a66c1'}\n\n```{.r .cell-code}\ngr = Graph$new()$add_pipeop(PipeOpDropConst$new())\ndropped_task = gr$train(irishead)[[1]]\n\ndropped_task$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Species Petal.Length Sepal.Length Sepal.Width\n1:  setosa          1.4          5.1         3.5\n2:  setosa          1.4          4.9         3.0\n3:  setosa          1.3          4.7         3.2\n4:  setosa          1.5          4.6         3.1\n5:  setosa          1.4          5.0         3.6\n```\n:::\n:::\n\n\n\nWe can also see that the `$state` was correctly set.\nCalling `$.predict()` with this graph, even with different data (the whole Iris [`Task`](https://mlr3.mlr-org.com/reference/Task.html)!) will still drop the `\"Petal.Width\"` column, as it should.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-036_f5df9bd1854294f4ae921bf05e6872f2'}\n\n```{.r .cell-code}\ngr$pipeops$drop.const$state\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$cnames\n[1] \"Petal.Length\" \"Sepal.Length\" \"Sepal.Width\" \n\n$affected_cols\n[1] \"Petal.Length\" \"Petal.Width\"  \"Sepal.Length\" \"Sepal.Width\" \n\n$intasklayout\n             id    type\n1: Petal.Length numeric\n2:  Petal.Width numeric\n3: Sepal.Length numeric\n4:  Sepal.Width numeric\n\n$outtasklayout\n             id    type\n1: Petal.Length numeric\n2: Sepal.Length numeric\n3:  Sepal.Width numeric\n\n$outtaskshell\nEmpty data.table (0 rows and 4 cols): Species,Petal.Length,Sepal.Length,Sepal.Width\n```\n:::\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-037_bdad0a6fcfa16f1ea7b5eaaa6e0e8eae'}\n\n```{.r .cell-code}\ndropped_predict = gr$predict(task)[[1]]\n\ndropped_predict$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Sepal.Length Sepal.Width\n  1:    setosa          1.4          5.1         3.5\n  2:    setosa          1.4          4.9         3.0\n  3:    setosa          1.3          4.7         3.2\n  4:    setosa          1.5          4.6         3.1\n  5:    setosa          1.4          5.0         3.6\n ---                                                \n146: virginica          5.2          6.7         3.0\n147: virginica          5.0          6.3         2.5\n148: virginica          5.2          6.5         3.0\n149: virginica          5.4          6.2         3.4\n150: virginica          5.1          5.9         3.0\n```\n:::\n:::\n\n\n\n#### Example: `PipeOpScaleAlwaysSimple`\n\nThis example will show how a `PipeOpTaskPreprocSimple` can be used when only working on feature data in form of a `data.table`.\nInstead of calling the `scale()` function, the `center` and `scale` values are calculated directly and saved to the `$state` slot.\nThe `.transform_dt()` function will then perform the same operation during both training and prediction: subtract the `center` and divide by the `scale` value.\nAs in the [`PipeOpScaleAlways` example above](#example-pipeopscalealways), we use `.select_cols()` so that we only work on numeric columns.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-038_6f43c5e685369f8e3fa012c553e79f85'}\n\n```{.r .cell-code}\nPipeOpScaleAlwaysSimple = R6::R6Class(\"PipeOpScaleAlwaysSimple\",\n  inherit = mlr3pipelines::PipeOpTaskPreprocSimple,\n  public = list(\n    initialize = function(id = \"scale.always.simple\") {\n      super$initialize(id = id)\n    }\n  ),\n\n  private = list(\n    .select_cols = function(task) {\n      task$feature_types[type == \"numeric\", id]\n    },\n\n    .get_state_dt = function(dt, levels, target) {\n      list(\n        center = sapply(dt, mean),\n        scale = sapply(dt, sd)\n      )\n    },\n\n    .transform_dt = function(dt, levels) {\n      t((t(dt) - self$state$center) / self$state$scale)\n    }\n  )\n)\n```\n:::\n\n\n\nWe can compare this [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) to the one above to show that it behaves the same.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-039_e3fbfbe385636aa597fa6ab1e221e261'}\n\n```{.r .cell-code}\ngr = Graph$new()$add_pipeop(PipeOpScaleAlways$new())\nresult_posa = gr$train(task)[[1]]\n\ngr = Graph$new()$add_pipeop(PipeOpScaleAlwaysSimple$new())\nresult_posa_simple = gr$train(task)[[1]]\n```\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-040_5e7118354c4f3021f40455b75515a020'}\n\n```{.r .cell-code}\nresult_posa$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n```\n:::\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-041_a9cc75a4ec276956456c418da279d712'}\n\n```{.r .cell-code}\nresult_posa_simple$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa   -1.3357516  -1.3110521  -0.89767388  1.01560199\n  2:    setosa   -1.3357516  -1.3110521  -1.13920048 -0.13153881\n  3:    setosa   -1.3923993  -1.3110521  -1.38072709  0.32731751\n  4:    setosa   -1.2791040  -1.3110521  -1.50149039  0.09788935\n  5:    setosa   -1.3357516  -1.3110521  -1.01843718  1.24503015\n ---                                                            \n146: virginica    0.8168591   1.4439941   1.03453895 -0.13153881\n147: virginica    0.7035638   0.9192234   0.55148575 -1.27867961\n148: virginica    0.8168591   1.0504160   0.79301235 -0.13153881\n149: virginica    0.9301544   1.4439941   0.43072244  0.78617383\n150: virginica    0.7602115   0.7880307   0.06843254 -0.13153881\n```\n:::\n:::\n\n\n\n### Hyperparameters {#ext-pipe-hyperpars}\n\n[mlr3pipelines](https://mlr3pipelines.mlr-org.com) uses the [[paradox](https://paradox.mlr-org.com)](https://paradox.mlr-org.com) package to define parameter spaces for [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s.\nParameters for [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)s can modify their behavior in certain ways, e.g. switch centering or scaling off in the [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html) operator.\nThe unified interface makes it possible to have parameters for whole [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html)s that modify the individual [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)'s behavior.\nThe [`Graph`](https://mlr3pipelines.mlr-org.com/reference/Graph.html)s, when encapsulated in [`GraphLearner`](https://mlr3pipelines.mlr-org.com/reference/mlr_learners_graph.html)s, can even be tuned using the tuning functionality in [mlr3tuning](https://mlr3tuning.mlr-org.com).\n\nHyperparameters are declared during initialization, when calling the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html)'s `$initialize()` function, by giving a `param_set` argument.\nThe `param_set` must be a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) from the [paradox](https://paradox.mlr-org.com) package; see [the tuning chapter](#searchspace) or the [in-depth [paradox](https://paradox.mlr-org.com) chapter](#paradox) for more information on how to define parameter spaces.\nAfter construction, the [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) can be accessed through the `$param_set` slot.\nWhile it is *possible* to modify this [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html), using e.g. the `$add()` and `$add_dep()` functions, *after* adding it to the [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html), it is strongly advised against.\n\nHyperparameters can be set and queried through the `$values` slot.\nWhen setting hyperparameters, they are automatically checked to satisfy all conditions set by the `$param_set`, so it is not necessary to type check them.\nBe aware that it is always possible to *remove* hyperparameter values.\n\nWhen a [`PipeOp`](https://mlr3pipelines.mlr-org.com/reference/PipeOp.html) is initialized, it usually does not have any parameter values---`$values` takes the value `list()`.\nIt is possible to set initial parameter values in the `$initialize()` constructor; this must be done *after* the `super$initialize()` call where the corresponding [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) must be supplied.\nThis is because setting `$values` checks against the current `$param_set`, which would fail if the `$param_set` was not set yet.\n\nWhen using an underlying library function (the `scale` function in [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html), say), then there is usually a \"default\" behaviour of that function when a parameter is not given.\nIt is good practice to use this default behaviour whenever a parameter is not set (or when it was removed).\nThis can easily be done when using the [mlr3misc](https://mlr3misc.mlr-org.com) library's [`mlr3misc::invoke()`](https://mlr3misc.mlr-org.com/reference/invoke.html) function, which has functionality similar to `\"do.call()\"`.\n\n#### Hyperparameter Example: [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html)\n\nHow to use hyperparameters can best be shown through the example of [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html), which is very similar to the example above, `PipeOpScaleAlways`.\nThe difference is made by the presence of hyperparameters.\n[`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html) constructs a [`ParamSet`](https://paradox.mlr-org.com/reference/ParamSet.html) in its `$initialize` function and passes this on to the `super$initialize` function:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-042_9e54657e4fae94dd8553c7462646448d'}\n\n```{.r .cell-code}\nPipeOpScale$public_methods$initialize\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (id = \"scale\", param_vals = list()) \n.__PipeOpScale__initialize(self = self, private = private, super = super, \n    id = id, param_vals = param_vals)\n<environment: namespace:mlr3pipelines>\n```\n:::\n:::\n\n\n\nThe user has access to this and can set and get parameters.\nTypes are automatically checked:\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-043_bbe18d1c4433f7262eae57c52e20a8a1'}\n\n```{.r .cell-code}\npss = po(\"scale\")\nprint(pss$param_set)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<ParamSet:scale>\n               id    class lower upper nlevels        default value\n1:         center ParamLgl    NA    NA       2           TRUE      \n2:          scale ParamLgl    NA    NA       2           TRUE      \n3:         robust ParamLgl    NA    NA       2 <NoDefault[3]> FALSE\n4: affect_columns ParamUty    NA    NA     Inf  <Selector[1]>      \n```\n:::\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-044_7029fb65af3050a7e299f4af9adbc55a'}\n\n```{.r .cell-code}\npss$param_set$values$center = FALSE\nprint(pss$param_set$values)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$robust\n[1] FALSE\n\n$center\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell hash='extending_cache/pdf/extending-045_5bbd23f1d61999f2937ef65952482941'}\n\n```{.r .cell-code}\npss$param_set$values$scale = \"TRUE\" # bad input is checked!\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in self$assert(xs): Assertion on 'xs' failed: scale: Must be of type 'logical flag', not 'character'.\n```\n:::\n:::\n\n\n\nHow [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html) handles its parameters can be seen in its `$.train_dt` method: It gets the relevant parameters from its `$values` slot and uses them in the [`mlr3misc::invoke()`](https://mlr3misc.mlr-org.com/reference/invoke.html) call.\nThis has the advantage over calling `scale()` directly that if a parameter is not given, its default value from the `\"scale()\"` function will be used.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-046_41a2cbef8a0fd730b50f33bc3f95985a'}\n\n```{.r .cell-code}\nPipeOpScale$private_methods$.train_dt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nfunction (dt, levels, target) \n.__PipeOpScale__.train_dt(self = self, private = private, super = super, \n    dt = dt, levels = levels, target = target)\n<environment: namespace:mlr3pipelines>\n```\n:::\n:::\n\n\n\nAnother change that is necessary compared to `PipeOpScaleAlways` is that the attributes `\"scaled:scale\"` and `\"scaled:center\"` are not always present, depending on parameters, and possibly need to be set to default values $1$ or $0$, respectively.\n\nIt is now even possible (if a bit pointless) to call [`PipeOpScale`](https://mlr3pipelines.mlr-org.com/reference/mlr_pipeops_scale.html) with both `scale` and `center` set to `FALSE`, which returns the original dataset, unchanged.\n\n\n\n::: {.cell hash='extending_cache/pdf/extending-047_82150efa0b6cfc626cdcc245a08f687a'}\n\n```{.r .cell-code}\npss$param_set$values$scale = FALSE\npss$param_set$values$center = FALSE\n\ngr = Graph$new()\ngr$add_pipeop(pss)\n\nresult = gr$train(task)\n\nresult[[1]]$data()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Species Petal.Length Petal.Width Sepal.Length Sepal.Width\n  1:    setosa          1.4         0.2          5.1         3.5\n  2:    setosa          1.4         0.2          4.9         3.0\n  3:    setosa          1.3         0.2          4.7         3.2\n  4:    setosa          1.5         0.2          4.6         3.1\n  5:    setosa          1.4         0.2          5.0         3.6\n ---                                                            \n146: virginica          5.2         2.3          6.7         3.0\n147: virginica          5.0         1.9          6.3         2.5\n148: virginica          5.2         2.0          6.5         3.0\n149: virginica          5.4         2.3          6.2         3.4\n150: virginica          5.1         1.8          5.9         3.0\n```\n:::\n:::\n\n\n\n## Adding new Tuners {#extending-tuners}\n\nIn this section, we show how to implement a custom tuner for [mlr3tuning](https://mlr3tuning.mlr-org.com).\nThe main task of a tuner is to iteratively propose new hyperparameter configurations that we want to evaluate for a given task, learner and validation strategy.\nThe second task is to decide which configuration should be returned as a tuning result - usually it is the configuration that led to the best observed performance value.\nIf you want to implement your own tuner, you have to implement an R6-Object that offers an  [`.optimize`](#tuner-optimize) method that implements the iterative proposal and you are free to implement [`.assign_result`](#tuner-add-result) to differ from the before-mentioned default process of determining the result.\n\nBefore you start with the implementation make yourself familiar with the main R6-Objects in [bbotk](https://bbotk.mlr-org.com) (Black-Box Optimization Toolkit).\nThis package does not only provide basic black box optimization algorithms and but also the objects that represent the optimization problem ([`OptimInstance`](https://bbotk.mlr-org.com/reference/OptimInstance.html)) and the log of all evaluated configurations ([`Archive`](https://bbotk.mlr-org.com/reference/Archive.html)).\n\nThere are two ways to implement a new tuner:\na ) If your new tuner can be applied to any kind of optimization problem it should be implemented as a [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html).\nAny [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html) can be easily transformed to a [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html).\nb) If the new custom tuner is only usable for hyperparameter tuning, for example because it needs to access the task, learner or resampling objects it should be directly implemented in [mlr3tuning](https://mlr3tuning.mlr-org.com) as a [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html).\n\n### Adding a new Tuner {#extending-tuners-summary}\n\nThis is a summary of steps for adding a new tuner.\nThe fifth step is only required if the new tuner is added via [bbotk](https://bbotk.mlr-org.com).\n\n1. Check the tuner does not already exist as a [[`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html)](https://github.com/mlr-org/bbotk/tree/master/R) or [[`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html)](https://github.com/mlr-org/mlr3tuning/tree/master/R) in the GitHub repositories.\n1. Use one of the existing optimizers / tuners as a [template](#tuner-template).\n1. Overwrite the [`.optimize`](#tuner-optimize) private method of the optimizer / tuner.\n1. Optionally, overwrite the default [`.assign_result`](#tuner-add-result) private method.\n1. Use the [`mlr3tuning::TunerFromOptimizer`](#tuner-from-optimizer) class to transform the [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html) to a [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html).\n1. Add [unit tests](#tuner-test) for the tuner and optionally for the optimizer.\n1. Open a new pull request for the [[`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html)](https://github.com/mlr-org/mlr3tuning/pulls) and optionally a second one for the [[`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html)](https://github.com/mlr-org/bbotk/pulls).\n\n### Template {#tuner-template}\n\nIf the new custom tuner is implemented via [bbotk](https://bbotk.mlr-org.com), use one of the existing optimizer as a template e.g. [`bbotk::OptimizerRandomSearch`](https://github.com/mlr-org/bbotk/blob/master/R/OptimizerRandomSearch.R). There are currently only two tuners that are not based on a [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html): [`mlr3hyperband::TunerHyperband`](https://github.com/mlr-org/mlr3hyperband/blob/master/R/TunerHyperband.R) and [`mlr3tuning::TunerIrace`](https://github.com/mlr-org/mlr3tuning/blob/master/R/TunerIrace.R). Both are rather complex but you can still use the documentation and class structure as a template. The following steps are identical for optimizers and tuners.\n\nRewrite the meta information in the documentation and create a new class name.\nScientific sources can be added in `R/bibentries.R` which are added under `@source` in the documentation.\nThe example and dictionary sections of the documentation are auto-generated based on the `@templateVar id <tuner_id>`.\nChange the parameter set of the optimizer / tuner and document them under `@section Parameters`.\nDo not forget to change `mlr_optimizers$add()` / `mlr_tuners$add()` in the last line which adds the optimizer / tuner to the dictionary.\n\n### Optimize method {#tuner-optimize}\n\nThe `$.optimize()` private method is the main part of the tuner.\nIt takes an instance, proposes new points and calls the `$eval_batch()` method of the instance to evaluate them.\nHere you can go two ways: Implement the iterative process yourself or call an external optimization function that resides in another package.\n\n#### Writing a custom iteration\n\nUsually, the proposal and evaluation is done in a `repeat`-loop which you have to implement.\nPlease consider the following points:\n\n- You can evaluate one or multiple points per iteration\n- You don't have to care about termination, as `$eval_batch()` won't allow more evaluations then allowed by the `bbotk::Terminator`. This implies, that code after the `repeat`-loop will not be executed.\n- You don't have to care about keeping track of the evaluations as every evaluation is automatically stored in `inst$archive`.\n- If you want to log additional information for each evaluation of the [`Objective`](https://bbotk.mlr-org.com/reference/Objective.html) in the [`Archive`](https://bbotk.mlr-org.com/reference/Archive.html) you can simply add columns to the `data.table` object that is passed to `$eval_batch()`.\n\n#### Calling an external optimization function\n\nOptimization functions from external packages usually take an objective function as an argument.\nIn this case, you can pass `inst$objective_function` which internally calls `$eval_batch()`.\nCheck out [`OptimizerGenSA`](https://github.com/mlr-org/bbotk/blob/master/R/OptimizerGenSA.R) for an example.\n\n### Assign result method {#tuner-add-result}\n\nThe default `$.assign_result()` private method simply obtains the best performing result from the archive.\nThe default method can be overwritten if the new tuner determines the result of the optimization in a different way.\nThe new function must call the `$assign_result()` method of the instance to write the final result to the instance.\nSee [`mlr3tuning::TunerIrace`](https://github.com/mlr-org/mlr3tuning/blob/master/R/TunerIrace.R) for an implementation of `$.assign_result()`.\n\n### Transform optimizer to tuner {#tuner-from-optimizer}\n\nThis step is only needed if you implement via [bbotk](https://bbotk.mlr-org.com).\nThe `mlr3tuning::TunerFromOptimizer` class transforms a [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html) to a [`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html).\nJust add the [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html) to the `optimizer` field.\nSee [`mlr3tuning::TunerRandomSearch`](https://github.com/mlr-org/mlr3tuning/blob/master/R/TunerRandomSearch.R) for an example.\n\n### Add unit tests {#tuner-test}\n\nThe new custom tuner should be thoroughly tested with unit tests.\n[`Tuner`](https://mlr3tuning.mlr-org.com/reference/Tuner.html)s can be tested with the `test_tuner()` helper function.\nIf you added the Tuner via a [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html), you should additionally test the [`Optimizer`](https://bbotk.mlr-org.com/reference/Optimizer.html) with the `test_optimizer()` helper function.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}