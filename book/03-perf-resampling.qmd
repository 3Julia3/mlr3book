# Resampling {#sec-resampling}

{{< include _setup.qmd >}}

When evaluating the performance of a model, we are interested in its generalization performance -- how well will it perform on new data that has not been seen during training?
We can estimate the generalization performance by evaluating a model on a test set, as we have done above, that was created to contain only observations that are not contained in the training set.
There are many different strategies for partitioning a data set into training and test; in `r mlr_pkg("mlr3")` we call these strategies "resampling".
`r mlr_pkg("mlr3")` includes the following predefined [resampling](#resampling) strategies:

| Name | Identifier |
|:---|:---|
| `r ref("mlr_resamplings_cv", text = "cross validation")` | `"cv"` |
| `r ref("mlr_resamplings_loo", text = "leave-one-out cross validation")` | `"loo"` |
| `r ref("mlr_resamplings_repeated_cv", text = "repeated cross validation")` | `"repeated_cv"` |
| `r ref("mlr_resamplings_bootstrap", text = "bootstrapping")`| `"bootstrap"` |
| `r ref("mlr_resamplings_subsampling", text = "subsampling")` | `"subsampling"` |
| `r ref("mlr_resamplings_holdout", text = "holdout")` | `"holdout"` |
| `r ref("mlr_resamplings_insample", text = "in-sample resampling")`| `"insample"` |
| `r ref("mlr_resamplings_custom", text = "custom resampling")` | `"custom"` |

:::{.callout-note}
It is often desirable to repeatedly split the entire data in different ways to ensure that a "lucky" or "unlucky" split does not bias the generalization performance estimate.
Without resampling strategies like the ones we provide here, this is a tedious and error-prone process.
:::

The following sections provide guidance on how to select a resampling strategy and how to use it.

Here is a graphical illustration of the resampling process in general:

```{r 03-perf-resampling-001, echo=FALSE, fig.align="center"}
knitr::include_graphics("images/ml_abstraction.svg")
```

## Settings {#resampling-settings}

We will use the `r ref("mlr_tasks_penguins", text = "penguins")` task and a simple classification tree from the `r cran_pkg("rpart")` package as an example here.

```{r 03-perf-resampling-002}
task = tsk("penguins")
learner = lrn("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach to use.
`r mlr_pkg("mlr3")` resampling strategies and their parameters can be queried by looking at the `data.table` output of the `r ref("mlr_resamplings")` dictionary; this also lists the parameters that can be changed to affect the behavior of each strategy:

```{r 03-perf-resampling-003}
as.data.table(mlr_resamplings)
```

:::{.callout-tip}
Additional resampling methods for special use cases are available via extension packages, such as `r gh_pkg("mlr-org/mlr3spatiotemporal")` for spatial data.
:::

What we showed in the [train/predict/score](#train-predict) part is the equivalent of holdout resampling, done manually, so let's consider this one first.
We can retrieve elements from the dictionary `r ref("mlr_resamplings")` with the convenience function `r ref("rsmp()")`:

```{r 03-perf-resampling-004}
resampling = rsmp("holdout")
print(resampling)
```

:::{.callout-note}
Note that the `$is_instantiated` field is set to `FALSE`.
This means we did not apply the strategy to a dataset yet.
:::

By default we get a .66/.33 split of the data into training and test.
There are two ways in which this ratio can be changed:

1. Overwriting the slot in `$param_set$values` using a named list:

```{r 03-perf-resampling-005}
resampling$param_set$values = list(ratio = 0.8)
```

2. Specifying the resampling parameters directly during construction:

```{r 03-perf-resampling-006}
rsmp("holdout", ratio = 0.8)
```

## Instantiation {#resampling-inst}

So far, we have only chosen a resampling strategy; we now need to instantiate it with data.

To perform the splitting and obtain indices for the training and the test split, the resampling needs a `r ref("Task")`.
By calling the method `instantiate()`, we split the row indices of the task into indices for training and test sets.
These resulting indices are stored in the `r ref("Resampling")` objects.

```{r 03-perf-resampling-007}
resampling$instantiate(task)
train = resampling$train_set(1)
test = resampling$test_set(1)
str(train)
str(test)

# are they disjunct?
intersect(train, test)

# are all row ids either in train or test?
setequal(c(train, test), task$row_ids)
```

Note that if you want to compare multiple [Learners](#learners) in a fair manner, using the same instantiated resampling for each learner is mandatory, such that each learner gets exactly the same training data and the performance of the trained model is evaluated in exactly the same test set.
A way to greatly simplify the comparison of multiple learners is discussed in the [section on benchmarking](#benchmarking).


## Execution {#resampling-exec}

With a `r ref("Task")`, a `r ref("Learner")`, and a `r ref("Resampling")` object we can now perform a resampling: fit the learner on a subset of the task repeatedly and predict on the left-out observations.
For this, we call the `r ref("resample()")` function which returns a `r ref("ResampleResult")` object.
We can tell `r ref("resample()")` to keep the fitted models (for example for later inspection) by setting the `store_models` option to `TRUE`.

:::{.callout-note}
The `r cran_pkg("butcher")` package allows you to remove components of a fitted model that may no longer be needed.
However, from the point of view of the mlr3 framework, it is impossible to find a good compromise here.
Of course, as much balast as possible should be removed.
However, if too much is removed, some graphics, outputs or third-party extensions may not work anymore as intended.
Our conservative policy here is that the training data should not be part of the stored model.
If this is the case somewhere, please [open an issue](https://github.com/mlr-org/mlr3/issues).

To manually apply butcher, just replace the stored model in the learner:

```{r 03-perf-resampling-008, eval = FALSE}
task = tsk("penguins")
learner = lrn("classif.rpart")
learner$train(task)

learner$model = butcher::butcher(learner$model)
```
:::

Per default, mlr3 discards fitted models after the prediction step to keep the `r ref("ResampleResult")` object small w.r.t. its memory consumption.

```{r 03-perf-resampling-009}
task = tsk("penguins")
learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling = rsmp("cv", folds = 3)

rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

Here we use a three-fold cross-validation resampling, which trains and evaluates on three different training and test sets.
The returned `r ref("ResampleResult")`, stored as `rr`, provides various getters to access and aggregate the stored information.
Here are a few examples:

- Calculate the average performance across all resampling iterations, in terms of classification error:

  ```{r 03-perf-resampling-010}
  rr$aggregate(msr("classif.ce"))
  ```

  This is useful to check if one (or more) of the iterations are very different from the average.

- Check for warnings or errors:

  ```{r 03-perf-resampling-011}
  rr$warnings
  rr$errors
  ```

- Extract and inspect the resampling splits; this allows us to see in detail which observations were used for what purpose when:

  ```{r 03-perf-resampling-012}
  rr$resampling
  rr$resampling$iters
  str(rr$resampling$train_set(1))
  str(rr$resampling$test_set(1))
  ```

- Retrieve the model trained in a specific iteration and inspect it, for example, to investigate why the performance in this iteration was very different from the average:

  ```{r 03-perf-resampling-013}
  lrn = rr$learners[[1]]
  lrn$model
  ```

- Extract the individual predictions:

  ```{r 03-perf-resampling-014}
  rr$prediction() # all predictions merged into a single Prediction object
  rr$predictions()[[1]] # predictions of first resampling iteration
  ```

- Filter the result, keep only specified resampling iterations:

  ```{r 03-perf-resampling-015}
  rr$filter(c(1, 3))
  print(rr)
  ```

## Custom resampling {#resamp-custom}

Sometimes it is necessary to perform resampling with custom splits, e.g., to reproduce results reported in a study.
A manual resampling instance can be created using the `"custom"` template.

```{r 03-perf-resampling-016}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:10, 51:60, 101:110)),
  test = list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

## Resampling with (predefined) groups

In some cases, it is desirable to keep observations together, i.e., either have all observations of the same group in the training set or have all observations of the same group in the test set.
This can be defined through the column role `"group"` during Task creation, i.e., a column in the data specifies the groups (see also the [help page](https://mlr3.mlr-org.com/reference/Resampling.html#grouping-blocking) on this column role).

Suppose you want the ensure that the observations in the training and test set are similarly distributed as in the complete task.
In that case, you can mark one or multiple columns for stratification with the role `stratum`.

See also the [mlr3gallery post on this topic](https://mlr3gallery.mlr-org.com/posts/2020-03-30-stratification-blocking/) for a practical introduction.

:::{.callout-note}
In the old `mlr` package, grouping was called "blocking".
:::

## Plotting Resample Results {#autoplot-resampleresult}

`r mlr_pkg("mlr3viz")` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method for resampling results.
As an example, we create a binary classification task with two features, perform a resampling with a 10-fold cross-validation and visualize the results:

```{r 03-perf-resampling-017}
task = tsk("pima")
task$select(c("glucose", "mass"))
learner = lrn("classif.rpart", predict_type = "prob")
rr = resample(task, learner, rsmp("cv"), store_models = TRUE)
rr$score(msr("classif.auc"))

# boxplot of AUC values across the 10 folds
library("mlr3viz")
autoplot(rr, measure = msr("classif.auc"))

# ROC curve, averaged over 10 folds
autoplot(rr, type = "roc")
```

We can also plot the predictions of individual models:

```{r 03-perf-resampling-018}
# learner predictions for the first fold
rr1 = rr$clone()$filter(1)
autoplot(rr1, type = "prediction")
```

:::{.callout-tip}
All available plot types are listed on the manual page of `r ref("autoplot.ResampleResult()")`.
:::
