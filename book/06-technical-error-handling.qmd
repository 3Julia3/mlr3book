# Error Handling {#error-handling}

{{< include _setup.qmd >}}

To demonstrate how to properly deal with misbehaving learners, `r mlr_pkg("mlr3")` ships with the learner `r ref("mlr_learners_classif.debug", "classif.debug")`:

```{r 06-technical-error-handling-001}
task = tsk("iris")
learner = lrn("classif.debug")
print(learner)
```

This learner comes with special hyperparameters that let us control

1. what conditions should be signaled (message, warning, error, segfault) with what probability.
1. during which stage the conditions should be signaled (train or predict).
1. the ratio of predictions being `NA` (`predict_missing`).

```{r 06-technical-error-handling-002}
learner$param_set
```

With the learner's default settings, the learner will do nothing special: The learner remembers a random label and creates constant predictions.

```{r 06-technical-error-handling-003}
task = tsk("iris")
learner$train(task)$predict(task)$confusion
```

We now set a hyperparameter to let the debug learner signal an error during the train step.
By default, `r gh_pkg("mlr-org/mlr3")` does not catch conditions such as warnings or errors raised by third-party code like learners:

```{r 06-technical-error-handling-004, error = TRUE}
learner$param_set$values = list(error_train = 1)
learner$train(tsk("iris"))
```
If this would be a regular learner, we could now start debugging with `r ref("traceback()")` (or create a [MRE](https://stackoverflow.com/help/minimal-reproducible-example) to file a bug report).

However, machine learning algorithms raising errors is not uncommon as algorithms typically cannot process all possible data.
Thus, we need a mechanism to

  1. capture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc, and
  1. a statistically sound way to proceed the calculation and be able to aggregate over partial results.

These two mechanisms are explained in the following subsections.

## Encapsulation {#encapsulation}

With encapsulation, exceptions do not stop the program flow and all output is logged to the learner (instead of printed to the console).
Each `r ref("Learner")` has a field `encapsulate` to control how the train or predict steps are executed.
One way to encapsulate the execution is provided by the package `r cran_pkg("evaluate")` (see the documentation of the `r ref("encapsulate()")` helper function for more details):

```{r 06-technical-error-handling-005}
task = tsk("iris")
learner = lrn("classif.debug")
learner$param_set$values = list(warning_train = 1, error_train = 1)
learner$encapsulate = c(train = "evaluate", predict = "evaluate")

learner$train(task)
```

After training the learner, one can access the recorded log via the fields `log`, `warnings` and `errors`:

```{r 06-technical-error-handling-006}
learner$log
learner$warnings
learner$errors
```

Another method for encapsulation is implemented in the `r cran_pkg("callr")` package.
`r cran_pkg("callr")` spawns a new R process to execute the respective step and thus even guards the current R session against segfaults.
On the downside, starting new processes comes with comparably more computational overhead.

```{r 06-technical-error-handling-007}
learner$encapsulate = c(train = "callr", predict = "callr")
learner$param_set$values = list(segfault_train = 1)
learner$train(task = task)
learner$errors
```

Without a model, it is not possible to get predictions though:

```{r 06-technical-error-handling-008, error = TRUE}
learner$predict(task)
```

To handle the missing predictions in a graceful way during `r ref("resample()")` or `r ref("benchmark()")`, fallback learners are introduced next.

## Fallback learners

Fallback learners have the purpose of allowing scoring results in cases where a `r ref("Learner")` is misbehaving.
We will first handle the case that a learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory.
There are three possibilities to proceed:

1. Ignore missing scores.
  Although this is arguably the most frequent approach in practice, this is not statistically sound.
  For example, consider the case where a researcher wants a specific learner to look better in a benchmark study.
  To do this, the researcher takes an existing learner but introduces a small adaptation: If an internal goodness-of-fit measure is not achieved, an error is thrown.
  In other words, the learner only fits a model if the model can be reasonably well learned on the given training data.
  In comparison with the learning procedure without this adaptation and a good threshold, however, we now compare the mean over only the "easy" splits with the mean over all splits - an unfair advantage.
2. Penalize failing learners.
  If a score is missing, we can simply impute the worst possible score (as defined by the `r ref("Measure")`) and thereby heavily penalize the learner for failing.
  However, this often seems too harsh for many problems.
3. Impute a value that corresponds to a (weak) baseline.
  Instead of imputing with the worst possible score, impute with a reasonable baseline, e.g., by just predicting the majority class or the mean of the training data with a featureless learner.
  Note that the imputed value depends on the data in the training split.
  Retrieving the right values after a larger benchmark is possible but tedious.


We strongly recommend option (3).
To make this procedure very convenient during resampling and benchmarking, we support fitting a proper baseline with a fallback learner.
In the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner.
So whenever the debug learner fails (which is every single time with the given parametrization) and encapsulation is enabled, `r mlr_pkg("mlr3")` falls back to the predictions of the featureless learner internally:

```{r 06-technical-error-handling-009}
task = tsk("iris")
learner = lrn("classif.debug")
learner$param_set$values = list(error_train = 1)
learner$fallback = lrn("classif.featureless")
learner$train(task)
learner
```

Note that we don't have to enable encapsulation explicitly; it is automatically set to `"evaluate"` for the training and the predict step while setting a fallback learner for a learner without encapsulation enabled.
Furthermore, the log contains the captured error (which is also included in the print output), and although we don't have a model, we can still get predictions:

```{r 06-technical-error-handling-010}
learner$model
prediction = learner$predict(task)
prediction$score()
```

In this this stepwise train-predict procedure, the fallback learner is of limited use.
However, it is invaluable for larger benchmark studies.

In the following snippet we compare the previously created debug learner with a simple classification tree.
We re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step:

```{r 06-technical-error-handling-011}
learner$param_set$values = list(error_train = 0.3)

bmr = benchmark(benchmark_grid(tsk("iris"), list(learner, lrn("classif.rpart")), rsmp("cv")))
aggr = bmr$aggregate(conditions = TRUE)
aggr
```

Even though the debug learner occasionally failed to provide predictions, we still got a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree.
To further investigate the errors, we can also extract the `"ResampleResult")`:

```{r 06-technical-error-handling-012}
rr = aggr[learner_id == "classif.debug"]$resample_result[[1L]]
rr$errors
```


A similar problem emerges when a learner predicts only a subset of the observations in the test set (and predicts `NA` for others).
A typical case is, e.g., when new and unseen factor levels are encountered in the test data.
Imagine again that our goal is to benchmark two algorithms using a cross validation on some binary classification task:

* Algorithm A is an ordinary logistic regression.
* Algorithm B is also an ordinary logistic regression, but with a twist:
  If the logistic regression is rather certain about the predicted label (> 90% probability), it returns the label and returns a missing value otherwise.

Clearly, at its core, this is the same problem as outlined before.
Algorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for all observations.
Long story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner.
This is illustrated in the following example:
```{r 06-technical-error-handling-013}
task = tsk("iris")
learner = lrn("classif.debug")

# this hyperparameter sets the ratio of missing predictions
learner$param_set$values = list(predict_missing = 0.5)

# without fallback
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")

# with fallback
learner$fallback = lrn("classif.featureless")
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")
```

Summed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or unstable learning algorithms in a convenient and statistically sound fashion.
