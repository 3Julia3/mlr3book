---
author:
  - name: Raphael Sonabend
    orcid: 0000-0001-9225-4654
    email: raphaelsonabend@gmail.com
    affiliations:
      - name: Imperial College London
  - name: Patrick Schratz
    orcid: 0000-0003-0748-6624
abstract: Previous chapters have considered the core functionality of functions and features in the mlr3 universe and other helper packages, which altogether create a modular ecosystem. This chapter highlights the power of that modularity by introducing implemented machine learning tasks beyond regression and classification. The chapter begins with an overview to mlr3proba, which is an extension for probabilistic supervised learning. mlr3proba currently includes survival analysis, density estimation, and some support for probabilistic regression. Next, the chapter looks at the packages mlr3spatial and mlr3spatiotempcv, which combined implement spatial and spatiotemporal analysis. This section also includes detailed consideration to custom cross-validation required for spatiotemporal modelling. Finally, mlr3cluster is introduced, which includes support for unsupervised clustering. As each of these tasks may be new to readers, we include an overview to the theory behind each, before looking at the extensions to common mlr3 objects (learners, predictions, tasks, and measures), and then finally consider task-specific extensions.
---

```{r, include = FALSE}
set.seed(8)
```

# Beyond Deterministic Regression and Single-Label Classification {#sec-special}

{{< include _setup.qmd >}}

This chapter title is a mouthful, but it highlights the limited scope we have considered up until now.
So far, this book has focused on detailed explanation of the `r mlr3` universe of packages, which is abstracted in such a way that allows users to choose a level of complexity for your project that suits you.
For example, you could limit your usage of these packages to just `r mlr3` to train a very small subset of regression and classification learners on a set of data and then use your model to make predictions for new unseen data.
You could extend this to many regression and classification learners with `r mlr3learners` and `r mlr3extralearners`.
By making use of `r mlr3tuning` and `r mlr3pipelines` you could optimise your model further including with pre- and post-processing of data and algorithms.
However, the real power of the `r mlr3` universe, is in its ability to take all of these features, and extend them to any machine learning task.
In @sec-basics we introduced deterministic regression and deterministic & probabilistic single-label classification (@tbl-alltasks).
But our infrastructure also works well for survival analysis, density estimation, clustering, spatiotemporal analysis, time-series forecasting, deep learning architectures, image analysis, multi-label classification, probabilistic regression, and many many more.
Some of these tasks are implemented in extension packages as part of the `r mlr3` universe (see @fig-mlr3verse), some are available by creating pipelines with `r mlr3pipelines`, some are in development, and some are waiting to be developed (maybe by some readers of this book).
In this chapter, we will take you through just a subset of these new tasks, focusing on the ones that have a stable API.
As we work through this chapter we will refer to the 'building blocks' of `r mlr3`, this refers to the base classes that must be extended to create new tasks, these are `r ref("Prediction")`, `r ref("Learner")`, `r ref("Measure")`, and `r ref("Task")`.
In many cases, to implement a new machine learning paradigm, all you need to do is create a class that extends the above and then implement new concrete learners that inherit from these (read @sec-extending if you are interested in extending new tasks or adding new learners and/or measures).

@tbl-alltasks summarises available extension tasks, including the package they are implemented in, the task status, and the suffix used in the building block functions.
Only tasks with stable APIs are discussed in this chapter, the table also includes tasks in development.

| Task | Package | Status | Description |
| ---- | ------- | ------ | --------------- |
| Deterministic regression | `r mlr3` | Stable | Point prediction of a continuous variable. |
| Probabilistic regression | `r mlr3proba` | Stable | Probability distribution prediction of a continuous variable. |
| Deterministic single-label classification | `r mlr3` | Stable | Prediction of a single category an observation falls into. |
| Probabilistic single-label classification | `r mlr3` | Stable | Prediction of the probability of an observation falling into one or more mutually exclusive categories. |
| Cost-sensitive classification | `r mlr3` and `r mlr3pipelines` | Stable | Classification predictions with unequal costs associated with misclassifications. |
| Survival analysis | `r mlr3proba` | Stable | Time-to-event predictions with possible 'censoring'. |
| Density estimation | `r mlr3proba` | Stable | Unsupervised estimation of probability density functions. |
| Spatiotemporal analysis | `r mlr3spatiotempcv` and `r mlr3spatial` | Stable | Supervised prediction of data with spatial (e.g., coordinates) and/or temporal outcomes. |
| Cluster analysis | `r mlr3cluster` | Stable | Unsupervised estimation of homogeneous clusters of data points. |
| Time series forecasting | `mlr3temporal` | Developing | Supervised predictions of target over time (e.g., daily temperature). |
| Functional data analysis | `mlr3fda` | Developing | Supervised predictions from functional data. |

: Table of extension tasks that can be used with `r mlr3` infrastructure. 'Stable' tasks have an interface that is unlikely to undergo any major changes in the future, these tasks are discussed in this chapter. 'Developing' tasks are in packages that might have breaking API changes. As we have a growing community of contributors, this list is far from exhaustive and many 'experimental' task implementations exist; this list just represents the tasks that have a functioning interface. {#tbl-alltasks}

## Cost-Sensitive Classification {#cost-sens}

We begin by discussing a task that does not require any additional packages or infrastructure, only the tools we have already learnt about from earlier chapters.
In regular classification, the aim is to minimize the misclassification rate, and thus all types of misclassification errors are deemed equally severe.
A more general setting is cost-sensitive classification.
Cost-sensitive classification does not assume that the costs caused by different kinds of errors are equal.
The objective of cost-sensitive classification is to minimize the expected costs.

Imagine you are an analyst for a big credit institution.
Let's also assume that a correct bank decision would result in 35% of the profit at the end of a specific period.
A correct decision means that the bank predicts that a customer will pay their bills (hence would obtain a loan), and the customer indeed has good credit.
On the other hand, a wrong decision means that the bank predicts that the customer's credit is in good standing, but the opposite is true.
This would result in a loss of 100% of the given loan.

|                           | Good Customer (truth)       | Bad Customer (truth)       |
| :-----------------------: | :-------------------------: | :------------------------: |
| Good Customer (predicted) | + 0.35                      | - 1.0                      |
| Bad Customer (predicted)  | 0                           | 0                          |


Expressed as costs (instead of profit), we can write down the cost-matrix as follows:

```{r special-022}
costs = matrix(c(-0.35, 0, 1, 0), nrow = 2)
dimnames(costs) = list(response = c("good", "bad"), truth = c("good", "bad"))
print(costs)
```

An exemplary data set for such a problem is the `r ref("mlr_tasks_german_credit", text = "German Credit")` task:

```{r special-023}
library("mlr3verse")
task = tsk("german_credit")
table(task$truth())
```

The data has 70% of customers who can pay back their credit and 30% of bad customers who default on their debt.
A manager, who doesn't have any model, could decide to give either everybody credit or to give nobody credit.
The resulting costs for the German credit data are:

```{r special-024}
# nobody:
(700 * costs[2, 1] + 300 * costs[2, 2]) / 1000

# everybody
(700 * costs[1, 1] + 300 * costs[1, 2]) / 1000
```

If the average loan is $20,000, the credit institute will lose more than one million dollars if it would grant everybody a credit:

```{r special-025}
# average profit * average loan * number of customers
0.055 * 20000 * 1000
```

Our goal is to find a model that minimizes the costs (and maximizes the expected profit).

### A First Model

For our first model, we choose an ordinary logistic regression (implemented in add-on package `r mlr3learners`).
We first create a classification task, then resample the model using 10-fold cross-validation and extract the resulting confusion matrix:

```{r special-026}
learner = lrn("classif.log_reg")
rr = resample(task, learner, rsmp("cv"))

confusion = rr$prediction()$confusion
print(confusion)
```

To calculate the average costs like above, we can simply multiply the elements of the confusion matrix with the elements of the previously introduced cost matrix and sum the values of the resulting matrix:

```{r special-027}
avg_costs = sum(confusion * costs) / 1000
print(avg_costs)
```

With an average loan of \$20,000, the logistic regression yields the following costs:

```{r special-028}
avg_costs * 20000 * 1000
```

Instead of losing over \$1,000,000, the credit institute now can expect a profit of more than \$1,000,000.

### Cost-sensitive Measure

Our natural next step would be to further improve the modeling step in order to maximize the profit.
For this purpose, we first create a cost-sensitive classification measure that calculates the costs based on our cost matrix.
This allows us to conveniently quantify and compare modeling decisions.
Fortunately, there already is a predefined measure `r ref("Measure")` for this purpose: `r ref("MeasureClassifCosts")`.
The costs have to be provided as a numeric matrix whose columns and rows are named with class labels (just like the previously constructed `costs` matrix):

```{r special-029}
cost_measure = msr("classif.costs", costs = costs)
print(cost_measure)
```

If we now call `r ref("resample()")` or `r ref("benchmark()")`, the cost-sensitive measures will be evaluated.
We compare the logistic regression to a simple featureless learner and to a random forest from package `r ref_pkg("ranger")` :

```{r special-030}
learners = list(
  lrn("classif.log_reg"),
  lrn("classif.featureless"),
  lrn("classif.ranger")
)
cv10 = rsmp("cv", folds = 10)
bmr = benchmark(benchmark_grid(task, learners, cv10))
autoplot(bmr, measure = cost_measure) + ggplot2::geom_hline(yintercept = 0, colour = "red")
```

As expected, the featureless learner is performing comparably badly.
The logistic regression and the random forest both yield a profit on average.

### Thresholding

Although we now correctly evaluate the models in a cost-sensitive fashion, the models themselves are unaware of the classification costs.
They assume the same costs for both wrong classification decisions (false positives and false negatives).
Some learners natively support cost-sensitive classification (e.g., XXX).
However, we will concentrate on a more generic approach that works for all models which can predict probabilities for class labels: thresholding.

Most learners can calculate the probability $p$ for the positive class.
If $p$ exceeds the threshold $0.5$, they predict the positive class and the negative class otherwise.

For our binary classification case of the credit data, we primarily want to minimize the errors where the model predicts "good", but truth is "bad" (i.e., the number of false positives) as this is the more expensive error.
If we now increase the threshold to values $> 0.5$, we reduce the number of false negatives.
Note that we increase the number of false positives simultaneously, or, in other words, we are trading false positives for false negatives.

```{r special-031}
# fit models with probability prediction
learner = lrn("classif.log_reg", predict_type = "prob")
rr = resample(task, learner, rsmp("cv"))
p = rr$prediction()
print(p)

# helper function to try different threshold values interactively
with_threshold = function(p, th) {
  p$set_threshold(th)
  list(confusion = p$confusion, costs = p$score(measures = cost_measure))
}

with_threshold(p, 0.5)
with_threshold(p, 0.75)
with_threshold(p, 1.0)
```

There is also an `autoplot()` method which systematically varies the threshold between 0 and 1 and calculates the corresponding scores:
```{r}
autoplot(p, type = "threshold", measure = cost_measure)
```

Instead of manually or visually searching for good values, the base R function `r ref("optimize()")` can do the job for us:

```{r special-032}
# simple wrapper function that takes a threshold and returns the resulting model performance
# this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1]
f = function(th) {
  with_threshold(p, th)$costs
}
best = optimize(f, c(0.5, 1))
print(best)

# optimized confusion matrix:
with_threshold(p, best$minimum)$confusion
```

Note that the function `"optimize()"` is intended for unimodal functions and, therefore, may converge to a local optimum here.
See the next section for better alternatives to tune the threshold.

### Threshold Tuning

Currently `r mlr3pipelines` offers two main strategies towards adjusting `classification thresholds`.
We can either expose the thresholds as a `hyperparameter` of the Learner by using `r ref("PipeOpThreshold")`.
This allows us to tune the `thresholds` via an outside optimizer from `r mlr3tuning`.
Alternatively, we can also use `r ref("PipeOpTuneThreshold")` which automatically tunes the threshold after each learner is fit.
Both methods are described in the following subsections.

### PipeOpThreshold

`r ref("PipeOpThreshold")` can be put directly after a `r ref("Learner")`.

A simple example would be:

```{r special-034}
gr = lrn("classif.rpart", predict_type = "prob") %>>% po("threshold")
l = as_learner(gr)
```

Note, that `predict_type = "prob"` is required for `po("threshold")` to have any effect.

The `thresholds` are now exposed as a `hyperparameter` of the `r ref("GraphLearner")` we created:

```{r special-035}
l$param_set
```

We can now tune those thresholds from the outside as follows:

Before tuning, we have to define which hyperparameters we want to tune over.
In this example, we only tune over the `thresholds` parameter of the `threshold` pipe operator.
You can easily imagine that we can also jointly tune over additional hyperparameters, i.e., rpart's `cp` parameter.

As the `r ref("Task")` we aim to optimize for is a binary task, we can simply specify the threshold param:

```{r special-036}
library("paradox")
ps = ps(threshold.thresholds = p_dbl(lower = 0, upper = 1))
```

We now create a `r ref("AutoTuner")` which automatically tunes the supplied learner over the `r ref("ParamSet")` we supplied above.

```{r special-037}
at = AutoTuner$new(
  learner = l,
  resampling = rsmp("cv", folds = 3L),
  measure = msr("classif.ce"),
  search_space = ps,
  terminator = trm("evals", n_evals = 5L),
  tuner = tnr("random_search")
)

at$train(tsk("german_credit"))
```

Inside the `trafo`, we simply collect all set params into a named vector via `map_dbl` and store it
in the `threshold.thresholds` slot expected by the learner.

Again, we create a `r ref("AutoTuner")`, which automatically tunes the supplied learner over the `r ref("ParamSet")` we supplied above.

One drawback of this strategy is that this requires us to fit a new model for each new threshold setting.
While setting a threshold and computing performance is relatively cheap, fitting the learner is often
more computationally demanding.
An often better strategy is, therefore, to optimize the thresholds separately after each model fit.

### PipeOpTunethreshold

`r ref("PipeOpTuneThreshold")` on the other hand works together with `r ref("PipeOpLearnerCV")`.
It directly optimizes the `cross-validated` predictions made by this `r ref("PipeOp")`.
This is necessary to avoid over-fitting the threshold tuning.

A simple example would be:

```{r special-038}
gr = po("learner_cv", lrn("classif.rpart", predict_type = "prob")) %>>% po("tunethreshold")
l2 = as_learner(gr)
```

Note, that `predict_type` = "prob" is required for `po("tunethreshold")` to work.
Additionally, note that this time no `threshold` parameter is exposed, and it is automatically tuned internally.

```{r special-039}
l2$param_set
```

Note that we can set `rsmp("intask")` as a resampling strategy for "learner_cv" in order to evaluate
predictions on the "training" data.
This is generally not advised, as it might lead to over-fitting on the thresholds but can significantly reduce runtime.

For more information, see the post on Threshold Tuning on the [mlr3 gallery](https://mlr3gallery.mlr-org.com/).

## Probabilistic Regression {#sec-probregr}

`r index("Probabilistic regression")` is a simple extension to deterministic regression in which the goal is to predict a full distribution around a single '`r index("point prediction")`'.
By example, instead of just predicting someone's height, $\mu$, we could also predict how confident we are about our prediction, usually estimated as a standard error, $\sigma$.
This 'confidence' could be expressed as a confidence interval, for example $\mu \pm \sigma$, or by assuming a distribution, for example $Norm(\mu = 120, \sigma = 2)$.

### Confidence interval predictions

Predicting a `r index("confidence interval")` is possible using features in `r mlr3`.
By setting `predict_type = "se"` for any `r ref("LearnerRegr")` object, enables predictions of the `r index("point prediction")` (`mean`) as well as measure of confidence for that prediction (`se`).
These can be manually combined to create a confidence interval.
An example is in @fig-probregrconfint where we assume a symmetric Normal 95% confidence interval.

```{r, warning=FALSE}
#| fig-alt: x-label says "Response plus-minus 1.96se" and y-label says "Truth". Plot shows a black line running through the x=y axis. 7 blue points are plotted with error bars. Plot shows that 3 points are lying on the x=y line. Other points are not on the line and their error bars do not overlap with the line either.
#| fig-cap: Scatter plot of predictions (x) vs truth (y) where the straight line is $x=y$. Points lying on $x=y$ indicate very good predictions, points with error bars overlapping $x=y$ indicates truth is within 95% confidence interval.
#| label: fig-probregrconfint
t = tsk("mtcars")
split = partition(t, 0.8)
l = lrn("regr.ranger", predict_type = "se")
l$train(t, split$train)
p = l$predict(t, split$test)
autoplot(p, type = "confidence", quantile = 1.96)
```

This representation provides more detail about our prediction by including our confidence about the predictions.
One of the ways in which this is helpful is in telling us how 'far' our predictions are from the truth.
For example the top prediction in @fig-probregrconfint has wide error bars and is further from the truth than the third prediction which has narrower error bars and is closer to the truth.
Analogously to classification, we might think of the first prediction as "P(Y = 28) = 0.4" versus the third prediction as "P(Y = 24) = 0.8" (numbers not exact).
This level of detail allows more nuanced metrics to be utilised to measure probabilistic predictions, which we will turn to in the next section.

### Probability distribution predictions

In the example above we used the standard error to calculate an approximate symmetric Normal confidence interval.
Instead we could have plugged the estimated `reponse` and `se` into a `r index("probability distribution")` as an estimate of the distribution mean and standard deviation.
By assuming a probability distribution, we can make our predictions even more detailed by not just considering the mean of the distribution (the 'response' in `r mlr3` terminology), but also the probability density function, cumulative distribution function, and all other properties.
`r ref_pkg("distr6")` implements many probability distributions with `r ref_pkg("R6")` and provides a flexible interface that was originally designed to be used with `r mlr3proba`.
Below we demonstrate construction of two distributions (Uniform and Normal) in `r ref_pkg("distr6")` and how to access their methods and properties, below we call the `pdf` (probability density function) and `cdf` (cumulative distribution function) methods and look at the most important properties with `summary`.

```{r}
library(distr6)
response = 5
se = 2

# construct Uniform distribution
unif = dstr("Uniform",
  lower = response - se * sqrt(3),
  upper = response + se * sqrt(3))
# summary of distribution properties
summary(unif)
# call pdf
unif$cdf(1:6)

# construct Normal distribution
norm = dstr("Normal", mean = response, sd = se)
# summary of distribution properties
summary(norm)
# evaluate cdf
norm$pdf(4:8)
```

Full details of the `r ref_pkg("distr6")` interface can be found on the package website (`r link("https://alan-turing-institute.github.io/distr6/")`).
In the above example we manually transformed the `response` and `se` 'predictions' to be compatible with a Uniform distribution construction, but learning these transformations can be time consuming.
`r mlr3proba` offers a pipeline (@sec-pipelines) that incorporates the key steps of distribution prediction for probabilistic regression, which are:

1. Train and predict a learner to make `response` predictions
2. Train and predict a learner to make `se` predictions
3. Select a probability distribution to model these predictions
4. Transform predictions and construct distribution
5. Return prediction

This pipeline, `"probregr"`, is a wrapper around `r ref("mlr3proba::PipeOpProbregr")`, and is showcased in the code below where we specify three inputs: 1) a random forest as the main learner, i.e., the one making the 'usual' regression point prediction; 2) a featureless learner to estimate the standard deviation, which simply predicts the future standard error as the sample standard deviation in the training set; and 3) we assume a Normal distribution for the predicted distribution. We visualise the quality of predictions in @fig-probregrdist.

```{r}
#| fig-alt: Three Normally distributed curves are plotted in blue, red, and green. Each curve has a point of the same colour, representing the true value. The x-label says "x" and the y-label says "f(x)". The point on the green curve is very close to the distribution maximum. The point on the red curve is close to, but not exactly the maximum. The point on the blue curve is about a third of the way down from the top of the curve. The title of the plot says "Logloss = 2.82".
#| fig-cap: Three probability distribution predictions plotted as probability density functions. True values are represented as points. Shows fairly good predictions for the green and red observations as the truth is close to the distribution mean/mode but a less good (but not terrible) prediction for the blue observation. The logloss value is low but could be better (0 is the minimum).
#| label: fig-probregrdist
library(mlr3verse)
library(mlr3proba)
library(ggplot2)

# create our pipeline
l = as_learner(ppl("probregr",
  learner = lrn("regr.ranger"),
  learner_se = lrn("regr.featureless"),
  dist = "Normal")
)

# train and predict
task = tsk("mtcars")
split = partition(task)
l$train(task, split$train)
p = l$predict(task, split$test)
p

plot_probregr(p, 3, "point", "random") +
  ggtitle(sprintf("Logloss = %s", round(p$score(msr("regr.logloss")), 2)))
```

The `$distr` column in our `r ref("PredictionRegr")` contains objects inheriting from `r ref("distr6::Distribution")`, which allows querying of all standard probability distribution methods and properties, as we have seen in the code above.
This example shows that although none of our predictions were perfect -- the truth was not exactly in line with the peak of the predicted distributions -- in each case the truth was at least within an area of reasonable confidence.
In future versions of `r mlr3proba` we will extend functionality further by adding models that can make probabilistic predictions (e.g., with Bayesian methods), more losses (e.g., Brier score), and a dedicated predict type for interval predictions.
We now turn to the next task implemented in `r mlr3proba`, which follows closely from probabilistic regression.

## Survival Analysis {#sec-survival}

Survival analysis is concerned with the prediction or estimation of time-to-event distributions, for example the time until a marathon runner finishes a race.
The predictive problem is unique in that models are trained and tested on data that may include 'censoring'.
Censoring occurs when the event of interest does *not* take place, for example if a marathon runner gives up and does not finish a race.
Instead of throwing away information about censoring events, survival analysis datasets include a status variable that provides information about the 'status' of an observation, so in our marathon runner example we might write their outcome as (4, 1) if they finish the race at 4 hours, otherwise if they give up at 2 hours we would write (2, 1).
The key to understanding survival analysis is that we assume there exists a hypothetical time the marathon runner would have finished if they had not been censored, it is then the job of a survival learner to estimate what the true survival time would have been for a similar runner, assuming they are *not* censored (see @fig-censoring).
Mathematically, this is represented by the hypothetical event time, $Y$, the hypothetical censoring time, $C$, the observed outcome time, $T = min(Y, C)$, the event indicator $\Delta = (T = Y)$, and as usual some features, $X$.
Learners are trained on $(T, \Delta)$ but, critically, make predictions of $Y$ from previously unseen features.
This means that unlike classification and regression, learners are trained on two variables, the time until an event takes place and the status or event indicator.
For convenience, this is often stored using the `r ref("survival::Surv")` object.
An example of this is in the code below, where we randomly generate 6 survival times and 6 event indicators, an outcome with a `+` indicates the outcome is censoring, otherwise it is the event of interest.
```{r}
library(survival)
Surv(runif(6), rbinom(6, 1, 0.5))
```

Readers familiar with survival analysis will recognise that the description above applies specifically to 'right-censoring'.
Currently, this is the only form of censoring available in the `r mlr3` universe hence restricting our discussion to that setting.
For a good introduction to survival analysis see @Collett2014 or for machine learning in survival analysis specifically see @MLSA.

For the remainder of this section we will look at how `r mlr3proba` [@mlr3proba] extends the building blocks of `r mlr3` for survival analysis. We will begin by looking at objects used to construct machine learning tasks for survival analysis, then we will turn to the learners we have implemented to solve these tasks, before looking at measures for evaluating survival analysis predictions, and then finally we will consider how to transform prediction types.

```{r, echo=FALSE}
#| fig-alt:
#| fig-cap: Dead and censored subjects (y-axis) over time (x-axis). Black diamonds indicate true death times and white circles indicate censoring times. Vertical line is the study end time. Subjects 1 and 2 die in the study time. Subject 3 is censored in the study and (unknown) dies within the study time. Subject 4 is censored in the study and (unknown) dies after the study. Subject 5 dies after the end of the study. Figure and caption from @Sonabend2021b.
#| label: fig-censoring
library(ggplot2)
ggplot(data.frame(x = c(1, 2, 5, 7, 10), y = 1:5), aes(x = x, y = y)) +
  geom_blank() +
  geom_vline(xintercept = 8) +
  geom_segment(y = 1, yend = 1, x = 0, xend = 7) +
  geom_segment(y = 2, yend = 2, x = 0, xend = 8) +
  geom_segment(y = 3, yend = 3, x = 0, xend = 4) +
  geom_segment(y = 3, yend = 3, x = 4, xend = 6, linetype = 2) +
  geom_segment(y = 4, yend = 4, x = 0, xend = 1) +
  geom_segment(y = 4, yend = 4, x = 1, xend = 9, linetype = 2) +
  geom_segment(y = 5, yend = 5, x = 0, xend = 8) +
  geom_segment(y = 5, yend = 5, x = 8, xend = 9, linetype = 2) +
  geom_point(x = 4, y = 3, shape = 21, colour = "black", fill = "white", size = 4) +
  geom_point(x = 6, y = 3, shape = 18, size = 4) +
  geom_point(x = 7, y = 1, shape = 18, size = 4) +
  geom_point(x = 8, y = 2, shape = 18, size = 4) +
  geom_point(x = 9, y = 5, shape = 18, size = 4) +
  geom_point(x = 8, y = 5, shape = 21, size = 4, fill = "white") +
  geom_point(x = 1, y = 4, shape = 21, size = 4, fill = "white") +
  geom_point(x = 9, y = 4, shape = 18, size = 4) +
  labs(y = "Subject", x = "Time") +
  scale_x_continuous(labels = as.character(1:10), breaks = 1:10) +
  theme_minimal() + theme(panel.grid.minor = element_blank())
```

### TaskSurv

As we saw in the introduction to this section, survival algorithms require two targets for training, this means the new `r ref("TaskSurv")` object  expects two targets.
The simplest way to create a survival task is to use `r ref("as_task_surv")`, as seen in the following code chunk.

```{r special-002}
library("mlr3verse")
library("mlr3proba")
library("survival")

task = as_task_surv(survival::rats, time = "time",
  event = "status", type = "right", id = "rats")

print(task)
```

The inputs to `r ref("as_task_surv")` follow `r ref("survival::Surv")`, so in theory there is support for left and interval censoring, as well as for competing risks, however no learners have been implemented at the time of writing that allow for these methods.

In construction, `r ref("TaskSurv")` objects coerce the target columns into a survival object.
We have also included a specific `autoplot` for these objects to quickly create Kaplan-Meier plots to visualise the outcome.

```{r, warning=FALSE, message=FALSE}
head(task$truth())
autoplot(task)
```

We have also included a few pre-loaded tasks in `r mlr3proba` that can be used in experiments

```{r}
as.data.table(mlr_tasks)[task_type == "surv"]
```

### LearnerSurv, PredictionSurv and predict types

The interface for `r ref("LearnerSurv")` and `r ref("PredictionSurv")` objects is identical to the regression and classification settings discussed in @sec-basics.
Similarly to these settings, survival learners are constructed with `"lrn"`, a few of the available learners are listed below.

```{r}
mlr_learners$keys("^surv")
```

The primary difference in constructing a survival learner (and the resulting predictions) is the number of possible `predict_type` options that can be passed to `r ref("lrn")`.

Similarly to deterministic regression (@sec-tasks), a survival model can make a single point prediction, and similarly to probabilistic regression (@sec-probregr), a survival model could also make a prediction of a probability distribution.
`r mlr3proba` has a slightly different design interface to `r mlr3` as all possible types of prediction ('predict types') are returned for all survival models.
The reason for this decision is that all these predict types can be transformed to one another and it is therefore computationally simpler to return all at once instead of rerunning models to change predict type.
In survival analysis, the following predictions can be made:

* `response` - Predicted survival time.
* `distr` - Predicted survival distribution, either discrete or continuous.
* `lp` - Linear predictor calculated as the fitted coefficients multiplied by the test data.
* `crank` - Continuous risk ranking.

We will go through each of these prediction types in more detail and with examples to make them less abstract.
We will use the following setup for most of the examples.
In this chunk we are partitioning the `r ref("survival::rats")` and training a `r define("Cox Proportional Hazards")` model on the training set and making predictions for the predict set.
For this model, all predict types except `response` can be computed.

```{r}
t = tsk("rats")
split = partition(t)
p = lrn("surv.coxph")$train(t, split$train)$predict(t, split$test)
p
```

#### `predict_type = "response"`

Counterintuitively for many, the `response` prediction of predicted survival times is actually the least common predict type in survival analysis.
The likely reason for this is due to the presence of censoring.
We rarely observe the true survival time for many observations and therefore it is unlikely any survival model can confidently make predictions for survival times.
This is illustrated in the code below.
We train and predict from a survival support vector machine from `r ref_pkg("survivalsvm")`, we use `type = "regression"` to select the algorithm that optimises survival time predictions.
We then compare the predictions from the model to the true data.
As can be seen from the output, our predictions are all less than the true observed time, which means we know our model definitely underestimated the truth.
However, because each of the true values are censored times, we have absolutely no way of knowing if these predictions are slightly bad or absolutely terrible, i.e., the true survival times could be $105, 92, 92$ or they could be $300, 1000, 200$.
Hence, with no realistic way to evaluate these models, survival time predictions are rarely useful.
It is also worth noting here that the nature of survival analysis means that it is rare to find high-quality datasets without a substantial amount of censoring.

```{r}
library(mlr3extralearners)
# we use a different learner here as few survival models make a response prediction
pred = lrn("surv.svm", type = "regression", gamma.mu = 1e-3)$
  train(t, split$train)$predict(t, split$test)
data.frame(pred = pred$response[1:3], truth = pred$truth[1:3])
```

#### `predict_type = "distr"`

So unlike regression in which deterministic/point predictions are most common, in survival analysis the most common prediction type is actually distribution predictions.
You will therefore find that the majority of survival models in `r mlr3proba` will make distribution predictions by default.
As with probabilistic regression, we implement these predictions using the `r ref_pkg("alan-turing-institute/distr6")` interface, which allows visualisation and evaluation of survival curves.
In the example below we train a Cox PH model on the `rats` dataset and then plot the survival function of the first three predictions and evaluation this function for all three predictions at $t = 40,70,120$.

```{r}
t = tsk("rats")
split = partition(t)
p = lrn("surv.coxph")$train(t, split$train)$predict(t, split$test)
plot(p$distr[1:3], fun = "survival")
p$distr[1:3]$survival(c(40, 70, 120))
```

#### `predict_type = "lp"`

`lp`, often written as $\eta$ in academic writing, is computationally the simplest prediction and has a natural analogue in regression modelling.
Readers familiar with linear regression will know that when fitting a simple linear regression model, $Y = X\beta$, we are actually estimating the values for $\beta$, and the estimated linear predictor is then $X\hat{\beta}$, where $\hat{\beta}$ are our estimated values.
In simple survival models, the linear predictor is the same quantity (but estimated in a slighty more complicated way).
The implementations in `r mlr3proba` are primarily machine-learning focused and few of these models have a simple linear form, which means that `lp` cannot be computed for most of these.

#### `predict_type = "crank"`

The final prediction type, `crank`, is the most common in survival analysis and also the most confusing.
Academic texts will often refer to 'risk' predictions in survival analysis (hence why survival models are often known as 'risk prediction models'), without defining what 'risk' means.
Often risk is defined as $exp(\eta)$ as this is a common quantity found in simple linear survival models.
However, sometimes risk is defined as $exp(-\eta)$, and sometimes it can be an arbitrary quantity that does not have a meaningful interpretation.
To prevent this confusion in `r mlr3proba`, we define the predict type `crank`, which stands for **c**ontinuous **rank**ing.
This is best explained by example.
Continuing from the previous example we output the first three crank predictions.
The output tells us that the first rat is at the higher risk of death (larger values represent higher risk) and the second rat is at the lowest risk of death.
The distance between predictions also tells us that the difference in risk between the first and second rat is smaller than the difference between the second and third.
The key points in this interpretation are: 1) lower values represent lower risk; 2) the difference between values has meaning but should not be over-interpreted; and 3) the actual values are meaningless.
The last point is the most important, comparing these values between samples is not meaningful.
The reason why this predict type interesting, and so common, is because it allows identifying observations at lower/higher risk to each other, which is useful for resource allocation and prioritisation, and clinical trials (e.g., "are people in a treatment arm at lower risk of X than people in the control arm.").

```{r}
p$crank[1:3]
```

### MeasureSurv

Survival models in `r mlr3proba` are evaluated with `r ref("MeasureSurv")` objects, which are constructed in the usual way with `"msr"`; measures currently implemented are listed below.

```{r}
mlr_measures$keys("^surv")
```

Survival measures will evaluate one of the four prediction types listed above.
In general survival measures can be grouped into the following:

1. Discrimination measures - Quantify if a model correctly identifies if one observation is at higher risk than another. Evaluates `crank` predictions.
1. Calibration measures - Quantify if the average prediction is close to the truth (all definitions of calibration are unfortunately vague in a survival context). May evaluate `crank` or `distr` predictions.
1. Scoring rules - Quantify if probabilistic predictions are close to true values. Evalutes `distr` predictions.

```{r}
head(as.data.table(mlr_measures)[task_type == "surv", c("key", "predict_type")])
```

There is a lot of debate in the literature around the 'best' survival measures to use to evaluate models, as a general rule we recommend `"surv.rcll"` to evaluate the quality of `distr` predictions, `"surv.cindex"` to evaluate a model's discrimination, and `"surv.dcal"` to evaluate a model's calibration.

### Composition

Throughout `r mlr3proba` documentation we refer to "native" and "composed" predictions.
We define a 'native' prediction as the prediction made by a model without any post-processing, whereas a 'composed' prediction is one that is returned after post-processing.

#### Internal composition

In `r mlr3proba` we make use of composition internally to return a `"crank"` prediction for every learner.
This is to ensure that we can meaningfully benchmark all models according to at least one critierion.
The package uses the following rules to create `"crank"` predictions:

1. If a model returns a 'risk' prediction then `crank = risk` (we may multiply this by $-1$ to ensure the 'low value low risk' interpretation).
1. Else if a model returns a `response` prediction then we set `crank = -response`.
1. Else if a model returns a `lp` prediction then we set `crank = lp` (or `crank = -lp` if needed).
1. Else if a model returns a `distr` prediction then we set `crank` as the sum of the cumulative hazard function (see @Sonabend2022 for full discussion as to why we picked this method).

#### Explicit composition and pipelines

At the start of this section we mentioned that it is possible to transform prediction types between each other.
In `r mlr3proba` this is possible with 'compositor' pipelines (@sec-pipelines).
There are a number of pipelines implemented in the package but two in particular focus on predict type transformation:

1. `r ref("pipeline_crankcompositor")` - Transforms a `"distr"` prediction to `"crank"`; and
1. `r ref("pipeline_distrcompositor")` - Transforms a `"lp"` prediction to `"distr"`.

In practice, the second pipeline is more common as we internally use a version of the first pipeline whenever we return predictions from survival models.
You may want to use the first pipeline to overwrite the default method of transforming distributions to rankings.
For now we will just look at the second pipeline.

In the example below we load the `rats` dataset, remove factor columns, and then partition the data into training and testing.
We construct the `distrcompositor` pipeline around a survival GLMnet learner which by default can only make predictions for `"lp"` and `"crank"`.
In the pipeline we specify that we will estimate the baseline distribution with a `r define("Kaplan-Meier")` estimator (`estimator = "kaplan") and that we want to assume a proportional hazards form for our estimated distribution.
We then train and predict in the usual way and in our output we can now see a `distr` prediction.

```{r special-004, eval = FALSE}
library(mlr3verse)
library(mlr3extralearners)

task = tsk("rats")
task$select(c("litter", "rx"))
split = partition(task)

learner = lrn("surv.glmnet")

# no distr output
learner$train(task, split$train)$predict(task, split$test)

pipe = as_learner(ppl(
  "distrcompositor",
  learner = learner,
  estimator = "kaplan",
  form = "ph"
))

# now with distr
pipe$train(task, split$train)$predict(task, split$test)
```

Mathematically, we have done the following:

1. Assume our estimated distribution will have the form $h(t) = h_0(t)exp(\eta)$ where $h$ is the hazard function and $h_0$ is the baseline hazard function.
1. Estimate $\hat{\eta}$ prediction using GLMnet
1. Estimate $\hat{h}_0(t)$ with the Kaplan-Meier estimator
1. Put this all together as $h(t) = \hat{h}_0(t)exp(\hat{\eta})$

For more details about prediction types and compositions we recommend @Kalbfleisch2011.


### Putting it all together {#sec-survival-all}

Finally, let's put all the above into practice in a small benchmark experiment.
We first load the `r ref("grace")` dataset and subset to the first 500 rows.
We then select the right-censored logloss measure to evaluate distribution predictions, setup the same pipeline we used in the previous experiment, and load a Cox PH and Kaplan-Meier estimator.
We run our experiment with 3-fold cross-validation and aggregate the results.

```{r special-005, warning=FALSE}
library(mlr3verse)
library(mlr3extralearners)

task = tsk("grace")$filter(1:500)
measures = msrs(paste0("surv.", c("cindex", "dcalib", "rcll")))

pipe = as_learner(ppl(
  "distrcompositor",
  learner = lrn("surv.glmnet"),
  estimator = "kaplan",
  form = "ph"
))
pipe$id = "Coxnet"
learners = c(lrns(c("surv.coxph", "surv.kaplan")), pipe)

bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measures)[,
  c("learner_id", "surv.cindex", "surv.dcalib", "surv.rcll")]
```

In this small experiment, Coxnet and Cox PH have the best discrimination, Cox PH has the best calibration, and Coxnet and Cox PH seem equally good at overall predictive accuracy.

## Density Estimation {#sec-density}

Density estimation is the learning task to estimate the unknown distribution from which a univariate dataset is generated, or put more simply to estimate the probability density (or mass) function for a variable.
Density estimation is implemented in `r mlr3proba`, similarly to the previous two tasks that can also predict probability distributions (hence the name "**mlr3proba**bilistic").
Unconditional density estimation, i.e., estimation of a target without any covariates, is viewed as an unsupervised task, which means the 'truth' is never known.
For a good overview to density estimation see *Density estimation for statistics and data analysis* [@Silverman1986].

The package `r mlr3proba` extends `r mlr3` with the following objects for density estimation:

* `r ref("mlr3proba::TaskDens", text = "TaskDens")` to define density tasks
* `r ref("mlr3proba::LearnerDens", text = "LearnerDens")` as the base class for density estimators
* `r ref("mlr3proba::PredictionDens", text = "PredictionDens")` for density predictions
* `r ref("mlr3proba::MeasureDens", text = "MeasureDens")` as specialized class for density performance measures

We will consider each in turn.

### TaskDens

As density estimation is an unsupervised task, there is no target for prediction.
In the code below we construct a density task using `r ref("as_task_dens")` which only takes one argument, a `data.frame` type object with exactly one column (which we will use to estimate the underlying distribution).

```{r}
task = as_task_dens(data.frame(x = rnorm(1000)))
task
```

As with other tasks, we have included a couple tasks that come shipped with `r mlr3proba`:

```{r}
as.data.table(mlr_tasks)[task_type == "dens", c(1:2, 4:5)]
```

### LearnerDens and PredictionDens

Density learners can make one of three possible prediction types.

1. `distr` - probability distribution
1. `pdf` - probability density function
1. `cdf` - cumulative distribution function

All learners will return a `distr` and `pdf` prediction but only some can make `cdf` predictions.
Similarly to survival analysis and probabilistic regression, the `distr` predict type is implemented using `r ref_pkg("alan-turing-institute/distr6")`.

```{r}
learn = lrn("dens.hist")
p = learn$train(task, 1:900)$predict(task, 901:1000)
x = seq.int(-2, 2, 0.01)
ggplot(data.frame(x = x, y = p$distr$pdf(x)), aes(x = x, y = y)) +
  geom_line() + theme_minimal()
```

The `pdf` and `cdf` predict types are simply wrappers around `distr$pdf` and `distr$cdf` respectively, which is best demonstrated by example:

```{r special-007}
learn = lrn("dens.hist")
p = learn$train(task, 1:10)$predict(task, 11:13)
p
cbind(p$distr$pdf(task$data()$x[11:13]), p$pdf[1:3])
```

The reason for returning `pdf` and `cdf` in this way is to enable measures to be used to evaluate the quality of our estimations, which we will return to in the next section.

### MeasureDens

Currently the only measure implemented in `r mlr3proba` for density estimation is logloss, which is defined in the same way as in classification and probabilistic regression, $L(y) = -log(f_Y(y))$, where $\hat{f}_Y$ is our estimated probability density function.
Putting this together with the above we are now ready to train a density learner and to estimate a distribution.
Carrying on from before:

```{r}
meas = msr("dens.logloss")
meas

p$score(meas)
```

### Unconditional density estimation vs. probabilistic regression

Some readers may spot parallels between density estimation and probabilistic regression.
In fact, density estimation is just a special case of probabilistic regression (@sec-probregr), occurring when we make an unconditional prediction.
For example, given average rainfall across the USA, estimating the distribution of rainfall would be unsupervised density estimation.
If we now wanted to estimate the distribution of rainfall next year given rainfall this year, then we would be back to supervised probabilistic regression.
This is illustrated in the code below.
We load the `r ref("precip")` dataset and train, predict, and score a Normal kernel density estimation density learner.
We then create a new dataset based on `r ref("precip")` but with an added variable for rainfall next year.
Now we can specify a target for prediction based on another covariate, thus making this a supervised learning task.
We then use the `probregr` pipeline (@sec-probregr) and train, predict, and score our random forest learner.
Note the key difference in these outputs is that whilst both make distribution predictions, the former (density) *estimates* a single probability distribution for the variable of interest, whereas the latter (regression) *predicts* a distribution for each observation.

```{r}
task = tsk("precip")
split = partition(task)
learn = lrn("dens.kde", kernel = "Norm")
p = learn$train(task, split$train)$predict(task, split$test)
p
p$score(msr("dens.logloss"))

df = task$data()
df$next_year = (df$precip * 1.5) + rnorm(length(df$precip))
task = as_task_regr(df, target = "next_year")
pipe = as_learner(ppl("probregr", lrn("regr.ranger"), dist = "Normal"))
p = pipe$train(task, split$train)$predict(task, split$test)
p
p$score(msr("regr.logloss"))
```

### Putting it all together {#sec-density-all}

Finally, we conduct a small benchmark study on the `r ref("mlr_tasks_faithful")` task using some of the integrated density learners:

```{r special-009, message=FALSE, warning=FALSE, results='hide'}
library(mlr3extralearners)
task = tsk("faithful")
learners = lrns(c("dens.hist", "dens.pen", "dens.kde"))
measure = msr("dens.logloss")
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measure)
autoplot(bmr, measure = measure)
```

The results of this experiment show that the sophisticated Penalized Density Estimator does not outperform the baseline histogram, but that the Kernel Density Estimator has at least consistently better (i.e. lower logloss) results.

## Cluster Analysis {#sec-cluster}

`r index("Cluster analysis")` is another unsupervised task implemented in `r mlr3`.
The objective of cluster analysis is to group data into clusters, where each cluster contains similar observations.
The similarity is based on specified metrics that are task and application-dependent.
Unlike classification where we try to predict a category for each observation, in cluster analysis there is no 'true' label or category to predict.

The package `r mlr3cluster` extends `r mlr3` with the following objects for cluster analysis:

* `r ref("mlr3cluster::TaskClust", text = "TaskClust")` to define clustering tasks
* `r ref("mlr3cluster::LearnerClust", text = "LearnerClust")` as base class for clustering learners
* `r ref("mlr3cluster::PredictionClust", text = "PredictionClust")` as specialized class for `r ref("Prediction")` objects
* `r ref("mlr3cluster::MeasureClust", text = "MeasureClust")` as specialized class for performance measures

We will consider each in turn.

### TaskClust

Similarly to density estimation (@sec-density), there is no target for prediction and so no `truth` field in `r ref("TaskClust")`.
In the code below we construct a cluster task using `r ref("as_task_clust")` which only takes one argument, a `data.frame` type object.

```{r warning=FALSE, message=FALSE}
library(mlr3verse)
task = as_task_clust(tsk("penguins_simple")$data()[, -1])
task

autoplot(task)
```

Currently we only have one clustering task shipped with `r mlr3cluster`:

```{r}
as.data.table(mlr_tasks)[task_type == "clust", c(1:2, 4:5)]
```

### LearnerClust and PredictionClust

As with density estimation, we refer to `training` and `predicting` for clustering to be consistent with the `r mlr3` interface, but strictly speaking this should be `clustering` and `assigning` (the latter we will return to shortly).
Two `predict_types` are available for clustering learnings:

1. `partition` - which cluster does an observation fall into
1. `prob` - probability of an observation belonging to each cluster

Hence, similarly to classification, prediction types of clustering learners are either deterministic (`partition`) - estimate the cluster an observation falls into - or probabilistic (`prob`) - estimate the probability of an observation falling into each cluster.

Below we construct a C-Means clustering learner with `prob` prediction type, train it on the `r ref("USArrests")` datasets and then return the cluster assignments (`$assignments`) for each observation - as with other tasks, `learner$model` returns the model representation specificed by the underlying implementation.

```{r}
learner = lrn("clust.cmeans", predict_type = "prob", centers = 3)
learner

learner$train(task)
learner$assignments
```

As clustering is unsupervised, it rarely makes sense to use `predict` for new data however this is still possible using the `r mlr3` interface.
Continuing our example above:

```{r, warning = FALSE, message = FALSE}
# using same data for estimation (rare use case)
learner$train(task, 1:30)$predict(task, 31:32)

# using same data for estimation (common use case)
prediction = learner$train(task)$predict(task)
autoplot(prediction, task)
```

Whilst two prediction types are possible, there are some learners where 'prediction' can never make sense, for example in `r define("hierarchical clustering")`.
In hierarchical clustering, the goal is to build a hierarchy of nested clusters by either splitting large clusters into smaller ones or merging smaller clusters into bigger ones.
The final result is a tree or dendrogram which can change if a new data point is added.
For consistency, `r mlr3cluster` offers `predict` method for hierarchical clusterers but it simply assigns all points to a specified number of clusters by cutting the resulting tree at a corresponding level.

```{r}
learner = lrn("clust.hclust")
learner$train(task)
learner$predict(task)
autoplot(learner)
```

### MeasureDens

As previously discussed, unsupervised tasks do not have ground truth data to compare to in model evaluation.
However, we can still measure the quality of cluster assignments by quantifying how closely objects within the same cluster are related (cluster cohesion) as well as how distinct different clusters are from each other (cluster separation).

There are a few built-in evaluation metrics available to assess the quality of clustering.

```{r}
mlr_measures$keys("clust")
```

Two common measures are the within sum of squares (WSS), `r ref("mlr_measures_clust.wss")`, and silhouette quality index (SQI), `r ref("mlr_measures_clust.silhouette")`.
WSS calculates the sum of squared differences between observations and centroids, which is a quantification of cluster cohesion (smaller values indicate clusters more compact).
SQI quantifies how well each point belongs to its assigned cluster versus neighboring cluster, where scores closer to `1` indicate well clustered and scores closer to `-1` indicate poorly clustered.

Putting this together with the above we can now score our cluster estimation (note we must pass the `task` to `r ref("msr")`):

```{r}
meas = msrs(c("clust.wss", "clust.silhouette"))

prediction$score(meas, task = task)
```

The very high WSS and middling SQI is 0.6 indicates that our clusters could do with a bit more work.
Often reducing an unsupervised task to a quantitative measure may not be useful (given no ground truth) and instead visualisation may be a more effective tools for assessing the quality of the clusters.

### Visualization

As with other tasks, we use `r mlr3viz` to visualise performance of cluster learners. The two most important plots are principal components analysis (PCA) and silhouette plots.

PCA is a commonly used dimension reduction method in machine learning to reduce the number of variables in a dataset or to visualise the most important 'components', which are linear transformations of the dataset features.
Components are considered more important if they have higher variance (and therefore more predictive power).
In the context of clustering, by plotting observations against the first two components, and then colouring them by cluster, we would expect to see distinct groups of observations if we are clustering well.

Continuing our running example, by plotting a PCA we see that our model has created cleanly separated clusters along the first two principal components.

```{r special-042, message=FALSE, warning=FALSE}
autoplot(prediction, task, type = "pca")
```

Silhouette plots help to visually assess the quality of the analysis and help choose a number of clusters for a given data set.
The average silhouette index is 0.5, which means observations that are scored below this line are considered to be poor estimations whereas observations above are well clustered, the SQI is then jus the average across all observations.
To visualise this, in a silhouette plot, we plot a dotted line at 0.5 and a bar for each data point, coloured by cluster.The red dotted line shows the mean silhouette value and each bar represents a data point.

Continuing our running example, we find that the majority of observations are actually below the average line, and therefore the quality of our prediction is not very good meaning that many observations are likely assigned to the wrong cluster.

```{r special-043, message=FALSE, warning=FALSE}
autoplot(prediction, task, type = "sil")
```

### Putting it all together {#sec-cluster-all}

Finally, we conduct a small benchmark study using the same task and with a few integrated cluster learners:

```{r special-041, message=FALSE, warning=FALSE}
task = as_task_clust(USArrests)
learners = list(
  lrn("clust.featureless"),
  lrn("clust.kmeans"),
  lrn("clust.cmeans", centers = 3L)
)
measures = list(msr("clust.wss"), msr("clust.silhouette"))
bmr = benchmark(benchmark_grid(task, learners, rsmp("cv", folds = 3)))
bmr$aggregate(measures)
```

The experiment shows that using c-means algorithm with three centers produces a better within sum of squares score than any other learner considered. However, the featureless learner performs the best when considering silhouette measure which takes into the account both cluster cohesion and separation.

## Spatiotemporal Analysis {#sec-spatiotemporal}

```{r special-010, include = FALSE}
library(mlr3spatiotempcv)
```

Data observations may entail reference information about spatial or temporal characteristics.
Spatial information is stored as coordinates, usually named "x" and "y" or "lat"/"lon".
Treating spatiotemporal data using non-spatial data methods can lead to over-optimistic performance estimates.
Hence, methods specifically designed to account for the special nature of spatiotemporal data are needed.

In the `r mlr3` framework, the following packages relate to this field:

- `r mlr3spatiotempcv` (biased-reduced performance estimation)
- `r mlr3spatial` (spatial prediction support)

The following (sub-)sections introduce the potential pitfalls of spatiotemporal data in machine learning and how to account for it.
Note that not all functionality will be covered, and that some of the used packages are still in early lifecycles.
If you want to contribute to one of the packages mentioned above, please contact [Patrick Schratz](https://github.com/pat-s).

### Changes to mlr3 core functionality {#sec-spatiotemporal-changes}

### Creating a spatial Task {#spatial-task}

To make use of spatial resampling methods, a {mlr3} task that is aware of its spatial characteristic needs to be created.
Two `Task` child classes exist in {mlr3spatiotempcv} for this purpose:

- `TaskClassifST`
- `TaskRegrST`

To create one of these, you have multiple options:

1. Use the constructor of the `Task` directly via `$new()` - this only works for data.table backends (!)
1. Use the `as_task_*` converters (e.g. if your data is stored in an `sf` object)

We recommend the latter, as the `as_task_*` converters aim to make task construction easier, e.g., by creating the `DataBackend` (which is required to create a Task in {mlr3}) automatically and setting the `crs` and `coordinate_names` fields.
Let's assume your (point) data is stored in with an `sf` object, which is a common scenario for spatial analysis in R.

```{r special-011}
# create 'sf' object
data_sf = sf::st_as_sf(ecuador, coords = c("x", "y"), crs = 32717)

# create `TaskClassifST` from `sf` object
task = as_task_classif_st(data_sf, id = "ecuador_task", target = "slides", positive = "TRUE")
```

You can also use a plain `data.frame`.
In this case, `crs` and `coordinate_names` need to be passed along explicitly as they cannot be inferred directly from the `sf` object:

```{r special-012}
task = as_task_classif_st(ecuador, id = "ecuador_task", target = "slides",
  positive = "TRUE", coordinate_names = c("x", "y"), crs = 32717)
```

The `*ST` task family prints a subset of the coordinates by default:

```{r special-013}
print(task)
```

All `*ST` tasks can be treated as their super class equivalents `TaskClassif` or `TaskRegr` in subsequent {mlr3} modeling steps.

### Autocorrelation {#autocorrelation}

Data which includes spatial or temporal information requires special treatment in machine learning (similar to [survival](#survival), [ordinal](#ordinal) and other task types listed in the [special tasks](#special-tasks) chapter).
In contrast to non-spatial/non-temporal data, observations inherit a natural grouping, either in space or time or in both space and time [@legendre1993].
This grouping causes observations to be autocorrelated, either in space (spatial autocorrelation (SAC)), time (temporal autocorrelation (TAC)) or both space and time (spatiotemporal autocorrelation (STAC)).
For simplicity, the acronym STAC is used as a generic term in the following chapter for all the different characteristics introduced above.

*What effects does STAC have in statistical/machine learning?*

The overarching problem is that STAC violates the assumption that the observations in the train and test datasets are independent [@hastie2001].
If this assumption is violated, the reliability of the resulting performance estimates, for example retrieved via cross-validation, is decreased.
The magnitude of this decrease is linked to the magnitude of STAC in the dataset, which cannot be determined easily.

One approach to account for the existence of STAC is to use dedicated resampling methods.
`r mlr3spatiotempcv` provides access to the most frequently used spatiotemporal resampling methods.
The following example showcases how a spatial dataset can be used to retrieve a bias-reduced performance estimate of a learner.

The following examples use the `r ref("mlr_tasks_ecuador", text = "ecuador")` dataset created by [Jannes Muenchow](https://scholar.google.com/citations?user=Slq94Y4AAAAJ&hl=de&authuser=1&oi=ao).
It contains information on the occurrence of landslides (binary) in the Andes of Southern Ecuador.
The landslides were mapped from aerial photos taken in 2000.
The dataset is well suited to serve as an example because it it relatively small and of course due to the spatial nature of the observations.
Please refer to @muenchow2012 for a detailed description of the dataset.

To account for the spatial autocorrelation probably present in the landslide data, we will make use one of the most used spatial partitioning methods, a cluster-based k-means grouping [@brenning2012], (`r ref("mlr_resamplings_spcv_coords", text = "spcv_coords")` in `r mlr3spatiotempcv`).
This method performs a clustering in 2D space which contrasts with the commonly used random partitioning for non-spatial data.
The grouping has the effect that train and test data are more separated in space as they would be by conducting a random partitioning, thereby reducing the effect of STAC.

By contrast, when using the classical random partitioning approach with spatial data, train and test observations would be located side-by-side across the full study area (a visual example is provided further below).
This leads to a high similarity between train and test sets, resulting in "better" but biased performance estimates in every fold of a CV compared to the spatial CV approach.
However, these low error rates are mainly caused due to the STAC in the observations and the lack of appropriate partitioning methods and not by the power of the fitted model.

### Spatiotemporal Cross-Validation and Partitioning {#spatiotemp-cv}

One part of spatiotemporal machine learning is dealing with the spatiotemporal components of the data during performance estimation.
Performance is commonly estimated via cross-validation and `r mlr3spatiotempcv` provides specialized resamplings methods for spatiotemporal data.
The following chapters showcases how these methods can be applied and how they differ compared to non-spatial resampling methods, e.g. random partitioning.
In addition, examples which show how resamplings with spatial information can be visualized using `r mlr3spatiotempcv`.

Besides performance estimation, prediction on spatiotemporal data is another challenging task.
See @sec-spatial-prediction for more information about how this topic is handled within the mlr3 framework.

#### Spatial CV vs. Non-Spatial CV {#sp-vs-nsp-cv}

In the following a spatial and non-spatial CV will be applied to showcase the mentioned performance differences.

The performance of a simple classification tree (`"classif.rpart"`) is evaluated on a random partitioning (`r ref("mlr_resamplings_repeated_cv", text = "repeated_cv")`) with four folds and two repetitions.
The chosen evaluation measure is "classification error" (`"classif.ce"`).

For the spatial example, `r ref("mlr_resamplings_repeated_spcv_coords", text = "repeated_spcv_coords")` is chosen whereas `r ref("mlr_resamplings_repeated_cv", text = "repeated_cv")` represents the non-spatial example.

:::{.callout-note}
The selection of `r ref("mlr_resamplings_repeated_spcv_coords", text = "repeated_spcv_coords")` in this example is arbitrary.
For your use case, you might want to use a different spatial partitioning method (but not necessarily!).
Have a look at the ["Getting Started"](https://mlr3spatiotempcv.mlr-org.com/dev/articles/mlr3spatiotempcv.html#resampling-methods) vignette of `r mlr3spatiotempcv` to see all available methods and choose one which **fits your data and its prediction purpose**.
:::

##### Non-Spatial CV {#nsp-cv}

In this example the `r ref("mlr_tasks_ecuador", text = "ecuador")` example task is taken to estimate the performance of an `rpart` learner with fixed parameters on it.

:::{.callout-warning}
In practice you usually might want to tune the hyperparameters of the learner in this case and apply a nested CV in which the inner loop is used for hyperparameter tuning.
:::

```{r special-014}
library("mlr3")
library("mlr3spatiotempcv")
set.seed(42)

# be less verbose
lgr::get_logger("bbotk")$set_threshold("warn")
lgr::get_logger("mlr3")$set_threshold("warn")

task = tsk("ecuador")

learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling_nsp = rsmp("repeated_cv", folds = 4, repeats = 2)
rr_nsp = resample(
  task = task, learner = learner,
  resampling = resampling_nsp)

rr_nsp$aggregate(measures = msr("classif.ce"))
```

##### Spatial CV {#sp-cv}

```{r special-015}
task = tsk("ecuador")

learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling_sp = rsmp("repeated_spcv_coords", folds = 4, repeats = 2)
rr_sp = resample(
  task = task, learner = learner,
  resampling = resampling_sp)

rr_sp$aggregate(measures = msr("classif.ce"))
```

Here, the performance of the classification tree learner is around 7 percentage points worse when using Spatial Cross-Validation (SpCV) compared to Non-Spatial Cross-Validation (NSpCV).
The resulting difference in performance is variable as it depends on the dataset, the magnitude of STAC and the learner itself.
For algorithms with a higher tendency of overfitting to the training set, the difference between the two methods will be larger.

Now, what does it mean that the performance in the spatial case is worse?
Should you ditch SpCV and keep using non-spatial partitioning?
The answer is **NO**.
The reason why the spatial partitioning scenario results in a lower predictive performance is because throughout the CV the model has been trained on data that is less similar than the test data compared against the non-spatial scenario.
Or in other words: in the non-spatial scenario, train and test data are almost identical (due to spatial autocorrelation).

This means that the result from the SpCV setting is more close to the true predictive power of the model - whereas the result from non-spatial CV is overoptimistic and biased.

:::{.callout-note}
The result of the SpCV setting is by no means the absolute truth - it is also biased, but (most often) less compared to the non-spatial setting.
:::

### Visualization of Spatiotemporal Partitions {#vis-spt-partitions}

Every partitioning method in `r mlr3spatiotempcv` comes with S3 methods for `plot()` and `autoplot()` to visualize the created groups.
In a 2D space `r ref_pkg("ggplot2")` is used in the backgroudn while for spatiotemporal methods 3D visualizations via `r ref_pkg("plotly")` are created.

The following examples shows how the `resampling_sp` object from the previous example can be visualized.
In this case I want to look at the first four partitions of the first repetition.
The point size is adjusted via argument `size`.
After the plot creation, additional `scale_*` calls are used to adjust the coordinate labels on the x and y axes, respectively.

```{r special-016, fig.asp=0.8}
autoplot(resampling_sp, task, fold_id = c(1:4), size = 0.7) *
  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *
  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.02))
```

:::{.callout-warning}
Note that setting the correct CRS for the given data is important which is done **during task creation**.
Spatial offsets of up to multiple meters may occur if the wrong CRS is supplied initially.
:::

This example used a built-in mlr3 task via `r ref("tsk()")`.
In practice however, one needs to create a spatiotemporal task via `r ref("TaskClassifST")`/`r ref("TaskRegrST")` and set the `crs` argument (unless a `sf` object is handed over).

`r mlr3spatiotempcv` can also visualize non-spatial partitonings.
This helps to visually compare differences.
Let's use the objects from the previous example again, this time `resampling_nsp`.

```{r special-017, fig.asp=0.8}
autoplot(resampling_nsp, task, fold_id = c(1:4), size = 0.7) *
  ggplot2::scale_y_continuous(breaks = seq(-3.97, -4, -0.01)) *
  ggplot2::scale_x_continuous(breaks = seq(-79.06, -79.08, -0.02))
```

The visualization show very well how close train and test observations are located next to each other.

#### Spatial Block Visualization {#vis-spatial-block}

This examples showcases another SpCV method: `r ref("mlr_resamplings_spcv_block", text = "spcv_block")`.
This method makes use of rectangular blocks to divide the study area into equally-sized parts.
{mlr3spatiotempcv} has support for visualizing the created blocks and displaying their respective fold ID to get a better understanding how the final folds were composed out of the partitions.
E.g. the "Fold 1 Repetition 1" plot shows that the test set is composed out of two "blocks" with the ID "1" in this case.

:::{.callout-note}
The use of `range = 1000L` is arbitrary here and should not be copy-pasted into a real application.
:::

```{r special-018}
task = tsk("ecuador")
resampling = rsmp("spcv_block", range = 1000L)

# Visualize train/test splits of multiple folds
autoplot(resampling, task, size = 0.7,
  fold_id = c(1, 2), show_blocks = TRUE, show_labels = TRUE) *
  ggplot2::scale_x_continuous(breaks = seq(-79.085, -79.055, 0.02))
```

#### Spatiotemporal Visualization {#vis-spatiotemp}

When going spatiotemporal, two dimensions are not enough anymore.
To visualize space and time together, a 3D solution is needed.
{mlr3spatiotempcv} makes use of `r ref_pkg("plotly")` for this purpose.

The following examples uses a modified version of the `cookfarm_mlr3` task for showcasing reasons.
By adjusting some levels, the individual partitions can be recognized very well.

:::{.callout-warning}
In practice, you should not modify your data to achieve "good looking" visualizations as done in this example.
This is only done for (visual) demonstration purposes.
:::

In the following we use the `r ref("mlr_resamplings_sptcv_cstf", text = "sptcv_cstf")` method after @meyer2018.
Only the temporal variable is used in this first example, denoted by setting the column role "time" to variable "Date".
This column role is then picked up by the resampling method.
Last, `autoplot()` is called with an explicit definition of `plot3D = TRUE`.
This is because `r ref("mlr_resamplings_sptcv_cstf", text = "sptcv_cstf")` can also be visualized in 2D (which only makes sense if the "space" column role is used for partitioning).

Last, `sample_fold_n` is used to take a stratified random sample from all partitions.
The call to `r ref("plotly::layout()")` only adjusts the default viewing angle of the plot - in interactive visualizations, this is not needed and the viewing angle can be adjusted with the mouse.

:::{.callout-warning}
This is done to reduce the final plot size and keep things small for demos like this one.
We don't recommend doing this in actual studies - unless you prominently communicate this alongside the resulting plot.
:::

```{r special-019, eval = FALSE}
data = cookfarm_mlr3
set.seed(42)
data$Date = sample(rep(c(
  "2020-01-01", "2020-02-01", "2020-03-01", "2020-04-01",
  "2020-05-01"), times = 1, each = 35768))
task_spt = as_task_regr_st(data,
  id = "cookfarm", target = "PHIHOX",
  coordinate_names = c("x", "y"), coords_as_features = FALSE,
  crs = 26911)
task_spt$set_col_roles("Date", roles = "time")

rsmp_cstf_time = rsmp("sptcv_cstf", folds = 5)

plot = autoplot(rsmp_cstf_time,
  fold_id = 5, task = task_spt, plot3D = TRUE,
  sample_fold_n = 3000L
)
plotly::layout(plot, scene = list(camera = list(eye = list(z = 0.58))))
```

If both space and time are used for partitioning in `r ref("mlr_resamplings_sptcv_cstf", text = "sptcv_cstf")`, the visualization becomes even more powerful as it allows to also show the observations which are omitted, i.e., not being used in either train and test sets for a specific fold.

```{r special-020, eval = FALSE}
task_spt$set_col_roles("SOURCEID", roles = "space")
task_spt$set_col_roles("Date", roles = "time")

rsmp_cstf_space_time = rsmp("sptcv_cstf", folds = 5)

plot = autoplot(rsmp_cstf_space_time,
  fold_id = 4, task = task_spt, plot3D = TRUE,
  show_omitted = TRUE, sample_fold_n = 3000L)

plotly::layout(plot, scene = list(camera =
list(eye = list(z = 0.58, x = -1.4, y = 1.6))))
```

Combining multiple spatiotemporal plots with `r ref("plotly::layout()")` is possible but somewhat cumbersome.
First, a list of plots containing the individuals plots must be created.
These plots can then be passed to `r ref("plotly::subplot()")`.
This return is then passed to `r ref("plotly::layout()")`.

```{r special-021, eval = FALSE}
pl = autoplot(rsmp_cstf_space_time, task = task_spt,
  fold_id = c(1, 2, 3, 4), point_size = 3,
  axis_label_fontsize = 10, plot3D = TRUE,
  sample_fold_n = 3000L, show_omitted = TRUE
)

# Warnings can be ignored
pl_subplot = plotly::subplot(pl)

plotly::layout(pl_subplot,
  title = "Individual Folds",
  scene = list(
    domain = list(x = c(0, 0.5), y = c(0.5, 1)),
    aspectmode = "cube",
    camera = list(eye = list(z = 0.20, x = -1.4, y = 1.6))
  ),
  scene2 = list(
    domain = list(x = c(0.5, 1), y = c(0.5, 1)),
    aspectmode = "cube",
    camera = list(eye = list(z = 0.1, x = -1.4, y = 1.6))
  ),
  scene3 = list(
    domain = list(x = c(0, 0.5), y = c(0, 0.5)),
    aspectmode = "cube",
    camera = list(eye = list(z = 0.1, x = -1.4, y = 1.6))
  ),
  scene4 = list(
    domain = list(x = c(0.5, 1), y = c(0, 0.5)),
    aspectmode = "cube",
    camera = list(eye = list(z = 0.58, x = -1.4, y = 1.6))
  )
)
```

Unfortunately, titles in subplots cannot be created dynamically. However, there is a manual workaround via annotations show in [this RPubs post](https://rpubs.com/bcd/subplot-titles).

### Choosing a Resampling Method {#choose-spt-rsmp}

While the example in this section made use of the `r ref("mlr_resamplings_spcv_coords", text = "spcv_coords")` method, this should by no means infer that this method is the best or only method suitable for this task.
Even though this method is quite popular, it was mainly chosen because of the clear visual grouping differences when being applied on the `r ref("mlr_tasks_ecuador", text = "ecuador")` task when compared to random partitioning.

In fact, most often multiple spatial partitioning methods can be used for a dataset.
It is recommended (required) that users familiarize themselves with each implemented method and decide which method to choose based on the specific characteristics of the dataset.
For almost all methods implemented in `r mlr3spatiotempcv`, there is a scientific publication describing the strengths and weaknesses of the respective approach (either linked in the help file of `r mlr3spatiotempcv` or its respective dependency packages).

:::{.callout-tip}
In the example above, a cross-validation without hyperparameter tuning was shown.
If a nested CV is desired, it is recommended to use the same spatial partitioning method for the inner loop (= tuning level).
See @schratz2019 for more details and chapter 11 of [Geocomputation with R](https://geocompr.robinlovelace.net/spatial-cv.html) [@lovelace2019].
:::

:::{.callout-tip}
A list of all implemented methods in `r mlr3spatiotempcv` can be found in the [Getting Started](https://mlr3spatiotempcv.mlr-org.com/articles/mlr3spatiotempcv.html#resampling-methods) vignette of the package.
:::

If you want to learn even more about the field of spatial partitioning, STAC and the problems associated with it, the works of [Prof. Hanna Meyer](https://scholar.google.com/citations?user=9YibxW0AAAAJ&hl=en) and [Prof. Alexander Brenning](https://scholar.google.com/citations?hl=en&user=GD5bzTgAAAAJ) are very much recommended for further reference.

### Spatial Prediction {#sec-spatial-prediction}

Support for (parallel) spatial prediction with `r ref_pkg("terra")`, `r ref_pkg("raster")`, `r ref_pkg("stars")` and `r ref_pkg("sf")` objects is available via `r mlr3spatial`.

`r mlr3spatial` has two main scopes:

- Provide DataBackends for spatial objects (vector and raster data)
- Simplify spatial prediction tasks

Overall it aims to reduce the roundtripping between spatial objects -> data.frame / data.table -> spatial object during modeling.

#### Spatial Data Backends {#spatial-data-backends}

A common scenario is the existence of spatial information in (point) vector data.
`r mlr3spatial` provides `as_task_*` helpers to directly convert these into a spatiotemporal (ST) task:

```{r, message = FALSE}
library(mlr3verse)
library(mlr3spatial)
library(sf)

# load sample points
leipzig_vector = sf::read_sf(system.file("extdata",
  "leipzig_points.gpkg", package = "mlr3spatial"),
  stringsAsFactors = TRUE)

# create land cover task
tsk_leipzig = as_task_classif_st(leipzig_vector, target = "land_cover")
tsk_leipzig
```

This saves users from stripping the coordinates from the vector data or transforming the `sf` object to a `data.frame` or `data.table` in the first place.

`r mlr3spatial` adds support for the following spatial data classes:

- `stars` (from package `r ref_pkg("stars")`)
- `SpatRaster` (from package `r ref_pkg("terra")`)
- `RasterLayer` (from package `r ref_pkg("raster")`)
- `RasterStack` (from package `r ref_pkg("raster")`)

#### Spatial prediction {#spatial-prediction}

The goal of spatial prediction is usually a raster image that covers a specific area.
Most often this area is quite large and contains millions of pixels.
The goal is to predict on each pixel and return a raster image containing these predictions.

To save users from having to go the extra mile of extracting the final model and using it with the respective `predict()` function of the spatial R package of their desired outcome class, `r mlr3spatial` provides `r ref("predict_spatial()")`.
`r ref("predict_spatial()")` let's users

- choose the output class of the resulting raster image
- optionally predict in parallel via the `r ref_pkg("future")` package, using a parallel backend of their choice

To use a raster image for prediction, it must be wrapped into a `r ref("TaskUnsupervised")` for internal `r mlr3` reasons.
Next, the learner can be used to predict on the task.

```{r, cache.lazy=FALSE, cache=FALSE, eval=FALSE}
leipzig_raster = terra::rast(system.file("extdata", "leipzig_raster.tif",
  package = "mlr3spatial"))
tsk_predict = as_task_unsupervised(leipzig_raster)

lrn = lrn("classif.ranger")
lrn$train(tsk_leipzig)

# plan("multisession") # optional parallelization
pred = predict_spatial(tsk_predict, lrn, format = "terra")
class(pred)
```

```{r, cache.lazy=FALSE, cache=FALSE, echo=FALSE}
# need to load mlr3spatial explicitily as quarto does not take over NS loads from previous chunks - or caching interfers somehow
library(mlr3spatial)
leipzig_raster = terra::rast(system.file("extdata", "leipzig_raster.tif",
  package = "mlr3spatial"))
tsk_predict = as_task_unsupervised(leipzig_raster)

lrn = lrn("classif.ranger")
lrn$train(tsk_leipzig)

# plan("multisession") # optional parallelization
pred = predict_spatial(tsk_predict, lrn, format = "terra")
class(pred)
```

The resulting object can be visualized and treated as any other `terra` object.
Thanks to the improved handling of factor variables in `r ref_pkg("terra")`, the raster image contains the correct labeled classes from the prediction right away.

```{r, message = FALSE, cache.lazy=FALSE, cache=FALSE}
library(terra, exclude = "resample")
plot(pred, col = c("#440154FF", "#443A83FF", "#31688EFF",
  "#21908CFF", "#35B779FF", "#8FD744FF", "#FDE725FF"))
```

If you are interested in the performance of parallel spatial prediction, have a look at the [Spatial Prediction Benchmark](https://mlr3spatial.mlr-org.com/articles/benchmark.html) vignette - the resulting plot is shown below.

:::{.callout-warning appearance="simple"}
The results of the shown benchmark results should be taken with care as something seems to go wrong when using all-core parallelization with `terra::predict()`.
Also both relative and absolute numbers will vary on your local machine and results might change depending on eventual package updates in the future.
:::

```{r, include=FALSE}
#| fig-align: center
#| fig-alt: '`r mlr3spatial`'
knitr::include_graphics("https://mlr3spatial.mlr-org.com/dev/articles/plot-benchmark-1.png", auto_pdf = TRUE)
```

### New features {#sec-spatiotemporal-new}

### Putting it all together {#sec-spatiotemporal-all}

## Conclusion

### Resources {.unnumbered .unlisted}

## Exercises

1. Survival analysis
2. Density estimation
3. Spatiotemporal
4. Clustering