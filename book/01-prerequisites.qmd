# Prerequisites {#prerequisites}

This book concerns itself with machine learning ("ML"), a vast topic that encompasses many different areas. While it would be out of scope to given an introduction to the topic of ML itself (at this point we recommend our [Introduction to Machine Learning](https://slds-lmu.github.io/i2ml/) undergraduate level course available for free online), we will give a short recap of relevant topics here.

## Datasets, Models and Predictions

Supervised machine learning concerns itself with relationships that can be expressed as data. Possible examples are the relationship of engine characteristics with gas mileages in cars, the flower shapes of different kinds of plants, or the relationships between symptoms and progressions of different kinds of diseases. When these relationships are patterns in repeatable processes that happen somewhat independently of each other, then one can try to fit *models* that make *predictions* about yet unseen occurrences of these relationships based on data that was previously collected, the *training data*. These models then use some of the recorded variables, which we call *features*, to make predictions about a certain outcome, which we call the *target* variable.

While training data could be any kind of unstructured data such as images, videos, or sound files, in this book we will concern ourselves with how to apply `mlr3` to *tabular data*. This data is organized in such a way that it is contained in an `R` `data.frame` or `data.table`, with one row for each training example, and one column for each feature, as well as for the target. The feature columns may contain a variety of scalar values, such as numerics (e.g. the age of a person) or categorical values (such as their marital status). The target variable can be a single numeric value ("regression"), a single categorical value ("classification"), or various other types for more advanced supervised learning tasks.

The algorithm that uses training data to generate a model, which is then used to make predictions, is called a `Learner` by `mlr3`\footnote{It is also sometimes called an "inducer"}. The fitted model is represented by the learner as *model parameters*, which could for example be network weights in an artificial neural network, or split variables and split points in tree-based methods. These parameters should not be confused with "hyperparameters", which are configuration settings of the learner, such as how many layers the neural network should have, or how many split points a classification tree should have. The difference is that model hyperparameters are set before the training process and determine how this process operates, whereas the model parameters are determined *by* the training process.

After a learner has been trained, it can be used to make predictions. For this, the learner is given the feature variables as inputs, and it produces some useful output. This output can have the same type as the training target variable had: a numeric point prediction for regression, or a predicted category for classification. However, some learners are able to produce more informative output, such as confidence intervals for regression, or probability scores for the different possible classes in classification. Because a learner can often efficiently make predictions for many samples at once, they take a tabular data with one column for each feature, and produce tabular output with one predicted row for each input row. However, this is only for convenience: the model predictions should always be the same whether they are made one at a time, or all in one go; the predictions made about one sample does not depend on the presence or absence of other samples that the model is making predictions about.

## Model Performance and Tuning

Once a model has made predictions, one may wonder what the quality of these predictions is: How often does a classification model predict the correct class, or what is the average absolute distance between the predicted value of a regression model, and the actual "ground truth" value? These performance measures can only be determined by using the model to make predictions about samples with a known ground truth value that have *not* been used to train the model, called the test set\footnote{In some circumstances, it is called the "validation set", and sometimes "holdout set"}. To get a better estimate of a model's performance, it is often beneficial to train the learner multiple times on different training data subsets, estimating performance on the respective remaining data (i.e. the different test sets), and aggregating these performance values, e.g. by averaging them. 