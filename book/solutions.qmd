# Solutions to exercises {#sec-solutions}

{{< include _setup.qmd >}}

## Solutions to @sec-basics

## Solutions to @sec-performance

## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r optimization-030}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)
```

2. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r optimization-046}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  method = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```

3. Tune and benchmark an XGBoost model against a logistic regression `r ref("mlr_tasks_spam", text = "Spam)` data set and determine which has the best Brier score.
Use mlr3tuningspaces and nested resampling.

```{r}
library(mlr3tuningspaces)

learner_xgboost = lts(lrn("classif.xgboost", predict_type = "prob"))

at_xgboost = auto_tuner(
  method = tnr("random_search", batch_size = 5),
  learner = learner_xgboost,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.bbrier"),
  term_evals = 20,
)

learner_logreg = lrn("classif.log_reg", predict_type = "prob")

at_logreg = auto_tuner(
  method = tnr("random_search", batch_size = 5),
  learner = learner_logreg,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.bbrier"),
  term_evals = 20,
)

task = tsk("spam")
outer_resampling = rsmp("cv", folds = 3)

design = benchmark_grid(
  tasks = task,
  learners = list(at_xgboost, at_logreg),
  resamplings = outer_resampling
)

bmr = benchmark(design, store_models = TRUE)

bmr
```



## Solutions to @sec-feature-selection

## Solutions to @sec-pipelines

## Solutions to @sec-special

## Solutions to @sec-technical

## Solutions to @sec-interpretation

## Solutions to @sec-extending
