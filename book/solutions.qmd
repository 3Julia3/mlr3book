# Solutions to exercises {#sec-solutions}

{{< include _setup.qmd >}}

## Solutions to @sec-basics

## Solutions to @sec-performance

1. You want to evaluate a decision tree classification learner (`classif.rpart`) on the `zoo` task, but you're unsure which resampling strategy is appropriate.
One of your colleagues recommends to use 5-fold cross-validation while another colleague swears by leave-one-out cross-validation ("LOO").
Your goal is to maximize classification accuracy.
Which strategy would you use?
Try out both and visualize the resulting scores per resampling iteration.
Which strategy seems to be more reliable in this scenario? Why?


```{r}
set.seed(3)
rr1 = resample(
  task = tsk("zoo"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 5)
)

rr2 = resample(
  task = tsk("zoo"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("loo")
)


measure = msr("classif.acc")

rr1$aggregate(measure)
rr2$aggregate(measure)

mlr3viz::autoplot(rr1, measure = measure)
mlr3viz::autoplot(rr2, measure = measure)
```

LOO may sound good in theory since it gives you a lot of iterations, but with this measure it gives optimistic results.
We can investigate using `$score()` to see the accuracy for each iteration:

```{r}
rr2$score(measure)[, .(resampling_id, iteration, classif.acc)]
```
As you can see, they are all either 0 or 1, since there is only ever one observation in the test set, and this either matches the predicted target or it does not.
This does not invalidate the overall performance score per se, but at the very least it can lead to unexpected artifacts as seen in the boxplot for `rr2`.

2. Use the `spam` task and 5-fold cross-validation to benchmark Random Forest (`classif.ranger`), Logistic Regression (`classif.log_reg`), and XGBoost (`classif.xgboost`) with regards to AUC.
Which learner appears to do best? How confident are you in your conclusion?
How would you improve upon this?

```{r}
grid = benchmark_grid(
  tasks = tsk("spam"),
  learners = lrns(c("classif.ranger", "classif.log_reg", "classif.xgboost"), predict_type = "prob"),
  resamplings = rsmp("cv", folds = 5)
)

bmr = benchmark(grid)

mlr3viz::autoplot(bmr, measure = msr("classif.auc"))
```

This is only a small example for a benchmark workflow, but without tuning (see @sec-optimization), the results are naturally not suitable to make any broader statements about the superiority of either learner for this task.


3. A colleague claims to have achieved a 93.1% classification accuracy using the `classif.rpart` learner on the `penguins_simple` task.
You want to reproduce their results and ask them about their resampling strategy.
They said they used 3-fold cross-validation, and they assigned rows using the task's `row_id` modulo 3 to generate three evenly sized folds.
Reproduce their results using the custom CV strategy.

```{r}
task = tsk("penguins_simple")

resampling_customcv = rsmp("custom_cv")

resampling_customcv$instantiate(task = task, f = factor(task$row_ids %% 3))

rr = resample(
  task = task,
  learner = lrn("classif.rpart"),
  resampling = resampling_customcv
)

rr$aggregate(msr("classif.acc"))
```


## Solutions to @sec-optimization

1. Tune the `mtry`, `sample.fraction`, ` num.trees` hyperparameters of a random forest model (`regr.ranger`) on the `r ref("mlr_tasks_mtcars", text = "Motor Trend")` data set (`mtcars`).
Use a simple random search with 50 evaluations and select a suitable batch size.
Evaluate with a 3-fold cross-validation and the root mean squared error.

```{r optimization-030}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

instance = ti(
  task = tsk("mtcars"),
  learner = learner,
  resampling = rsmp("cv", folds = 3),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

tuner = tnr("random_search", batch_size = 10)

tuner$optimize(instance)
```

2. Evaluate the performance of the model created in Question 1 with nested resampling.
Use a holdout validation for the inner resampling and a 3-fold cross-validation for the outer resampling.
Print the unbiased performance estimate of the model.

```{r optimization-046}
set.seed(4)
learner = lrn("regr.ranger",
  mtry.ratio      = to_tune(0, 1),
  sample.fraction = to_tune(1e-1, 1),
  num.trees       = to_tune(1, 2000)
)

at = auto_tuner(
  method = tnr("random_search", batch_size = 10),
  learner = learner,
  resampling = rsmp("holdout"),
  measures = msr("regr.rmse"),
  terminator = trm("evals", n_evals = 50)
)

task = tsk("mtcars")
outer_resampling = rsmp("cv", folds = 3)
rr = resample(task, at, outer_resampling, store_models = TRUE)

rr$aggregate()
```

## Solutions to @sec-feature-selection

## Solutions to @sec-pipelines

## Solutions to @sec-special

## Solutions to @sec-technical

## Solutions to @sec-interpretation

## Solutions to @sec-extending
