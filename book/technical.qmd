***

author:

*   name: Michel Lang
    orcid: 0000-0001-9754-0393
    email: michel.lang@tu-dortmund.de
    affiliations:
    *   name: Research Center Trustworthy Data Science and Security
    *   name: TU Dortmund University
*   name: Author 2
    orcid:
    email:
    affiliations:
    *   name: Affiliation 2
        abstract: TODO (150-200 WORDS)

***

# Technical {#sec-technical}

{{< include \_setup.qmd >}}

This chapter provides an overview of the technical details of the `r mlr3` framework.
This includes the following topics:

*   Parallelization with the `r ref_pkg("future")` framework (@sec-parallelization),
*   how to handle errors and troubleshoot (@sec-error-handling),
*   working with out-of-memory data, e.g., data stored in databases (@sec-backends),
*   working with hyperparameters (@sec-paradox), and
*   adjust the logger to your needs (@sec-logging).

## Parallelization {#sec-parallelization}

Parallelization refers to running multiple jobs in parallel, i.e., executing them simultaneously on multiple CPU cores, CPUs, or computational nodes.
This process allows for significant savings in computing power.

In general, there are many possibilities to parallelize, depending on the hardware to run the computations: If you only have a single CPU with multiple cores, [threads](https://en.wikipedia.org/wiki/Thread_\(computing\)) or [forks](https://en.wikipedia.org/wiki/Fork_\(system_call\)) are ways to utilize all cores.
If you have multiple machines, they need a way to communicate and exchange information, e.g. via protocols like network sockets or the [Message Passing Interface (MPI)](https://en.wikipedia.org/wiki/Message_Passing_Interface).
We don't want to delve too deep into such details here, but want to introduce some terminology:

*   We call the parallelization platform together with its implementation for R the **parallelization backend**.
    As many parallelization backends have a different API, we are using the `r ref_pkg("future")` package as an additional abstraction layer.
    `r mlr3` just interfaces `r ref_pkg("future")` while the user can control how the code is executed.
*   The R session or process which orchestrates the computational work is called **main**, and it starts computational **jobs**.
*   The R sessions, processes, forks or machines which receive the jobs, do the calculation and then send back the result are called **workers**.

We distinguish between [*implicit parallelism*](#sec-implicit) and [*explicit parallelism*](#sec-explicit).
For the former, no special directives are required to enable the parallelization, everything works fully automatically.
For the latter, parallelization has to be manually configured.
On the one hand, this gives you full control over the execution, but on the other hand, this poses a greater obstacle for non-experts.

:::{.callout-note}
We don't cover parallelization on GPUs here.
`mlr3` only distributes the fitting of multiple learners, e.g., during resampling, benchmarking, or tuning.
On this rather abstract level, GPU parallelization doesn't work efficiently.
Some learning procedures can be compiled against CUDA/OpenCL to utilize the GPU while fitting a single model.
We refer to the respective documentation of the learner's implementation, e.g., [here](https://xgboost.readthedocs.io/en/stable/gpu/index.html) for `r ref("mlr_learners_classif.xgboost", text = "xgboost")`.
:::

### Implicit Parallelization {#sec-implicit}

We talk about implicit parallelization in the context of `mlr3`, if `mlr3` calls external code (i.e., code from foreign CRAN packages which implements a `r ref("Learner")`) that itself runs in parallel.
Note that this definition includes GPU acceleration.

Many machine learning algorithms can parallelize their model fit using threading, e.g., the random forest implementation in `r ref_pkg("ranger")` or the boosting implemented in `r ref_pkg("xgboost")`.
During threading, the implementation instructs some sequential parts of the code to be executed independently of the other parts in the same process.

For example, while fitting a decision tree, each split that divides the data into two disjoint partitions requires a search for the best cut point on all $p$ features.
So instead of iterating over all features sequentially, the search can be broken down into $p$ threads, each searching for the best cut point on a single feature.
These threads can easily be parallelized by the scheduler of the operating system, as there is no need for communication between the threads.
After all threads have finished, the results are collected and merged before terminating the threads.
I.e., for our example of the decision tree, (1) the $p$ best cut points per feature are collected and then (2) aggregated to the single best cut point across all features by just iterating over the $p$ results sequentially.

:::{.callout-warning}
It does not make practical sense to actually execute in parallel every operation that can be parallelized.
Starting and terminating workers (here: threads) as well as possible communication between workers comes at a price in the form of additionally required runtime which is called (parallelization) overhead.
The overhead must be related to the runtime of the sequential execution.
If the sequential execution is comparably fast, enabling parallelization often just introduces additional complexity and slows down the execution.
:::

Unfortunately, threading conflicts with certain parallel backends used during explicit parallelization, causing the system to be overutilized in the best case and causing hangs or segfaults in the worst case.
For this reason, we introduced the convention that implicit parallelization is turned off per default.
Hyperparameters that control the number of threads are tagged with the label `"threads"`.
Currently, controlling the number of threads is possible for some learners and filters from the `r ref_pkg("mlr3filters")` package:

```{r technical-001}
library("mlr3learners") # for the ranger learner

learner = lrn("classif.ranger")
learner$param_set$ids(tags = "threads")
```

To enable the parallelization for this learner, we provide the helper function `r ref("set_threads()")` which

```{r technical-002}
# use 4 CPUs
set_threads(learner, n = 4)

# auto-detect cores on the local machine
set_threads(learner)
```

:::{.callout-caution}
Automatic detection of the number of CPUs is sometimes flaky, and utilizing all available cores is occasionally counterproductive as overburdening the system often has negative effects on the overall runtime.
The function which determines the number of CPUs for `r mlr3` is implemented in `r ref("parallelly::availableCores()")` and comes with reasonable heuristics for many setups.
See [this blog post](https://www.jottr.org/2022/12/05/avoid-detectcores/) for some background information about the heuristic.
However, there are still some scenarios where it is better to reduce the number of utilized CPUs manually:

*   You want to simultaneously work on the same system, e.g., browse the web or watch a video.
*   You are on a multi-user system and want to spare some resources for other users.
*   You have energy-efficient CPU cores, for example, the "Icestorm" cores on a Mac M1 chip.
    These are comparably slower than the high-performance "Firestorm" cores and not well suited for heavy computations.
*   You have linked R to a threaded [BLAS](https://de.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms) implementation like [OpenBLAS](https://www.openblas.net/), and your learners make heavy use of linear algebra.

    You can manually set the number of CPUs to overrule the heuristic via option `"mc.cores"`:

```{r technical-003, eval = FALSE}
options(mc.cores = 4)
```

We recommend setting this in your system's `.Rprofile` file, c.f. `r ref("Startup")`.
:::

### Explicit Parallelization {#sec-explicit}

Here, we talk about explicit parallelization if `r mlr3` starts and controls the parallelization itself.
For this purpose, an additional abstraction layer is used to be able to operate on a unified interface for a broad range of parallel backends: the `r ref_pkg("future")` package.
There are two operations where `r mlr3` calls the `future` package: while performing resampling via `r ref("resample()")` and while benchmarking via `r ref("benchmark()")`.
During resampling, because all resampling iterations are independent of each other, all iterations can be executed in parallel.
The same holds for benchmarking, where additionally to the independent model fits of a single resampling, all combinations in the provided design are also independent.
These iterations are performed by `r ref_pkg("future")` using the parallel backend configured with `r ref("future::plan()")`.
Extension packages like `r mlr3tuning` internally call `r ref("benchmark()")` during tuning and thus work in parallel, too.

:::{.callout-tip}
When computational problems are so easy to parallelize, they are often referred to as "embarrassingly parallel".

Whenever you loop over elements with a map-like function (e.g., `r ref("lapply()")`, `r ref("sapply()")`, `r ref("mapply()")`, `r ref("vapply()")` or a function from package `r ref_pkg("purrr")`), you are facing an embarrassingly parallel problem.
Such problems are straightforward to parallelize with R, e.g., with the `r ref_pkg("furrr")` package providing map-like functions executed in parallel via the `r ref_pkg("future")` framework.
The same holds for `for`-loops with independent iterations, i.e., loops where the current iteration does not rely on the results of previous iterations.
:::

In this section, we will use the `r ref("mlr_tasks_spam", text = "spam task")` and a simple `r ref("mlr_learners_classif.rpart", text = "classification tree")` to showcase the explicit parallelization.
We use the `r ref("future::multisession")` parallel backend that should work on all systems.

```{r technical-005, eval = FALSE}
# select the multisession backend to use
future::plan("multisession")

# define objects to perform a resampling
task = tsk("spam")
learner = lrn("classif.rpart")
resampling = rsmp("cv", folds = 3)

time = proc.time()[3]
resample(task, learner, resampling)
diff = proc.time()[3] - time
```

By default, all CPUs of your machine are used unless you specify the argument `workers` in `r ref("future::plan()")` (possible problems with this default have already been discussed for implicit parallelization).
You should see a decrease in the reported elapsed time, but in practice, you cannot expect the runtime to fall linearly as the number of cores increases ([Amdahl's law](https://www.wikiwand.com/en/Amdahl%27s_law)).
In contrast to threads, the technical overhead for starting workers, communicating objects, sending back results, and shutting down the workers is quite large for the multisession backend.
Therefore, it is advised to only consider parallelization for resamplings where each iteration runs at least several seconds.

@fig-parallel-overview illustrates the parallelization from the above example. From left to right:

1.  The main process calls the `resample()` function.
2.  The task is split into 3 folds.
3.  The folds are passed to three workers, each fitting a model on the respective subset of the task and predicting on the left-out observations.
4.  The predictions (and trained models) are communicated back to main process which combines them into a `ResampleResult`.

```{mermaid}
%%| label: fig-parallel-overview
%%| fig-cap: Parallelization of a resampling using a 3-fold cross-validation
graph LR
    M[fa:fa-server Main]
    S{"resample()"}
    C{ResampleResult}

    M --> S
    S -->|Fold 1| W1[fa:fa-microchip Worker 1]
    S -->|Fold 2| W2[fa:fa-microchip Worker 2]
    S -->|Fold 3| W3[fa:fa-microchip Worker 3]
    W1 -->|Prediction 1| C
    W2 -->|Prediction 2| C
    W3 -->|Prediction 3| C
```

:::{.callout-note}
If you are transitioning from `r ref_pkg("mlr")`, you might be used to selecting different parallelization levels, e.g., for resampling, benchmarking, or tuning.
In `r mlr3`, this is no longer required (except for nested resampling, briefly described in the following section).
All kind of experiments are rolled out on the same level.
Therefore, there is no need to decide whether you want to parallelize the tuning OR the resampling.

Just lean back and let the machine do the work :-)
:::

### Reproducibility

Usually reproducibility is a major concern during parallelization as special [pseudorandom number generators](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) (PRNGs) are required.
Luckily, this problem is already solved for us by the excellent `r ref_pkg("future")` package `r mlr3` calls under the hood.
`r ref_pkg("future")` ensures that all workers will receive the exactly same PRNG streams.
Although this alone does not guarantee full reproducibility, it is one problem less to worry about.

You can find more details about the used pseudo RNG in [this blog post](https://www.jottr.org/2020/09/22/push-for-statistical-sound-rng/).

### Nested Resampling Parallelization {#sec-nested-resampling-parallelization}

[Nested resampling](#nested-resampling) results in two nested resampling loops, and the user can choose which of them should be parallelized.
Let's consider the following example:
You want to tune the `minsplit` argument of a classification tree using the `r ref("AutoTuner")` of `r mlr3tuning` (simplified version taken from the [nested resampling section](#sec-model-performance)):

```{r}
library("mlr3tuning")

learner = lrn("classif.rpart",
  minsplit  = to_tune(2, 128, logscale = TRUE)
)

at = auto_tuner(
  method = tnr("random_search"),
  learner = learner,
  resampling = rsmp("cv", folds = 2), # inner CV
  measure = msr("classif.ce"),
  term_evals = 20,
)
```

To evaluate the performance on an independent test set, resampling is used:

```{r}
resample(
  task = tsk("penguins"),
  learner = at,
  resampling = rsmp("cv", folds = 5) # outer CV
)
```

Here, we have two opportunities to tune: the inner cross-validation of the auto tuner with 2 folds, or the outer cross-validation of the resampling with 5 folds.
Let's say that we have a single CPU with four cores available.

If we opt to parallelize the outer CV, all four cores would be utilized first with the computation of the first 4 resampling iterations.
The computation of the fifth iteration has to wait, i.e., depending on the parallelization backend and its scheduling strategy,

(a) until all four iterations have been finished, and the results have been collectively reported back to the main process, or
(b) either one of the four cores has terminated -- the first core reporting back will get a new task as soon as possible.

:::{.callout-note}
The former method usually comes with less synchronization overhead and is best suited for short jobs with homogeneous runtimes.
The latter yields better runtimes if the runtimes are heterogeneous, especially if the parallelization overhead is neglectable in comparison with the runtime for the computation.
E.g., for `r ref("parallel::mclapply()")`, the behavior of the scheduler can be controlled with the `mc.preschedule` option.
For many backends, you cannot control the scheduling.
However, `r ref_pkg("future")` allows you to first chunk jobs together which combines multiple tasks into blocks that run sequentially on a worker, avoiding the intermediate synchronization steps.
:::

The resulting CPU utilization of the nested resampling example on 4 CPUs is visualized in two Figures:

*   @fig-parallel-outer as an example for parallelizing the outer 5-fold cross-validation.
    ```{r, eval = FALSE}
    # Runs the outer loop in parallel and the inner loop sequentially
    future::plan(list("multisession", "sequential"))
    ```
    We assume that each fit during the inner resampling takes 4 seconds to compute and that there is no other significant overhead.
    First, each of the four workers starts with the computation of an inner 2-fold cross-validation.
    As there are more jobs than workers, the remaining fifth iteration of the outer resampling is queued on CPU1 **after** the first 4 iterations are finished after 8 secs.
    During the computation of the 5th outer resampling iteration, only CPU1 is utilized, the other 3 CPUs are idling.
*   @fig-parallel-inner as an example for parallelizing the inner 2-fold cross-validation.
    ```{r technical-006, eval = FALSE}
    # Runs the outer loop sequentially and the inner loop in parallel
    future::plan(list("sequential", "multisession"))
    ```
    Here, the outer loop runs sequentially and distributes the 2 computations for the inner resampling on 2 CPUs. Meanwhile, CPU3 and CPU4 are idling.

```{mermaid}
%%| label: fig-parallel-outer
%%| fig-cap: CPU utilization while parallelizing the outer resampling of a 2-fold cross-validation nested inside a 5-fold cross-validation on 4 CPUs.
gantt
    title CPU Utilization
    dateFormat  s
    axisFormat %S
    section CPU1
    Iteration 1-1           :0, 4s
    Iteration 1-2           :4, 4s
    Iteration 5-1           :8, 4s
    Iteration 5-2           :12, 4s

    section CPU2
    Iteration 2-1           :0, 4s
    Iteration 2-2           :4, 4s
    Idle                    :crit, 8, 8s

    section CPU3
    Iteration 3-1           :0, 4s
    Iteration 3-2           :4, 4s
    Idle                    :crit, 8, 8s

    Iteration 4-1           :0, 4s
    section CPU4
    Iteration 4-2           :4, 4s
    Idle                    :crit, 8, 8s
```

```{mermaid}
%%| label: fig-parallel-inner
%%| fig-cap: CPU utilization while parallelizing the inner resampling of a 2-fold cross-validation nested inside a 5-fold cross-validation on 4 CPUs.
gantt
    title CPU Utilization
    dateFormat  s
    axisFormat %S
    section CPU1
    Iteration 1-1           :0, 4s
    Iteration 2-1           :4, 4s
    Iteration 3-1           :8, 4s
    Iteration 4-1           :12, 4s
    Iteration 5-1           :16, 4s

    section CPU2
    Iteration 1-2           :0, 4s
    Iteration 2-2           :4, 4s
    Iteration 3-2           :8, 4s
    Iteration 4-2           :12, 4s
    Iteration 5-2           :16, 4s

    section CPU3
    Idle                    :crit, 0, 20s

    section CPU4
    Idle                    :crit, 0, 20s
```

Both possibilities for parallelization are not exploiting the full potential of the 4 CPUs.
With parallelization of the outer loop, all results are computed after 16s, in contrast to parallelization of the inner loop where the results are only available after 20s.

If possible, the number of iterations can be adapted to the available hardware. There is no law set in stone that you have to do, e.g., 10 folds in cross-validation.
If you have 4 CPUs and a reasonable variance, 8 iterations are often sufficient, or you do 12 iterations because you get the last two iterations basically for free.

Alternatively, you can also enable parallelization for both loops for nested parallelization, even on different parallelization backends.
While nesting real parallelization backends is often unintended and causes unnecessary overhead, it is useful in some distributed computing setups.
In this case, the number of workers must be manually tweaked so that the system does not get overburdened:

```{r technical-007, eval = FALSE}
# Runs both loops in parallel
future::plan(list(
  future::tweak("multisession", workers = 2),
  future::tweak("multisession", workers = 4)
))
```

This example would run on 8 cores (`= 2 * 4`) on the local machine.
The [vignette](https://cran.r-project.org/web/packages/future/vignettes/future-3-topologies.html) of the `r ref_pkg("future")` package gives more insight into nested parallelization.
For more background information about parallelization during tuning, see Section 6.7 of @hpo\_practical.

:::{.callout-caution}
During tuning with `r mlr3tuning`, you can often adjust the **batch size** of the `r ref("Tuner")`, i.e., control how many hyperparameter configurations are evaluated in parallel.
If you want full parallelization, make sure that the batch size multiplied by the number of (inner) resampling iterations is at least equal to the number of available workers.
If you expect homogeneous runtimes, i.e., you are tuning over a single learner or linear pipeline and you have no hyperparameter which is likely to influence the performance, aim for a multiple of the number of workers.

In general, larger batches mean more parallelization, while smaller batches imply a more frequent evaluation of termination criteria.
We default to a `batch_size` of 1 that ensures that all `r ref("Terminator", text = "Terminators")` work as intended, i.e., you cannot exceed the computational budget.
:::

## Error Handling {#sec-error-handling}

In ML, it is not uncommon for something to break.
This is because the algorithms have to process arbitrary data, and not all eventualities can always be handled.
While we try to identify obvious problems before execution, such as when missing values occur, but a learner can't handle them, other problems are far more complex to detect.
Examples include correlations or collinearity that make model fitting impossible, outliers that lead to numerical problems, or new levels of categorical variables emerging in the predict step.
The learners behave quite differently when encountering such problems: some models signal a warning during the train step that they failed to fit but return a baseline model while other models stop the execution.
During prediction, some learners just refuse to predict the response for observations they cannot handle while others predict a missing value.
How to deal with these problems even in more complex setups like benchmarking or tuning is the topic of this section.

For illustration (and internal testing) of error handling, `r mlr3` ships with the learners `r ref("mlr_learners_classif.debug", "classif.debug")` and `r ref("mlr_learners_regr.debug", "regr.debug")`.
Here, we will concentrate on the debug learner for classification:

```{r technical-008}
task = tsk("penguins")
learner = lrn("classif.debug")
print(learner)
```

This learner comes with special hyperparameters that let us simulate problems frequently encountered in ML.
E.g., the debug learner comes with hyperparameters to control

1.  what conditions should be signaled (message, warning, error, segfault) with what probability,
2.  during which stage the conditions should be signaled (train or predict), and
3.  the ratio of predictions being `NA` (`predict_missing`).

```{r technical-009}
learner$param_set
```

With the learner's default settings, the learner will do nothing special: The learner remembers a random label and constantly predicts this label:

```{r technical-010}
task = tsk("penguins")
learner$train(task)$predict(task)$confusion
```

We now set a hyperparameter to let the debug learner signal an error during the train step.
By default, `r mlr3` does not catch conditions such as warnings or errors raised while calling learners:

````{r technical-011, error = TRUE}
# set probability to signal an error to 1
learner$param_set$values = list(error_train = 1)

learner$train(tsk("iris"))

If this have been a regular learner, we could now start debugging with `r ref("traceback()")` (or create a [Minimal Reproducible Example (MRE)](https://stackoverflow.com/help/minimal-reproducible-example) to file a bug report upstream).
=======
If this has been a regular learner, we could now start debugging with `r ref("traceback()")` (or create a [Minimal Reproducible Example (MRE)](https://stackoverflow.com/help/minimal-reproducible-example) to file a bug report upstream).

:::{.callout-note}
If you start debugging, make sure you have disabled parallelization to avoid various pitfalls related to parallelization.
It may also be helpful to set the option `mlr3.debug` to `TRUE`.
If this flag is set, `r mlr3` does not call into the `r ref_pkg("future")` package, resulting in an easier-to-interpret program flow and `traceback()`.
:::

### Encapsulation {#sec-encapsulation}

Since ML algorithms are confronted with arbitrary, often messy data, errors are not uncommon here, and we often just need to move on during benchmarking or tuning.
Thus, we need a mechanism to

1.  capture all signaled conditions such as messages, warnings and errors so that we can analyze them post-hoc (called "encapsulation", covered in this section), and
2.  a statistically sound way to proceed while being able to aggregate over partial results (next @sec-fallback).

Encapsulation ensures that signaled conditions (such as messages, warnings and errors) are intercepted: all conditions raised during the training or predict step are logged into the learner, and errors do not interrupt the program flow.
I.e., the execution of the calling function or package (here: `r mlr3`) continues as if there had been no error, though the result (fitted model during `train()`, predictions during `predict()`) are missing.
Each `r ref("Learner")` has a field `encapsulate` to control how the train or predict steps are wrapped.
The easiest way to encapsulate the execution is provided by the package `r ref_pkg("evaluate")` which evaluates R expressions while tracking conditions such as outputs, messages, warnings or errors (see the documentation of the `r ref("encapsulate()")` helper function for more details):

```{r technical-012}
task = tsk("penguins")
learner = lrn("classif.debug")

# this learner throws a warning and then stops with an error during train()
learner$param_set$values = list(warning_train = 1, error_train = 1)

# enable encapsulation for train() and predict()
learner$encapsulate = c(train = "evaluate", predict = "evaluate")

learner$train(task)
````

After training the learner, one can access the recorded log via the fields `log`, `warnings` and `errors`:

```{r technical-013}
learner$log
learner$warnings
learner$errors
```

Another method for encapsulation is implemented in the `r ref_pkg("callr")` package.
In contrast to `r ref_pkg("evaluate")`, the computation is taken out in a separate R process.
This guards the calling session against segfaults which otherwise would tear down the complete R session.
On the downside, starting new processes comes with comparably more computational overhead.

```{r technical-014}
learner$encapsulate = c(train = "callr", predict = "callr")
learner$param_set$values = list(segfault_train = 1)
learner$train(task = task)
learner$errors
```

With either of these encapsulation methods, we can now catch errors and post-hoc analyze the messages, warnings and error messages.
Unfortunately, this is only half the battle.
Without a model, it is not possible to get predictions:

```{r technical-015, error = TRUE}
learner$predict(task)
```

To handle the missing predictions gracefully during `r ref("resample()")`, `r ref("benchmark()")` or tuning, fallback learners are introduced next.

### Fallback learners {#sec-fallback}

Fallback learners have the purpose of allowing scoring results in cases where a `r ref("Learner")` failed to fit a model, refuses to provide predictions for all observations or predicts missing values.

We will first handle the case that a learner fails to fit a model during training, e.g., if some convergence criterion is not met or the learner ran out of memory.
There are in general three possibilities to proceed:

1.  Ignore missing scores.
    Although this is arguably the most frequent approach in practice, it is **not** statistically sound.
    For example, consider the case where a researcher wants a specific learner to look better in a benchmark study.
    To do this, the researcher takes an existing learner but introduces a small adaptation: If an internal goodness-of-fit measure is not achieved, an error is thrown.
    In other words, the learner only fits a model if the model can be reasonably well learned on the given training data.
    In comparison with the learning procedure without this adaptation and a good threshold, however, we now compare the mean over only the "easy" splits with the mean over all splits - an unfair advantage.
2.  Penalize failing learners.
    If a score is missing, we can simply impute the worst possible score (as defined by the `r ref("Measure")`) and thereby heavily penalize the learner for failing.
    However, this often seems too harsh for many problems, and for some measures there is no reasonable value to impute.
3.  Impute a value that corresponds to a (weak) baseline.
    Instead of imputing with the worst possible score, impute with a reasonable baseline, e.g., by just predicting the majority class or the mean of the response in the training data.
    Such simple baselines are implemented as featureless learners (`r ref("mlr_learners_classif.featureless")` or `r ref("mlr_learners_regr.featureless")`).
    Note that a reasonable baseline value is different in different training splits.
    Retrieving these values after a larger benchmark study has been conducted is possible, but tedious.

We strongly recommend option (3): it is statistically sound and very flexible.
To make this procedure very convenient during resampling and benchmarking, we support fitting a proper baseline with a fallback learner.
In the next example, in addition to the debug learner, we attach a simple featureless learner to the debug learner.
So whenever the debug learner fails (which is every single time with the given parametrization) and encapsulation is enabled, `r mlr3` falls back to the predictions of the featureless learner internally:

```{r technical-016}
task = tsk("penguins")

learner = lrn("classif.debug")
learner$param_set$values = list(error_train = 1)
learner$fallback = lrn("classif.featureless")

learner$train(task)
learner
```

Note that we don't have to enable encapsulation explicitly; it is automatically set to `"evaluate"` for the training and the predict step while setting a fallback learner for a learner without encapsulation enabled.
Furthermore, the log contains the captured error (which is also included in the print output), and although we don't have a model, we can still get predictions:

```{r technical-017}
learner$model
prediction = learner$predict(task)
prediction$score()
```

In this stepwise train-predict procedure, the fallback learner is of limited use.
However, it is invaluable for larger benchmark studies.

In the following snippet, we compare the previously created debug learner with a simple classification tree.
We re-parametrize the debug learner to fail in roughly 30% of the resampling iterations during the training step:

```{r technical-018}
learner$param_set$values = list(error_train = 0.3)

bmr = benchmark(benchmark_grid(tsk("penguins"), list(learner, lrn("classif.rpart")), rsmp("cv")))
aggr = bmr$aggregate(conditions = TRUE)
aggr[, .(learner_id, warnings, errors, classif.ce)]
```

Even though the debug learner occasionally failed to provide predictions, we still got a statistically sound aggregated performance value which we can compare to the aggregated performance of the classification tree.
It is also possible to split the benchmark up into separate `ResampleResult` objects which sometimes helps to get more context.
E.g., if we only want to have a closer look into the debug learner, we can extract the errors from the corresponding resample results:

```{r technical-019}
rr = aggr[learner_id == "classif.debug"]$resample_result[[1L]]
rr$errors
```

A similar problem to failed model fits emerges when a learner predicts only a subset of the observations in the test set (and predicts `NA` or no value for others).
A typical case is, e.g., when new and unseen factor levels are encountered in the test data.
Imagine again that our goal is to benchmark two algorithms using cross-validation on some binary classification task:

*   Algorithm A is an ordinary logistic regression.
*   Algorithm B is also an ordinary logistic regression, but with a twist:
    If the logistic regression is rather certain about the predicted label (> 90% probability), it returns the label and returns a missing value otherwise.

Clearly, at its core, this is the same problem as outlined before.
Algorithm B would easily outperform algorithm A, but you have not factored in that you can not generate predictions for all observations.
Long story short, if a fallback learner is involved, missing predictions of the base learner will be automatically replaced with predictions from the fallback learner.
This is illustrated in the following example:

```{r technical-020}
task = tsk("penguins")
learner = lrn("classif.debug")

# this hyperparameter sets the ratio of missing predictions
learner$param_set$values = list(predict_missing = 0.5)

# without fallback
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")

# with fallback
learner$fallback = lrn("classif.featureless")
p = learner$train(task)$predict(task)
table(p$response, useNA = "always")
```

Summed up, by combining encapsulation and fallback learners, it is possible to benchmark even quite unreliable or unstable learning algorithms in a convenient and statistically sound fashion.

### Actionable Errors

All problems demonstrated so far are artificial and non-actionable.
The usefulness of encapsulation and error logging usually only really becomes apparent in large benchmarks, especially in combination with parallelization.
For a fair comparison, you need to distinguish between the following cases:

(a) You have made a mistake, e.g., forgot a required preprocessing step in your pipeline. Action: Fix problems, restart computation.
(b) Temporary problems related to the executing system, e.g., network hiccups. Action: Restart computation.
(c) Intrinsic, deterministic and reproducible problem with the model fitting. Action: Impute with fallback learner.

The package [mlr3batchmark](https://github.com/mlr-org/mlr3batchmark) provides functionality to map jobs of a benchmark to computational jobs for the package `r ref_pkg("batchtools")`.
This provides a convenient way get fine-grained control over the execution of each single resampling iteration and then combine the results afterwards to a `r ref("BenchmarkResult")` again to proceed with the analysis.

## Data Backends {#sec-backends}

In mlr3, `r ref("Task", text = "Tasks")` store their data in an abstract data object, the `r ref("DataBackend")`.
A backend provides a unified API to retrieve subsets of the data or query information about it, regardless of how the data is actually stored.
The default backend uses `r ref_pkg("data.table")` via the `r ref("DataBackendDataTable")` as a very fast and efficient in-memory database.
For example, we can query some information of the `r ref("mlr_tasks_penguins")` task:

```{r}
task = tsk("penguins")
backend = task$backend
backend$nrow
backend$ncol
```

For bigger data, or when working with many tasks simultaneously in the same R session, it can be necessary to interface out-of-memory data to reduce the memory requirements.
This way, only the part of the data which is currently required by the learners will be placed in the main memory to operate on.
There are multiple options to archive this:

1.  `r ref("DataBackendDplyr")` which interfaces the R package `r ref_pkg("dbplyr")`, extending `r ref_pkg("dplyr")` to work on many popular databases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).
2.  `r ref("DataBackendDuckDB")` for the impressive [DuckDB](https://duckdb.org/) database connected via `r ref_pkg("duckdb")`: a fast, zero-configuration alternative to SQLite.
3.  `r ref("DataBackendDuckDB")`, again, but for [Parquet files](https://parquet.apache.org/).
    The data does not need to be converted to DuckDB's native storage format, you can work directly on directories containing one or multiple files stored in the popular Parquet format.

### Databases with DataBackendDplyr

To demonstrate the `r ref("DataBackendDplyr")` we use the NYC flights data set from the `r ref_pkg("nycflights13")` package and move it into a SQLite database.
Although `as_sqlite_backend()` provides a convenient function to perform this step, we construct the database manually here.

```{r technical-021, message = FALSE}
# load data
requireNamespace("DBI")
requireNamespace("RSQLite")
requireNamespace("nycflights13")
data("flights", package = "nycflights13")
str(flights)

# add column of unique row ids
flights$row_id = 1:nrow(flights)

# create sqlite database in temporary file
path = tempfile("flights", fileext = ".sqlite")
con = DBI::dbConnect(RSQLite::SQLite(), path)
tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))
DBI::dbDisconnect(con)

# remove in-memory data
rm(flights)
```

With the SQLite database stored in file `path`, we now re-establish a connection and switch to `r ref_pkg("dplyr")`/`r ref_pkg("dbplyr")` for some essential preprocessing.

```{r technical-022}
# establish connection
con = DBI::dbConnect(RSQLite::SQLite(), path)

# select the "flights" table, enter dplyr
library("dplyr")
library("dbplyr")
tbl = tbl(con, "flights")
```

First, we select a subset of columns to work on:

```{r technical-023}
keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",
  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")
tbl = select(tbl, all_of(keep))
```

Additionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:

```{r technical-024}
tbl = filter(tbl, !is.na(arr_delay))
```

To keep runtime reasonable for this toy example, we filter the data to only use every second row:

```{r technical-025}
tbl = filter(tbl, row_id %% 2 == 0)
```

The factor levels of the feature `carrier` are merged so that infrequent carriers are replaced by level "other":

```{r technical-026}
tbl = mutate(tbl, carrier = case_when(
  carrier %in% c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN") ~ "other",
  TRUE ~ carrier))
```

Next, the processed table is used to create a `r ref("mlr3db::DataBackendDplyr")` from `r mlr3db`:

```{r technical-027}
library("mlr3db")
b = as_data_backend(tbl, primary_key = "row_id")
```

We can now use the interface of `r ref("DataBackend")` to query some basic information about the data:

```{r technical-028}
b$nrow
b$ncol
b$head()
```

Note that the `r ref("DataBackendDplyr")` does not know about any rows or columns we have filtered out with `r ref_pkg("dplyr")` before, it just operates on the view we provided.

As we now have constructed a backend, we can switch over to `r mlr3` for model fitting and create the following `r mlr3` objects:

*   A `r ref("TaskRegr", text = "regression task")`, based on the previously created `r ref("mlr3db::DataBackendDplyr")`.
*   A regression learner (`r ref("mlr_learners_regr.rpart", text = "regr.rpart")`).
*   A resampling strategy: 3 times repeated subsampling using 2% of the observations for training ("`r ref("mlr_resamplings_subsampling", text = "subsampling")`")
*   Measures "`r ref("mlr_measures_regr.mse", text = "mse")`", "`r ref("mlr_measures_time_train", text = "time_train")`" and "`r ref("mlr_measures_time_predict", text = "time_predict")`"

```{r technical-029}
task = as_task_regr(b, id = "flights_sqlite", target = "arr_delay")
learner = lrn("regr.rpart")
measures = mlr_measures$mget(c("regr.mse", "time_train", "time_predict"))
resampling = rsmp("subsampling", repeats = 3, ratio = 0.02)
```

We pass all these objects to `r ref("resample()")` to perform a simple resampling with three iterations.
In each iteration, only the required subset of the data is queried from the SQLite database and passed to `r ref("rpart::rpart()")`:

```{r technical-030}
rr = resample(task, learner, resampling)
print(rr)
rr$aggregate(measures)
```

Note that we still have an active connection to the database.
To properly close it, we remove the `tbl` object referencing the connection and then close the connection.

```{r technical-031}
rm(tbl)
DBI::dbDisconnect(con)
```

### Parquet Files with DataBackendDuckDB

While storing the Task's data in memory is most efficient w.r.t. accessing it for model fitting, this has two major disadvantages:

1.  Although you might only need a small proportion of the data, the complete data frame sits in memory and consumes memory.
    This is especially a problem if you work with many tasks simultaneously.
2.  During parallelization, the complete data needs to be transferred to the workers which can cause significant overhead.

A very simple way to avoid this is given by just converting the `r ref("DataBackendDataTable")` to a `r ref("DataBackendDuckDB")`.
As we have already demonstrated how to operate on a SQLite database, and DuckDB is not different in that regard.
To convert a `data.frame` to DuckDB, we provide the helper function `r ref("as_duckdb_backend()")`.
Only two arguments are required: the `data.frame` to convert, and a `path` to store the data.

While this is useful while working with many tasks simultaneously in order to keep the memory requirements reasonable, the more frequent use case for DuckDB are nowadays [Parquet files](https://en.wikipedia.org/wiki/Apache_Parquet).
Parquet is a popular column-oriented data storage format supporting efficient compression, making it far superior to other popular data exchange formats such as CSV.

To demonstrate working with Parquet files, we first query the location of an example data set shipped with `r ref_pkg("mlr3db")`:

```{r}
path = system.file(file.path("extdata", "spam.parquet"), package = "mlr3db")
```

We can then create a `r ref("DataBackendDuckDB")` based on this file and convert the backend to a classification task, all without loading the dataset into memory:

```{r}
backend = as_duckdb_backend(path)
task = as_task_classif(backend, target = "type")
print(task)
```

Accessing the data internally triggers a query and data is fetched to be stored in an in-memory `data.frame`, but only the required subsets.

## Parameter Sets {#sec-paradox}

The `r ref_pkg("paradox")` package implements a language for systematically representing parameter spaces that is used throughout the `r ref_pkg("mlr3verse")`.
The first important use case is to define the space of valid parameter settings for learners and other objects.
Second, they are used to define search spaces for optimization algorithms.

While we have already learnt how to set parameters in @sec-training and to define tuning spaces in @sec-tuning-instance, this chapter will provide a more in-depth understanding of the involved concepts.
The two most important classes of the `r ref_pkg("paradox")` package are the:

*   `r ref("Param")` representing a single parameter
*   `r ref("ParamSet")` being a container object that combines multiple `r ref("Param")`s^\[This is an oversimplification, as the `ParamSet` itself can also hold information beyond what is stored in the individual parameters.]

Furthermore, there are two other important classes that represent intermediate objects.
These classes are used to provide a more user friendly interface to the classes above.

They are the:

*   `r ref("Domain")`, which is an intermediate object that is converted to a `r ref("Param")` object.
*   `r ref("TuneToken")` which allows for an convenient way to configure tuning spaces from existing `r ref("ParamSet")`s.

The *first* part of this section focuses on the definition of valid parameter settings for objects such as learners, while the *second* part deals with the definition of tuning spaces.

### Defining and Configuring Parameters {#sec-paradox-define-configure}

Within the `r ref_pkg("mlr3verse")`, `r ref("ParamSet")`s are ubiquitous.
A relatively simple example is the parameter set of the `r ref("PipeOpClassBalancing")`, which contains four parameters `ratio`, `reference`, `adjust`, and `shuffle`.
Akin to function arguments, they allow to configure the behaviour of the `r ref("PipeOp")` during training.
Unlike function arguments, these parameters are explicitly annotated with the type of values they expect.

The printed output below shows that the `ratio` must be a double(-like) value greater than 0, `reference` and `adjust` factors from a predefined set, and `shuffle` a logical flag.
None of the parameters has a default, but they have their `$values` slot initialized, which can be seen in the rightmost column.^\[The difference between default values and initial values is covered in @sec-technical-paradox-configure.]

```{r}
library("mlr3pipelines")
param_set = po("classbalancing")$param_set
param_set
```

We can inspect the individual parameters by accessing the field `$params`, which returns a named list of `r ref("Param")`s.
The printed information is similar to what we have already seen above, however it now also includes the available levels for the parameters `reference` and `adjust`.

```{r}
param_set$params
```

<!-- The simplest parameter set is the empty parameter set, which can for example be found in the measure for the mean squared error. -->

<!---->

<!-- ```{r} -->

<!-- #| echo: false -->

<!-- #| output: false -->

<!-- stopifnot(msr("regr.mse")$param_set$length == 0) -->

<!-- ``` -->

<!---->

<!-- ```{r} -->

<!-- msr("regr.mse")$param_set -->

<!-- ``` -->

#### Defining a Parameter Set {#sec-technical-paradox-define}

We can define a parameter set like seen earlier, by calling the `r ref("ps")` function.
The function takes named `r ref("Domain")` arguments that are converted into `r ref("Param")`s and creates a `r ref("ParamSet")` from them.
An empty parameter set -- albeit not yet very useful -- can be created by calling `ps()` without any  argumets.

```{r}
ps()
```

As a more interesting example, we can define a parameter set for a hypothetical statistical model that can either be fit by exactly solving a linear system (default), or by approximating the solution using a gradient method.
In case the solution is approximated, a tolerance level can be specified to determine a termination criterion for the gradient method.
We will call those parameters `approx` and `tol`.

In order to define this parameter set, we have to pick the right parameter types.
As of writing this book, there are five domain constructors that produce different parameters when passed as arguments to `ps()`.
The most flexible one is `r ref("p_uty()")` which is the fallback solution when none of the others apply.

| Constructor               | Description                          | Underlying Class    |
| :-----------------------: | :----------------------------------: | :-----------------: |
| `r ref("p_dbl")`          | Real valued parameter ("double")     | `r ref("ParamDbl")` |
| `r ref("p_int")`          | Integer parameter                    | `r ref("ParamInt")` |
| `r ref("p_fct")`          | Discrete valued parameter ("factor") | `r ref("ParamFct")` |
| `r ref("p_lgl")`          | Logical / Boolean parameter          | `r ref("ParamLgl")` |
| `r ref("p_uty")`          | Untyped parameter                    | `r ref("ParamUty")` |

: `r ref("Domain")` Constructors and their resulting `r ref("Param")`. {#tbl-paradox-define}

The most important arguments for these domain constructors allow to specify:

1.  the admissible set of values for the parameter; for example `lower` and `upper` for `p_int()` and `p_dbl()` or `levels` for `p_fct()`
2.  dependencies between parameters via the argument `depends`.
    It is an expression that must involve other parameters and be of the form `<param> == <scalar>`, `<param> %in% <vector>`, or multiple of these chained by `&&`
3.  the `default` value that allows to record the behaviour if no value is set
4.  parameter transformations via a `trafo`
5.  `tags` which allow to organize parameters.^\[An important example are the parameters of a learner, which are tagged with `"train"` and `"predict"` to annotate whether they influence the training or the prediction phase.]

For each of the domain objects, a `r ref("Param")` is created when passed to `ps()`.

Four our hypothetical method, we can create the parameter set as follows.

```{r}
param_set = ps(
  approx = p_lgl(default = TRUE),
  tol    = p_dbl(default = 10e-5, lower = 0, depends = approx == TRUE)
)
param_set
```

```{r}
param_set$params
```

However, we not only specified univariate constraints, but also dependencies between the parameters via the `depends` argument.
This information is recorded in the field `$deps`.

```{r}
param_set$deps
```

The rows in the table above can be read as follows: "Whether the parameter `<id>` can be set, depends on the parameter `<on>` satisfying the condition `<cond>`".
Constraints of the form `==` are converted to objects of class `r ref("CondEqual")`, while `%in%` is converted to `r ref("CondAnyOf")`.

#### Setting Parameter Values {#sec-technical-paradox-configure}

Once a parameter set is defined, we can set its state, i.e. the field `$values`.
The values in this slot are used when object that is configured in the parameter set is called, e.g. a learner used for training or prediction.

There are three ways to change the values of an existing parameter set:

| Method                                                      | Description                                        |
| :---------------------------------------------------------  | :-------------------------------------------------- |
| `param_set$values$<par> = <value>`                          | Changes only a single value                         |
| `param_set$values = list(<par1> = <value1>, ...)`           | Sets the provided values and unsets all other values |
| `param_set$set_values(<par1> = <value1>, ...)`              | Changes only the provided values                    |

: Setting Parameter Values {#tbl-paradox-configure}

When using either of these functions, the resulting parameter set is checked for compliance within the defined restrictions.

```{r}
#| error: true
# Parameter approx must be logical
param_set$set_values(approx = "no")
param_set$set_values(approx = FALSE)

# Parameter tol can only be set when approx is TRUE
param_set$set_values(tol = 1e-7)
param_set$set_values(approx = TRUE, tol = 1e-7)
```

#### Initial Values vs. Defaults {#sec-technical-paradox-defaults}

When working with `r mlr3` objects, it can be important to be aware of the difference between **initial parameter values** and **default values**.

To show their difference we inspect the parameter set of the classification partition tree, with a focus on the  `cp` and `xval` parameters.

```{r}
learner = lrn("regr.rpart")
learner$param_set
```

The parameter `cp` has a default of 0.01 and `xval` of 10.
However, the parameter `xval` is initialized to 0, which means that the slot `$values` looks as follows

```{r}
learner$param_set$values
```

The default value of a parameter indicates what value is used when no value is set.
In this case, the `r ref("rpart::rpart")` function that is called under the hood when training `r ref("LearnerClassifRpart")` uses the value 0.01 for `cp` if no value is provided and 10 for `xval`.

Because the value of `xval`, the `r ref("LearnerClassifRpart")` will not run with the default values (yes this is slightly confusing!).
Note that the initial parameter values are documented in the description of a `r ref("Learner")`.

This also has important practical consequences regarding how to set parameters.
Consider the difference between the following parameter sets.

```{r}
rpart1 = lrn("regr.rpart")
rpart1$param_set$set_values(cp = 0.3) # or rpart1$param_set$values$cp = 0.3
rpart1$param_set

rpart2 = lrn("regr.rpart")
rpart2$param_set$values = list(cp = 0.3)
rpart2$param_set$values
```

#### Combining Parameter Sets

In this section, we will learn how to combine multiple parameter sets.
In the `r mlr3verse` this is encountered when working with `r ref("Graph")`s from the `r ref_pkg("mlr3pipelines")` package.

Consider the following two `r ref("PipeOp")`s and their resulting `r ref("Graph")`.

```{r}
library("mlr3pipelines")
po_pca = po("pca")
po_rpart = po("learner", lrn("regr.rpart"))

graph = po_pca %>>% po_rpart
```

The resulting `graph` creates a `r ref("ParamSetCollection")` from the two parameter sets of the `r ref("PipeOp")`s.
Note that a `r ref("ParamSetCollection")` inherits from the class `r ref("ParamSet")` so it can be treated just the same.

```{r}
param_set_collection = ParamSetCollection$new(list(
  po_pca$param_set,
  po_rpart$param_set
))

class(param_set_collection)
```

In order to avoid id conflicts, the newly created parameters have their ids prefixed by their `$set_id`.

```{r}
param_set_collection$ids()

po_pca$param_set$set_id
po_rpart$param_set$set_id
```

### Defining Tuning Spaces {#sec-paradox-tune}

In this section we will cover how to define search spaces that can be used when optimizing black box functions with `r bbotk`.
A special instance of this problem is the optimization of hyperparameters of ML algorithms.
For a background on hyperparameter optimization within the `r ref_pkg("mlr3verse")` we refer to @sec-model-tuning.

#### Creating a Tuning Space from Scratch {#sec-paradox-tune-scratch}

Features of `r paradox` that are relevant for tuning, which were not relevant in the previous section, are the property of boundedness (via the field `$is_bounded`), the `r ref("TuneToken")` to conveniently define search spaces, and parameter transformations (`trafo`) to modify the sampling distribution.

As a guiding example, we will configure the tuning space for a `r ref("LearnerRegrSVM", text = "Support Vector Machine (SVM)")` from the `r ref_pkg("mlr3learners")` package.
If we want to tune a SVM, we have to configure the search space that is explored by the tuner.
Here the names, types, and valid ranges of each hyperparameter are important.
All this information is communicated with objects of the class `r ref("ParamSet")`, which is defined in `r ref_pkg("paradox")`.
This search space has to be bounded:

1.  `ParamFct` and `ParamLgl` are always bounded
2.  `ParamInt` and `ParamDbl` are bounded if `lower` and `upper` are finite
3.  `ParamUty` is never bounded.

One can check whether a `ParamSet` (or `Param`) is bounded by accessing the `$is_bounded` field.

```{r}
ps(cost = p_dbl(lower = 0.1, upper = 1))$is_bounded
ps(cost = p_dbl(lower = 0.1, upper = Inf))$is_bounded
```

One way to configure a tuning space is to define a `r ref("ParamSet")` analogously to @sec-technical-paradox-configure.

```{r optimization-028}
search_space = ps(
  cost   = p_dbl(lower = 0.1, upper = 1),
  kernel = p_fct(levels = c("polynomial", "radial"))
)
search_space
```

Optimizers such as `r ref("TunerGridSearch")` generate proposed points (aka a `r ref("Design")`) from a search space that is then explored.
Generating a grid with resolution 5, will result in the following points. ^\[Note that for factorial parameters all levels are used, independent of the resolution.]

```{r}
#| echo: false
#| label: fig-technical-paradox-tuning-vanilla
#| fig-cap: Design grid for tuning a SVM. The resolution is 5.
library(ggplot2)

dat1 = rbindlist(generate_design_grid(search_space, resolution = 5)$transpose(), fill = TRUE)
theme_set(theme_minimal())

ggplot(dat1, aes(cost, kernel)) +
    geom_point()
```

We notice that the `cost` parameter is taken on a linear scale.
We assume, however, that the difference of cost between 0.1 and 1 should have a similar effect as the difference between 1 and 10.
Therefore it makes more sense to tune it on a *logarithmic scale*.
This is done by using a **transformation** (`trafo`).
This is a function that is applied to a parameter after it has been sampled by the tuner.
We can tune `cost` on a logarithmic scale by sampling on the linear scale $\[-1, 1]$ and computing $10^x$ from that value.

```{r optimization-031}
search_space = ps(
  cost   = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial"))
)
```

```{r}
#| echo: false
#| label: fig-technical-paradox-tuning-log
#| fig-cap: Design grid for tuning a SVM. The resolution is 5 and the cost parameter on a logarithmic scale.
dat2 = rbindlist(generate_design_grid(search_space, 5)$transpose())

theme_set(theme_minimal())
ggplot(data = dat2, aes(cost, kernel)) +
    geom_point()
```

:::{.callout-tip}
Because the logscale tranformation is so common, the domain constructors `p_int()` and `p_dbl()` have a flag `logscale` that can be set to apply a logarithmic transformation.
:::

It is even possible to attach another transformation to the `r ref("ParamSet")` as a whole that gets executed after individual parameter's transformations were performed.
It is given through the `.extra_trafo` argument and should be a function with parameters `x` and `param_set` that takes a list of parameter values in `x` and returns a modified list.
This transformation can access all parameter values of an evaluation and modify them with interactions.
It is even possible to add or remove parameters.

In our example we now sssume that the parameter `cost` should be set to a higher value when the `kernel` is `"polynomial."`.

```{r optimization-032}
search_space = ps(
  cost = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial")),
  .extra_trafo = function(x, param_set) {
    if (x$kernel == "polynomial") {
      x$cost = x$cost + 2
    }
    x
  }
)
```

```{r}
#| echo: false
#| label: fig-technical-paradox-trafo
#| fig-cap: Design grid for tuning a SVM. The resolution is 5, the cost parameter is logaithmically transformed and when points with a `kernel` equal to `"polynomial"` are shifted to the right by a value of 2.
dat3 = rbindlist(generate_design_grid(search_space, 5)$transpose())
theme_set(theme_minimal())
ggplot(data = dat3, aes(cost, kernel)) +
    geom_point()
```

The available types of search space parameters are limited: continuous, integer, discrete, and logical scalars.
There are many machine learning algorithms, however, that take parameters of other types, for example vectors or functions.
These can not be defined in a search space `r ref("ParamSet")`, and they are often given as `r ref("ParamUty")` in the `r ref("Learner")`'s `r ref("ParamSet")`.^\[Recall that a parameter of type `r ref("ParamUty")` cannot be used in a search space because it is never bounded.]
When trying to tune over these hyperparameters, it is necessary to perform a Transformation that changes the type of a parameter.

An example is the `class.weights` parameter of the [Support Vector Machine](https://machinelearningmastery.com/cost-sensitive-svm-for-imbalanced-classification/) (SVM), which takes a named vector of class weights with one entry for each target class.
The trafo that would tune `class.weights` for the `r ref("mlr_tasks_spam", text = "span")` dataset could be:

```{r optimization-033}
search_space = ps(
  class.weights = p_dbl(lower = 0.1, upper = 0.9, 
    trafo = function(x) c(spam = x, nonspam = 1 - x)) 
)
```

A common use-case is the necessity to specify a list of values that should all be tried (or sampled from).
It may be the case that a hyperparameter accepts function objects as values and a certain list of functions should be tried.
Or it may be that a choice of special numeric values should be tried.
For this, the `r ref("p_fct")` constructor's `level` argument may be a value that is not a `character` vector, but something else.
If, for example, only the values 0.1, 3, and 10 should be tried for the `cost` parameter, even when doing random search, then the following search space would achieve that:

```{r optimization-034}
search_space = ps(
  cost = p_fct(c(0.1, 3, 10)),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This is equivalent to the following:

```{r optimization-035}
search_space = ps(
  cost   = p_fct(c("0.1", "3", "10"),
    trafo = function(x) list(`0.1` = 0.1, `3` = 3, `10` = 10)[[x]]),
  kernel = p_fct(c("polynomial", "radial"))
)
```

This may seem silly, but makes sense when considering that factorial tuning parameters are always `character` values:

```{r optimization-036}
search_space = ps(
  cost   = p_fct(c(0.1, 3, 10)),
  kernel = p_fct(c("polynomial", "radial"))
)
typeof(search_space$params$cost$levels)
```

:::{.callout-warning}
Be aware that this results in an "unordered" hyperparameter, however.
Tuning algorithms that make use of ordering information of parameters, like genetic algorithms or model based optimization, will perform worse when this is done.
For these algorithms, it may make more sense to define a `r ref("p_dbl")` or `r ref("p_int")` with a more fitting trafo.
:::

Like in @sec-technical-paradox-define it is possible to specify dependencies between parameters.
The SVM, for example, has the `degree` parameter that is only valid when `kernel` is `"polynomial"`.
As in the previous section, we can specify this constraint by using the `depends` argument of the domain constructor.

To tune the `degree` parameter, one would need to do the following:

```{r optimization-038}
search_space = ps(
  cost   = p_dbl(-1, 1, trafo = function(x) 10^x),
  kernel = p_fct(c("polynomial", "radial")),
  degree = p_int(1, 3, depends = kernel == "polynomial")
)
```

#### Creating Tuning ParamSets from other ParamSets {#sec-paradox-tune-token}

Having to define a tuning `r ref("ParamSet")` for a `r ref("Learner")` that already has parameter set information may seem unnecessarily tedious, and there is indeed a way to create tuning `r ref("ParamSet", "ParamSet")`s from a `r ref("Learner")`'s `r ref("ParamSet")`, making use of as much information as already available.

This is done by setting values of a `r ref("Learner")`'s `r ref("ParamSet")` to so-called `r ref("TuneToken")`s, constructed with a `r ref("to_tune")` call.
This can be done in the same way that other hyperparameters are set to specific values.
It can be understood as the hyperparameters being tagged for later tuning.
The resulting `r ref("ParamSet")` used for tuning can be retrieved using the `$search_space()` method.

```{r optimization-039}
learner = lrn("classif.svm")

learner$param_set$set_values(
  kernel = "polynomial", # for example
  degree = to_tune(lower = 1, upper = 3)
)

learner$param_set$search_space()
```

It is possible to omit `lower` here, because it can be inferred from the lower bound of the `degree` parameter itself.
For other parameters, that are already bounded, it is possible to not give any bounds at all, because their ranges are already bounded.
An example is the logical `shrinking` hyperparameter:

```{r optimization-040}
learner$param_set$set_values(shrinking = to_tune())

learner$param_set$search_space()

```

A `r ref("TuneToken")` can also be constructed with a `r ref("Domain")` object, i.e. something constructed with a `p_***` call.
This way it is possible to tune continuous parameters with discrete values, or to give trafos or dependencies.
One could, for example, tune the `cost` as above on three given special values, and introduce a dependency of `shrinking` on it.
Notice that a short form for `to_tune(<levels>)` is a short form of `to_tune(p_fct(<levels>))`.

:::{.callout-note}
When introducing the dependency, we need to use the `degree` value from *before* the implicit trafo, which is the name or `as.character()` of the respective value, here `"val2"`!
:::

```{r optimization-041}
learner$param_set$set_values(
  cost = to_tune(c(val1 = 0.3, val2 = 0.7)), 
  shrinking = to_tune(p_lgl(depends = cost == "val2"))
)

learner$param_set$search_space()
```

The search space picks up dependencies from the underlying `r ref("ParamSet")` automatically.
So if the `kernel` is tuned, then `degree` automatically gets the dependency on it, without us having to specify that.
(Here we reset `cost` and `shrinking` to `NULL` for the sake of clarity of the generated output.)

```{r optimization-042}
learner$param_set$values$cost = NULL
learner$param_set$values$shrinking = NULL
learner$param_set$values$kernel = to_tune(c("polynomial", "radial")) 

learner$param_set$search_space()
```

It is even possible to define whole `r ref("ParamSet")`s that get tuned over for a single parameter.
This may be especially useful for vector hyperparameters that should be searched along multiple dimensions.
This `r ref("ParamSet")` must, however, have an `.extra_trafo` that returns a list with a single element, because it corresponds to a single hyperparameter that is being tuned.
Suppose the `class.weights` hyperparameter should be tuned along two dimensions:

```{r optimization-043}
par = ps(
  spam = p_dbl(0.1, 0.9), 
  nonspam = p_dbl(0.1, 0.9), 
  .extra_trafo = function(x, param_set) {
    list(c(spam = x$spam, nonspam = x$nonspam))
  }
)
learner$param_set$set_values(class.weights = to_tune(par))
learner$param_set$search_space()
```

## Logging {#sec-logging}

We use the `r ref_pkg("lgr")` package for logging and progress output.

### Changing `r mlr3` logging levels

To change the setting for `r mlr3` for the current session, you need to retrieve the logger (which is a `r ref_pkg("R6")` object) from `r ref_pkg("lgr")`, and then change the threshold of the like this:

```{r technical-068, eval = FALSE}
requireNamespace("lgr")

logger = lgr::get_logger("mlr3")
logger$set_threshold("<level>")
```

The default log level is `"info"`.
All available levels can be listed as follows:

```{r technical-069}
getOption("lgr.log_levels")
```

To increase verbosity, set the log level to a higher value, e.g. to `"debug"` with:

```{r technical-070, eval = FALSE}
lgr::get_logger("mlr3")$set_threshold("debug")
```

To reduce the verbosity, reduce the log level to warn:

```{r technical-071, eval = FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
```

`r ref_pkg("lgr")` comes with a global option called `"lgr.default_threshold"` which can be set via `options()` to make your choice permanent across sessions.

Also note that the optimization packages such as `r mlr3tuning`  `r mlr3fselect` use the logger of their base package `r ref_pkg("bbotk")`.
To disable the output from `r mlr3`, but keep the output from `r mlr3tuning`, reduce the verbosity for the logger `r mlr3`
and optionally change the logger `r ref_pkg("bbotk")` to the desired level.

```{r technical-072, eval=FALSE}
lgr::get_logger("mlr3")$set_threshold("warn")
lgr::get_logger("bbotk")$set_threshold("info")
```

### Redirecting output

Redirecting output is already extensively covered in the documentation and vignette of `r ref_pkg("lgr")`.
Here is just a short example that adds an additional appender to log events into a temporary file in [JSON](https://en.wikipedia.org/wiki/JSON) format:

```{r technical-073, eval = knitr::is_html_output()}
tf = tempfile("mlr3log_", fileext = ".json")

# get the logger as R6 object
logger = lgr::get_logger("mlr")

# add Json appender
logger$add_appender(lgr::AppenderJson$new(tf), name = "json")

# signal a warning
logger$warn("this is a warning from mlr3")

# print the contents of the file
cat(readLines(tf))

# remove the appender again
logger$remove_appender("json")
```

### Immediate Log Feedback

`r mlr3` uses `r ref_pkg("future")` and [encapsulation](#encapsulation) to make evaluations fast, stable, and reproducible.
However, this may lead to logs being delayed, out of order, or, in case of some errors, not present at all.

When it is necessary to have immediate access to log messages, for example to investigate problems, one may therefore choose to disable `r ref_pkg("future")` and encapsulation.
This can be done by enabling the debug mode using `options(mlr.debug = TRUE)`; the `$encapsulate` slot of learners should also be set to `"none"` (default) or `"evaluate"`, but not `"callr"`.
This should only be done to investigate problems, however, and not for production use, because

1.  this disables parallelization, and
2.  this leads to different RNG behavior and therefore to results that are not reproducible when the debug mode is set.
