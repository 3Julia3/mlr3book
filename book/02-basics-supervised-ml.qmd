# Supervised Machine Learning {#r6}

{{< include _setup.qmd >}}

## Data and Tasks


Supervised machine learning concerns itself with relationships that can be expressed as data.
Possible examples are the relationship of engine characteristics with gas mileage in cars, the flower shapes of different kinds of plants, or the relationships between symptoms and progressions of different kinds of diseases.
When these relationships are patterns in repeatable processes that happen somewhat independently of each other, then one can try to fit *models* that make *predictions* about yet unseen occurrences of these relationships based on data previously collected, the *training data*.
These models then use some of the recorded variables, which we call *features*, to make predictions about a certain outcome, which we call the *target* variable.

While training data could be any kind of unstructured data such as images, videos, or sound files, in this book, we will concern ourselves with how to apply `r mlr_pkg("mlr3")` to *tabular data*.
This data is organized in such a way that it is contained in an `R` `data.frame` or `r cran_pkg("data.table")`, with one row for each training example and one column for each feature, as well as for the target.
The feature columns may contain a variety of scalar values, such as numerics (e.g., the age of a person) or categorical values (such as their marital status).
The target variable can be a single numeric value (for "regression" tasks), a single categorical value (for "classification" tasks), or various other types for more advanced supervised learning tasks.

## Models and Learners

The algorithm that uses training data to generate a model, which is used to make predictions, is called a `Learner` by `r mlr_pkg("mlr3")`.
The fitted model is represented by the learner as *model parameters*, which could, for example, be network weights in an artificial neural network, or split variables and split points in tree-based methods.
These parameters should not be confused with "hyperparameters", which are configuration settings of the learner, such as how many layers the neural network should have, or how many split points a classification tree should have.
The difference is that model hyperparameters are set before the training process and determine how this process operates, whereas the model parameters are determined *by* the training process.

## Predictions and Model Performance

After a learner has been trained, it can be used to make predictions.
For this, the learner is given the feature variables as inputs, and it produces some useful output.
This output can have the same type as the training target variable: a numeric point prediction for regression or a predicted category for classification.
However, some learners can produce more informative output, such as confidence intervals for regression or probability scores for the different possible classes in classification.
Because a learner can often efficiently make predictions for many samples at once, they take tabular data with one column for each feature and produce tabular output with one predicted row for each input row.
However, this is only for convenience: the model predictions should always be the same whether they are made one at a time or all in one go; the predictions made about one sample does not depend on the presence or absence of other samples that the model is making predictions about.

Once a model has made predictions, one may wonder what the quality of these predictions is: How often does a classification model predict the correct class, or what is the average absolute distance between the predicted value of a regression model and the actual "ground truth" value?
These performance measures can only be determined by using the model to make predictions about samples with a known ground truth value that have *not* been used to train the model, called the test set.
To get a better estimate of a model's performance, it is often beneficial to train the learner multiple times on different training data subsets, estimating performance on the respective remaining data (i.e.
the different test sets), and aggregating these performance values, e.g.
by averaging them.

:::{.callout-tip}
Terminology for some of the concepts used here may be different in other sources.
What is called a `Learner` here is also sometimes called an "inducer".
The "test set" is called the "validation set" in some circumstances and sometimes the "holdout set".
:::
