---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  In order to empirically evaluate machine learning algorithms, researchers not only need software tools, but also datasets.
  OpenML ([http://www.openml.org/](http://www.openml.org/)) is an open platform that facilitates the sharing and dissemination of machine learning research data. 
  It provides unique identifiers and standardized (meta-)data formats, which allow to easily find relevant datasets by querying for specific properties and to share them using their identifier.
  This chapter teaches some of the basic building blocks of OpenML and how it can be used via the interface package mlr3oml that conveniently integrates mlr3 and OpenML through the platform's REST API.
---

# OpenML {#openml}

{{< include _setup.qmd >}}

```{r}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")
```

`r ref_pkg("mlr3")` is a versatile machine learning framework that caters to the needs of both researchers focused on benchmarking, as well as applied data scientists seeking to solve real-world problems.
Practitioners and researchers in this field have different goals.
While the former are interested in solving concrete problems on specific datasets, the latter want to develop algorithms that perform well on a wide range of tasks.
Although applied data scientists may also find some parts of this chapter interesting, it is primarily intended for researchers who focus on the empirical evaluation of machine learning methods.

The standard scenario for empirical ML research is to compare a set of algorithms $A_1, ..., A_m$ by evaluating their performance on a set of tasks $T_1, ..., T_n$. 
So far in the book, we have mostly focused on the algorithms $A_i$ and the evaluation of the results, but not so much on how to easily find relevant tasks $T_i$.
However, to be able to derive meaningful conclusions from benchmark experiments, a good choice of the tasks $T_i$ is tantamount. 
It is therefore helpful to

1. have convenient access to a comprehensive collection of datasets and be able to filter datasets in this repository.
For example, we might be only interested in classification problems with many classes on large datasets.
2. be able to easily share these datasets with others, so that they can evaluate their methods on the same problems and thereby allow cross-study comparisons.

[OpenML](https://www.openml.org/) is a platform that facilitates the sharing and dissemination of machine learning research data and -- among many other things -- satisfies the two desiderata specified above.
Like mlr3, OpenML is free and open source.
Unlike mlr3, it is not tied to a programming language and is also useable from e.g. python or julia.
Its goal is to make it easier for researchers to find the data they need to answer scientific questions and to compare the results of their work with others.
Its design was guided by the [FAIR](https://www.go-fair.org/fair-principles/) principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The goal of these principles is to make scientific data and results more easily discoverable and reusable, promoting transparency, reproducibility, and collaboration in scientific research.

More concretely, OpenML is a repository for storing, organizing, and retrieving datasets, algorithms, and experimental results in a standardized way.
Entities have unique identifiers as well as standardized (meta-)data.
Everything can be accessed through a REST API or via the web interface: <https://openml.org>.
For example, the dataset with ID 31 -- the "credit-g" dataset -- can be accessed through the link <https://openml.org/d/31>.

In this chapter, we will cover the most important features of OpenML and how to use it via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform, as well as how to do it.

Note that while OpenML also contains representations of algorithms (Flows) and experiment results (Runs), we will only cover the OpenML Dataset, Task, and Task Collection in this section.
While the `r ref_pkg("mlr3oml")` package also supports working with flows and runs, datasets and tasks are what OpenML is primarily used for.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.
The following is a brief summarization of the OpenML entities that we will cover:

* [**Dataset**](https://www.openml.org/search?type=data\&status=active): A (usually tabular) data with additional meta-data. 
The latter includes e.g. a description of the dataset and its features, a license, or mathematical properties of the data.
It can be converted into a `r ref("DataBackend", text = "mlr3::DataBackend")`.
* [**Task**](https://www.openml.org/search?type=task\&sort=runs): A machine learning task, i.e. a concrete problem specification on a dataset.
    This also includes a split into train and test set, which differs from the mlr3 task. 
    For that reason, it can either be converted to a `r ref("Task", text = "mlr3::Task")`, or a `r ref("Resampling", text = "mlr3::Resampling")`.
<!-- * [**Flow**](https://www.openml.org/search?type=flow\&sort=runs): The representation of a machine learning algorithm or pipeline. The mlr3 counterpart is the `r ref("Learner", text = "mlr3::Learner")`. -->
<!-- * [**Run**](https://www.openml.org/search?type=run\&sort=date): An experiment result, which results from the application of a flow on a task. A conversion to a `r ref("ResampleResult", text = "mlr3::ResampleResult")` is possible. -->
* [**Task Collection**](https://www.openml.org/search?type=study&sort=tasks_included&study_type=task): A container object, which contains tasks.
Task collections allow for the creation of benchmark suites, which are designated tasks to evaluate the performance of certain machine learning algorithms, such as a benchmark suite for classification.
<!-- While a task collection would in mlr3 terms be simply a list of tasks, the run collection can be converted to a `r ref("BenchmarkResult", text = "mlr3::BenchmarkResult")`. -->


:::{.callout-note}
The mlr3oml package currently only supports downloading of objects.
Uploading objects to OpenML can be achieved via the website, for example.
:::

## Dataset {#openml-dataset}

The arguably most important entity on OpenML is the dataset.
When a researcher wants to access datasets on OpenML, the two possible situations are that they want to

1.  access specific datasets whose IDs they know, or to
2.  find datasets with specific properties.

We will start with the first scenario.
We might for example be interested in how the random forest implementation in `r ref_pkg("ranger")` compares with modern AutoML tools.
After some research, one might find the AutoML benchmark [@amlb2022] that compares 9 AutoML systems on over 100 classification and regression problems.
A sensible thing to do, would be to compare the performance of the random forest with the results from the AutoML benchmark.

Because they used datasets from OpenML and reported the IDs, we can easily retrieve these datasets.
As an example, we will access the dataset with ID 1590 which we can load it into R using the function `r ref("odt", text = "mlr3oml::odt")`, which returns an object of class `r ref("OMLData")`.
A difference to a typical `data.frame` in R is that this object comes with metadata that is accessible through its fields.
This includes for example a license or data qualities, which are properties of the dataset, including for example the number of features.

The example dataset we are using here, contains information about 48842 adults such as the *age* or the *education*, and the goal is usually to predict whether a person has an income above 50K dollars a year, which is indicated by the variable *class*.

```{r}
library("mlr3oml")

odata = odt(id = 1590)
odata
odata$license
odata$qualities["NumberOfFeatures", value]
```

The actual dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```

The corresponding `mlr3` object to the OpenML Dataset is the `r ref("DataBackend")` and we can convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.

```{r}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create an mlr3 task from the backend and use the mlr3 toolbox as usual.

```{r}
task = as_task_classif(backend, target = "class")
task
```

However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for querying the existing datasets for certain properties.
The `r ref("mlr3oml::list_oml_data")` allows to execute such queries.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(1, 4), 
  number_instances = c(100, 1000)
)
```

We can now inspect the returned `data.table`, where we only show a subset of the columns for readability.
We see indeed, that only datasets with the specified properties were returned.

```{r}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-info}
While the most common filters are hard-coded into the `r ref("list_oml_data")` function, other filters can be passed as well.
For an overview, see the REST tab of the OpenML API documentation: <https://www.openml.org/apis>.
:::

## Task {#openml-task}

While the previous section showed how to access and filter datasets, we will now focus on OpenML **tasks** that are built on top of datasets.
Like in mlr3, OpenML has different task types, including for example regression and classification.
And while we have ignored this issue in the previous section, merely using the same dataset that was used in a previous study is not enough to allow for cross-study comparisons.
Other important aspects are e.g. the choice of features and how the data was split into train- and test sets.
Luckily, the AutoML benchmark we mentioned earlier shares the Task IDs.

The accompanying task for the adult data that we used used earlier has ID 7592 and can be accessed through <https://openml.org/t/7592>.
We can load it into R using the `r ref("otsk")` function from the mlr3oml package, which returns an `r ref("OMLTask")` object.
When printing the task, we see that the dataset that is used by the task is indeed the adult data with ID 1590 from earlier.
Beyond the data ID, the printed output also informs us about the target variable^[This can deviate from default target used in the previous section.] and the data-split.

```{r}
otask = otsk(7592)
otask
```

Because an `r ref("OMLTask")` is built upon a dataset, we can also access the dataset via the `$data` field.

```{r}
otask$data
```

As OpenML tasks also contain training and test sets, they can be converted into either an mlr3 `r ref("Task")` or an instantiated `r ref("Resampling")`.

```{r}
resampling = as_resampling(otask)
task = as_task(otask)
```

As a short-form, it is also possible to create theses objects by using the `"oml"` task or resampling and providing the corresponding task ID.

```{r}
resampling = rsmp("oml", task_id = 7592)

task = tsk("oml", task_id = 7592)
```

If we wanted to compare the random forest with the results from the AutoML benchmark, we can simply run the experiment as follows. 
```{r}
#| eval: false
learner = as_learner(po("imputeoor") %>>% lrn("classif.ranger"))
rr = resample(task, rr, resampling)
rr$score(msr("classif.acc"))
```

```{r}
#| echo: false
learner = as_learner(po("imputeoor") %>>% lrn("classif.ranger"))
if (file.exists(file.path(getwd(), "openml", "rr.rds"))) {
  rr = readRDS(file.path(getwd(), "openml", "rr.rds"))
} else {
  rr = resample(task, learner, resampling)
  saveRDS(rr, file.path(getwd(), "openml", "rr.rds"))
}
rr$aggregate(msr("classif.acc"))
```

Analogously to the previous section however, we might want to filter the OpenML tasks for certain properties.
We might have e.g. come up with an algorithm that only works for binary classification tasks.
We can run this query by using the `r ref("list_oml_tasks")` function.
As earlier, we limit the response to 5 and show only a subset of the response for readability.

```{r}
otasks = list_oml_tasks(
  type = "classif", 
  number_classes = 2, 
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

<!-- ## Flow and Run {#openml-flow-run} -->
<!---->
<!-- OpenML not only allows to share datasets and tasks, but also the results of machine learning experiments. -->
<!-- To do so, OpenML uses the notion of the **flow** to represent machine learning algorithms and pipelines. -->
<!-- Continuing with the AutoML example from earlier, we can not only access the datasets and tasks that were used in the study, but also look at the results from the experiment. -->
<!-- One of the methods that was compared in the study is the h2o AutoML system by @ledell2020h2o that we will use as an example. -->
<!---->
<!-- In order to be able to assign experiment results to algorithms, there is an OpenML flow that corresponds to the h2o AutoML system. -->
<!-- It has ID 16115 and can be accessed using the function `r ref("oflw")` that creates an object of class `r ref("OMLFlow")`. -->
<!-- This flow contains information about its software dependencies and its parameters. -->
<!-- Because it is an AutoML system, there are only three parameters, which allow to set the number of cores, the allowed memory and the runtime. -->
<!---->
<!-- ```{r} -->
<!-- oflow = oflw(id = 16115) -->
<!-- oflow -->
<!-- oflow$parameter -->
<!-- ``` -->
<!---->
<!-- The flow itself however only becomes interesting in combination with the accompanying results. -->
<!-- A **run** is the application of a flow with a specific hyperparameter setting to a task. -->
<!---->
<!-- The run that resulted from from applying the h2o AutoML system to the adult task from earlier has ID 10363351. -->
<!-- It can be constructed using `r ref("orn")` that returns an object of class `r ref("OMLRun")`. -->
<!-- When printing it, we can see that it points to the flow and the task that we have mentioned earlier. -->
<!-- It also shows which resampling was used in the task. -->
<!---->
<!-- ```{r} -->
<!-- orun = orn(id = 10363351) -->
<!-- orun -->
<!-- ``` -->
<!---->
<!-- We can convert the OpenML run to its mlr3 pandeaunt, which is the `r ref("ResampleResult")`. -->
<!---->
<!-- ```{r} -->
<!-- rr_h2o = as_resample_result(orun) -->
<!-- ``` -->
<!---->
<!-- Note that in order to convert a run to a resample result, the OpenML Flow is also converted to a learner. -->
<!-- Because the flows from OpenML are not necessarily mlr3 learners, the resulting learner is not really executable and is essentially just a placeholder. -->
<!---->
<!-- If we wanted to compare the previous result from the random forest with h2o system, we can simply combine both resample results to a benchmark result. -->
<!-- We we see that -- probably unsurprisingly -- the standard ranger loses against the AutoML system. -->
<!---->
<!-- ```{r} -->
<!-- bmr = c(rr, rr_h2o) -->
<!-- bmr -->
<!---->
<!-- bmr$aggregate(msr("classif.acc"))[, .(learner_id, classif.acc)] -->
<!-- ``` -->


## Collection {#openml-collection}

The last entity that we will cover in this chapter is the **task collection**, which allows to bundle tasks.
One inherent problem of open platforms is that everyone can upload anything. 
This means that there are no quality guarantees for the datasets that are available through OpenML.
Benchmark Suites are a remedy to this problem, because it gives the opportunity to assemble and access high-quality collection of specific tasks that satisfy certain criteria.
Once again, we will use the AutoML benchmark as an illustrative example.
The classification tasks that were used in the study are summarized in the collection with ID 271, which can be accessed through [https://openml.org/s/271](https://openml.org/s/271). 

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML). 
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("ocl")` function.
The printed output shows that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r}
otask_collection
```


We can get an overview of these tasks by acessing the `$tasks` field.

```{r}
otask_collection$tasks[, .(id, data, target, task_splits)]
```


For a more exhaustive comparison between the random forest with the AutoML systems, we could create mlr3 tasks and resamplings from this information and evaluate the ranger on all of them.

<!-- The experiment results of the AutoML benchmark are store in the run collection that has ID 226. -->
<!---->
<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| output: false -->
<!---->
<!-- # Collections are not cached (because they can be altered on OpenML).  -->
<!-- # This is why we load it from disk -->
<!-- orun_collection = readRDS("./openml/orun_collection.rds") -->
<!-- ``` -->
<!---->
<!-- ```{r} -->
<!-- #| eval: false -->
<!-- orun_collection = ocl(id = 226) -->
<!-- ``` -->
<!---->
<!-- ```{r} -->
<!-- orun_collection -->
<!-- orun_collection$runs[, .(id, data, flow, task_splits)] -->
<!-- ``` -->
<!---->
