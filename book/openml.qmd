---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers not only need software tools, but also datasets.
  OpenML is a platform that aims to facilitate the sharing and dissemination of machine learning research data. 
  By providing standardized (meta-)data and unique identifiers, one can easily find relevant datasets and share them with others.
  This chapter teaches the fundamentals of OpenML, which is an open platform for sharing machine learning research data. 
  We will also show how mlr3oml coveniently integrates OpenML and mlr3.
---

# OpenML {#openml}

{{< include _setup.qmd >}}

```{r}
#| echo: false
#| output: false
set.seed(1)
options(mlr3oml.cache = "openml")

lgr::get_logger("mlr3oml")$set_threshold(NULL)
```

mlr3 is a versatile machine learning framework that caters to the needs of both methodological researchers focused on benchmarking, as well as applied data scientists seeking to solve real-world problems.
Practitioners and researchers in the field have different objectives.
Whereas the former are interested in solving concrete problems on specific datasets, the latter want to develop algorithms that perform well on a wide variety of tasks.
While applied data scientists might also find this chapter helpful, this chapter is primarily focused on researchers who conduct benchmark experiments.

When performing benchmark experiments, it is beneficial to

1. have convenient access to a comprehensive collection of datasets
1. be able to filter a data repository to easily get relevant datasets
1. be able to easily share research data with others

[OpenML](https://www.openml.org/) is a platform that aims to facilitate the sharing and dissemination of machine learning research data.
Its goal is to make it easier for researchers to find the data and algorithms they need to perform their experiments and to compare the results of their work with others. 
Its design was guided by the [FAIR principles](https://www.go-fair.org/fair-principles/), which stands for **F**indable, **A**ccessible, **I**nteroperable and **R**eusable.
The goal of these principles is to make scientific data and results more easily discoverable and usable by others, promoting transparency, reproducibility, and collaboration in scientific research.

This means that OpenML is a repository for storing, organizing, and retrieving datasets, algorithms, and experimental results in a standardized way.
Entities have unique identifiers and standardized (meta-data) that can be accessed through a REST API, as well as access through a web interface: [https://openml.org](https://openml.org).
The dataset with id 31 can for example be acces via [https://www.openml.org/d/31](https://www.openml.org/d/31).
Like mlr3, OpenML is free and open source.

In this chapter, we will teach the basic building blocks of OpenML and how to use it via the interface package `r ref_pkg("mlr3oml")`.

:::{.callout-note}
The mlr3oml package currently only supports downloading of objects.
Uploading objects to OpenML can be achieved via the website, for example.
:::

The most important entities on OpenML are the *Dataset*, *Task*, *Flow* and *Run*: 

* [Dataset](https://www.openml.org/search?type=data&status=active): A tabular dataset with rich metadata.
* [Task](https://www.openml.org/search?type=task&sort=runs): A machine learning task, i.e. a concrete problem specification on a dataset. 
This also contains a data-split, i.e. an instantiated resampling in mlr3 terminology.
* [Flow](https://www.openml.org/search?type=flow&sort=runs): The representation of a machine learning algorithm or pipeline.
* [Run](https://www.openml.org/search?type=run&sort=date): An experiment result, which results from the application of a flow on a task.

Furthermore, there is the *Collection*, which is a container object. A [*task collection*](https://www.openml.org/search?type=study&study_type=task`) contains tasks and a [*run collection*](https://www.openml.org/search?type=study&study_type=task) contains runs.

This conceptualization is similar to the central entities of mlr3.
A notable difference is that mlr3 tasks do not contain a data-split.
While we will briefly touching upon flows and runs, this chapter will be mostly focused on datasets and tasks, as these are the things openml is currently most used for.

## Dataset {#openml-dataset}

The arguably most important entity on OpenML is the **dataset**.
Like all objects on OpenML, hosted datasets are accessible through unique identifiers.
When a researcher wants to access datasets on OpenML, the two possible situations are that she wants to

1. access specific datasets with known ids
1. find datasets with specific properties.

We will show how to proceed in both scenarios, where we will start with the former.
We might have e.g. developed a new AutoML system and want to compare it with some already existing results.
For that we need to use the same datasets.
Luckily, the AutoML benchmark [@amlb2022] provides such datasets.

Because they used OpenML to share these datasets and reported the IDs in their paper, we can now easily retrieve these datasets.
As an example, we will access only the dataset with ID 55, which we can load it into R using the function `odt()`, which returns an object of class `r ref("OMLData")`.
A difference to a typical `data.frame` in R is that this object comes with metadata that is accessible through its fields. 
This includes for example a license or data qualities, which are properties of the dataset, which includes for example the number of features.

```{r}
library("mlr3oml")

odata = odt(55)
odata
odata$license
odata$qualities["NumberOfFeatures", value]
```

The actual dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```
The corresponding `mlr3` object to the OpenML Dataset is the `r ref("DataBackend")` and we can convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.

```{r}
backend = as_data_backend(odata)
backend
```

We can then continue to work with this backend, as we have seen throughout the book.

In the second scenario, one does not start out with an ID, but one wants to find certain datasets for an experiment.
We might for example only be interested in datasets with less than 4 features with 100 - 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for querying the existing datasets for certain properties.
The `r ref("mlr3oml::list_oml_data")` allows to execute such queries from R.
To keep the output readable, we only show the first 5 results from that query.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(0, 4), 
  number_instances = c(100, 1000)
)
```

We should only a subset of columns from the resulting table for readability.

```{r}
odatasets[, .(data_id, NumberOfFeatures, NumberOfInstances)]
```

We could now start to look deepter at the description of the datasets to verify whether they are suitable for our purposes.
For those datasets that satisfy our requirements, we can access them using the `r ref("odt")` function as shown above.


But not only does OpenML make finidng these datasets easier. 
If we used these datasets in our study, we can easily report the datasets by merely including their OpenML IDs.

:::{.callout-note}
While the most common filters are hard-coded into the `r ref("list_oml_data")` function, other filters can be passed as well.
For an overview, see the *REST* tab of the OpenML API documentation: [https://www.openml.org/apis](https://www.openml.org/apis). 
:::

## Task {#openml-task}

While the previous section showed how to access and filter datasets, we will now focus on **tasks** that are built on top of datasets.
Like in mlr3, OpenML has different task types, that requires certain metadata. 
The task types include e.g. regression, classification, clustering and survival analysis.
And while we have omitted this issue in the previous section, merely sharing a dataset that an experiment used, is of course not enough information to properly compare ones method with the results of others.
Another essential components is to share e.g. which target was used and how the data was split into train- and test set.

Luckily, the AutoML benchmark we mentioned earlier actually shares the Task IDs.

The task for the abalone data that was used has the ID 7592 and can be accessed through [https://openml.org/t/7592](https://openml.org/t/7592). 
We can access it using the `r ref("otsk")` function from the mlr3oml package, which returns a `r ref("OMLData")` object.
When printing the task, we see that indeed, the dataset the task uses is the adult dataset with ID 55.
Beyond this information, we see what the target variable is used and what resampling method.

```{r}
otask = otsk(7592)
otask
```

Because an OpenML Task contains also the training and test sets to be used, an `r ref("OMLTask")` object can be converted into either an mlr3 `r ref("Task")` or an instantiated `r ref("Resampling")`.

```{r}
resampling = as_resampling(otask)
task = as_task(otask)
```

As a short-form, it is also possible to create theses objects by using the `"oml"` task or resampling.

```{r}
resampling = rsmp("oml", task_id = 7592)

task = tsk("oml", task_id = 7592)
```

## Flow and Run {#openml-flow-run}


OpenML not only allows to share datasets and tasks, but also the results of machine learning experiments.
To do so, OpenML uses the notion of the **flow** to represent machine learning algotithms.
Continuing with the AutoML example from earlier, we can look at the resulting benchmark study that was conducted using the AutoML benchmark. 
They compare different AutoML systems on some datasets.
One of the methods that we will use an example is the h2o AutoML system.
It was assigned the ID 16115 and can be accessed using the function `r ref("oflw")` that creates an object of class `r ref("OMLFlow")`.

```{r}
oflow = oflw(16115)
```

As an AutoML system, it has of course only few parameters, which int his case are the numer of course, the memory it can use and the time it is allowed to run.

```{r}
oflow$parameters
```
Flows however only properly get their meaning when applied to tasks.

In the AutoML benchmark, they did exactly that. 
We can now retrieve the **run** that resulted from applying the h2o AutoML system to the aduklt task. 
The resulting run has ID 10363351. It can be constructed using `r ref("orn")` and returns an object of class `r ref("OMLRun")`.
When printing it, we can see that it points to the flow and the task that we have mentioned earlier. 
It also shows which resampling i used in the task.

```{r}
orun = orn(10363351)
orun
```


We can now convert the run to a mlr3 `r ref("ResampleResult")`.

```{r}
rr = as_resample_result(orun)
rr
```

Note that to do so, the OpenML Flow is also converted to a learner. Because the flows from OpenML are not necessarily mlr3 learners, the resulting learner is not really executable and is essentially just a placeholder.

```{r}
rr$learner
```

## Collection {#openml-collection}

THe last major feature from OpenML that we will cover is the collection.
So far we have only worked with atomic elements on OpenML. 
Because benchmark studies usually use a collection of tasks and experiments, OpenML allows to bundle them in collections.

There is the task collection and the run collection.

For the AutoML benchmark, both exists, i.e. the task collection that is used by the AutoML benchmark, as well as a collection of experiment results.

Both the run and task collection can be accessed through the `r ref("ocl")` function. 
In either case a `r ref("OMLCollection")` object is returned, but they can be distinguished by their printer as well as by inspecting the field `$main_entity_type`. 
We with accessing the AutoML task suite for classificaiton

```{r}
otask_collection = ocl(271)
otask_collection
otask_collection$task_ids
```



