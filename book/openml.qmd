---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers need not only software tools but also datasets.
  In addition, large-scale computational experiments require the use of high-performance computing (HPC) clusters.
  [OpenML (http://www.openml.org/), an open platform that facilitates the sharing and dissemination of machine learning research data, addresses the first of these issues.
  It provides unique identifiers and standardised (meta)data formats that make it easy to find relevant datasets by querying for specific properties, and to share them using their identifier.
  The first part of this chapter covers the basics of OpenML and shows how it can be used via the mlr3oml interface package, which conveniently integrates mlr3 and OpenML via its REST API.
  We then show how to simplify the execution of benchmark experiments on HPC clusters using the R package [batchtools](https://github.com/mllg/batchtools), which provides a convenience layer for interoperating with different scheduling systems in a standardised way.
---

# OpenML and Large-Scale Experiments {#sec-openml-benchmarking}

{{< include _setup.qmd >}}

```{r, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

@sec-performance illustrated how `r ref_pkg("mlr3")` can be used to benchmark machine learning algorithms, and @sec-parallelization showed how `r ref_pkg("future")` can be used to parallelize their execution.
Empirical evaluation of machine learning methods requires not only software tools, but also datasets.
Furthermore, if the computational experiments are large, there is a need to run them on high performance computing (HPC) clusters.
This chapter covers how to

1. run large-scale benchmark experiments on HPC clusters, and 
2. easily find relevant datasets.

For the first item, we will show how the R package `r ref_pkg("batchtools")` (@batchtools) and its mlr3 integration `r ref_pkg("mlr3batchmark")` can greatly simplify the effort required to interact with a scheduling system on an HPC cluster.
The second point will be addressed with the help of the OpenML platform by @openml2013.

<!-- Here we switch the batchtools/openml order because it is more natural, we first show the new terminology and then which role openml plays here -->
Since `r ref_pkg("batchtools")` is not bound to machine learning, but supports the execution of arbitrary benchmark experiments, we need to introduce some new terminology.
A common benchmarking scenario is to compare a set of *algorithms* $A_1, ..., A_n$ by evaluating their performance on a set of *problems* $P_1, ..., P_k$ using a performance *metric* $M$.
For example, the algorithms $A_i$ could be black-box optimisers and the problems $P_j$ could be functions to be minimised.
The associated performance metric $M$ could be the absolute difference between the global optimum and the best point found by the optimiser.
The goal of such a benchmark experiment is to explore which algorithm $A_i$ performs best on the problem set $\{P_j\}_{j \in 1, ..., k}$ for the given metric $M$.
An experiment is then defined by a tuple $(A_i, P_j)$.
Running the benchmark consists of running each of these experiments and evaluating the results using the measure $M$.
This is shown in the table below, where each row corresponds to an experiment result.

| Algorithm | Problem  | Metric     |
|-----------|----------|------------|
| $A_1$     | $P_1$    | $m_{1,1}$  |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_1$     | $P_k$    | $m_{n, 1}$ |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_n$     | $P_1$    | $m_{n, 1}$ |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_n$     | $P_k$    | $m_{n, k}$ |

: Illustration of a generic benchmark experiment using `r ref_pkg("batchtools")` terminology. {#tbl-generic-benchmark}


In the machine learning case, the algorithms $A_i$ are usually `r ref("Learner")`s $L_i$ and the problems `r ref("Task")`-`r ref("Resampling")` combinations $P_j = (T_j, A_j)$.
One experiment then consists of executing the resamling defined by the triple $(L_i, T_j, R_j)$ and scoring the result using metric $M$, which is a `r ref("mlr3::Measure")`.

:::{.callout: note}
Other categorizations are of course also possible. 
If one is interested in comparing different resampling methods, one could alternatively define the `r ref("Resampling")` as the algorithm and the `r ref("Task")`-`r ref("Learner")` combinations as the problem.
:::

Usually howeber, one is interested in the comparison of learners and table @tbl-generic-benchmark can be rewritten as shown below.
Here $m_{i,j}$ is a (scalar) performance metric that is computed by evaluating measure $M$ to the `r ref("ResampleResult")` that results from resampling learner $L_i$ on task $T_j$ using resampling method $R_j$.


| Algorithm | Problem      | Metric    |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment using `r ref_pkg("batchtools")` terminology. {#tbl-ml-benchmark}

The table above should seem familiar as it is essentially the result of calling the `$score()` method on a `r ref("BenchmarkResult")`.
The code below, compares a classification and partition tree with a random forest using a simple holdout resampling and three classification ctasks.

```{r}
library(mlr3learners)

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  lrns(c("classif.rpart", "classif.ranger")), 
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

bmr$score(msr("classif.acc"))
```

The post-processing of such an experiment can then be conducted using methods defined in `r ref_pkg("mlr3benchmark")` as was already shown in @sec-benchmarking.


**Batchtools**

Once we have set up the experiment design as in @tbl-generic-benchmark, the next step is to run it.
If the experiment is large, this is usually done using an HPC cluster.
While access to such clusters is common, the effort required to operate these systems is still considerable.
The R package `r ref_pkg("batchtools")` provides a framework for conveniently running large batches of computational experiments in parallel from R.
It is built around the concepts introduced earlier, namely the **algorithm**, **problem** and **experiment**.
As a result, it is highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimisation, simulation, and more.
The main features of the software are

* Convenience: All relevant batch system operations (submit, list, kill) are either handled internally or abstracted via simple R functions.
* Portability: With a well-defined interface, the source is independent of the underlying batch system - prototype locally, deploy on any high performance cluster.
* Reproducibility: Each computational part has an associated seed stored in a database, ensuring reproducibility even if the underlying batch system changes.
* Abstraction: The code layers for algorithms, problems, experiment definitions and execution are cleanly separated, allowing readable and maintainable code to be written to manage large-scale computational experiments.

In @sec-batchtools we will teach you how to use the package.

**OpenML**

In order to be able to draw meaningful conclusions from benchmark experiments, a good choice of the previously used tasks $T_i$ is tantamount. 
It is therefore helpful to

1. have convenient access to a large collection of datasets and be able to filter them for specific properties, and
2. be able to easily share these datasets with others so that they can evaluate their methods on the same problems and thus allow cross-study comparisons.

[OpenML](https://www.openml.org/) is a platform that facilitates the sharing and dissemination of machine learning research data and - among many other things - satisfies the two desiderata mentioned above.
Like mlr3, OpenML is free and open source.
Unlike mlr3, it is not tied to a programming language and can for example also be used from its Python interface (@feurer2021openml).
Its goal is to make it easier for researchers to find the data they need to answer the questions they pose.
Its design was guided by the [FAIR] (https://www.go-fair.org/fair-principles/) principles, which stand for **F**indability, **A**Accessibility, **I**Interoperability and **R**Reusability.
The purpose of these principles is to make scientific data and results more easily discoverable and reusable, thereby promoting transparency, reproducibility and collaboration in scientific research.

More specifically, OpenML is a repository for storing, organising and retrieving datasets, algorithms and experimental results in a standardised way.
Entities have unique identifiers and standardised (meta) data.
Everything is accessible via a REST API or via the web interface: <https://openml.org>.

In this chapter we will cover the main features of OpenML and how to use them via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform and how to do it.

OpenML has several types of objects that can be shared and accessed via the website or the API.
The following is a brief summary of the OpenML entities that we will cover:

* [**OML Dataset**](https://www.openml.org/search?type=data\&status=active): A (usually tabular) dataset with additional metadata. 
The latter may include a description of the dataset and its properties, a licence, or meta-features of the data.
When accessed via `r ref_pkg("mlr3oml")`, it can be converted to a `r ref("DataBackend", text = "mlr3::DataBackend")`.
* [**OML Task**](https://www.openml.org/search?type=task\&sort=runs): A machine learning task, i.e. a concrete problem specification on an OML dataset.
    This includes a split into a training and a test set, which differs from the mlr3 `r ref("Task")` definition. 
    For this reason it can be converted to either `r ref("Task", text = "mlr3::Task")` or `r ref("Resampling", text = "mlr3::Resampling")`.
* [**OML Task Collection**](https://www.openml.org/search?type=study&sort=tasks_included&study_type=task): A container object containing tasks. 
This allows the creation of benchmark suites, such as the OpenML CC-18 (@bischl2021openml), which is a curated collection of classification tasks.

For example, the dataset with ID 31 is the "credit-g" dataset and can be accessed through the link <https://openml.org/d/31>.
<!---->
<!-- ```{r} -->
<!-- library(mlr3oml) -->
<!-- library(mlr3) -->
<!-- odata = odt(31) -->
<!-- odata -->
<!-- as_data_backend(odata) -->
<!-- ``` -->
<!---->
Note that while OpenML also contains representations of algorithms (flows) and experiment results (runs), they will not be covered in this chapter.
While the `r ref_pkg("mlr3oml")` package also supports working with these objects, datasets and tasks are what OpenML is currently most used for.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.

## OpenML {#sec-openml}

### Dataset {#sec-openml-dataset}

Arguably the most important entity on OpenML is the dataset.
When someone wants to access datasets on OpenML, the two possible situations are that they want to

1. access specific datasets whose IDs they know, or
2. find datasets with specific properties.

We start with the first scenario.
As an exemplary use-case, we might be interested in how the random forest implementation in `r ref_pkg("ranger")` compares to modern AutoML tools.
After some research, one might find the AutoML benchmark [@amlb2022], where 9 AutoML systems are compared on over 100 classification and regression problems.
Our goal is then to compare the performance of the random forest with the results of this study.
By using the same tasks as in the AutoML benchmark, we avoid having to reevaluate the AutoML systems on a new set of tasks.
This will serve as the guiding example through this chapter.

Since the AutoML benchmark uses datasets from OpenML, we can easily retrieve these datasets using their IDs.
As an example, we can access the dataset with ID 1590. 
We load it into R using the function `r ref("mlr3oml::odt")`, which returns an object of the class `r ref("OMLData")`.

```{r}
library("mlr3oml")
odata = odt(id = 1590)
odata
```

This dataset contains information about 48842 adults -- such as their *age* or *education* -- and the goal is usually to predict whether a person has an income above 50K dollars per year, which is indicated by the variable *class*.
The major difference between a `r ref("OMLData")` object and a typical `data.frame` in R is that the former comes with additional metadata that is accessible through its fields.
This might include a licence or data *qualities*, which are properties of the datasets.

```{r}
odata$license
odata$qualities[1:4, ]
```
<!-- odata$qualities["NumberOfFeatures", value] -->

The underlying dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```

:::{.callout-note}
When the code snippets from this section are executed, various objects have to be downloaded from the OpenML server.
Many objects can be cached so that they don't have to be downloaded each time they are used.
This can be enabled by setting the `mlr3oml.cache` option to either `TRUE` or a specific path to be used as the cache folder.
:::

As we have now loaded the data of interest into R, the next step is to convert it into a format usable with mlr3.
The `r mlr3` class that comes closest to the OpenML Dataset is the `r ref("mlr3::DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.
This is a reoccuring theme throughout this section, i.e. the OpenML and mlr3 objects are well interoperable.

```{r}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create a `r ref("mlr3::Task")` from the `r ref("DataBackend")` and use the mlr3 toolbox as usual.

```{r}
task = as_task_classif(backend, target = "class")
task
```

However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
I.e. we might be second of the two scenarios outlined earlier, where we first have to find datasets matching our requirements.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for filtering the existing dataset.
The `r ref("mlr3oml::list_oml_data")` allows to execute such requests from R.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(1, 4), 
  number_instances = c(100, 1000)
)
```

We can inspect the returned `data.table`, where we only show a subset of the columns for readability.
By looking at the table we confirm that indeed only datasets with the specified properties were returned.

```{r}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-info}
While the most common filters are hard-coded into the `r ref("mlr3oml::list_oml_data")` function, other filters can be passed as well.
This includes the data qualities shown above.
For an overview, see the *REST* tab of the OpenML API documentation: <https://www.openml.org/apis>.
:::

### Task {#sec-openml-task}

While the previous section showed how to access, find and convert datasets, we will now focus on OpenML tasks that are built on top of the dataset object.
Similar to mlr3, OpenML has different types of tasks, such as regression and classification.
And while we ignored this issue in the previous section, simply using the same datasets from the AutoML benchmark is not enough to allow for a cross-study comparison.
Other important aspects include the choice of features and how the data was split into training and test sets.
Fortunately, the AutoML benchmark shares not only the data but also the task IDs.

The task associated with the adult data from earlier has task ID 359983 and can be accessed via <https://openml.org/t/359983>. 
We can load it into R using the `r ref("mlr3oml::otsk")` function, which returns an `r ref("OMLTask")` object.
From the output below we can see that the dataset from the task is indeed the adult data with ID 1590 from earlier.
In addition to the data ID, the printed output also tells us the task type, the target variable^[this may differ from the default target used in the previous section] and the estimation procedure.

```{r}
otask = otsk(359983)
otask
```

The `r ref("OMLData")` object associated with the underlying dataset can be accessed through the `$data` field.

```{r}
otask$data
```
The data splits associated with the estimation procedure are accessible through the field `$task_splits`.

```{r}
head(otask$task_splits)
```

The OpenML task can be converted to either an mlr3 `r ref("Task")` or an instantiated `r ref("ResamplingCustom")`.
For the first of these we will use the `r ref("as_task")` converter function.

```{r}
task = as_task(otask)
task
```

The resampling (instantiated on the task created above) can be created using the `r ref("as_resampling")` converter.

```{r}
resampling = as_resampling(otask)
resampling$task_hash == task$hash

resampling
```

As a shortcut, it is also possible to create the objects using the `"oml"` task or resampling using the `r ref("tsk")` and `r ref("rsmp")` constructors.

```{r}
resampling = rsmp("oml", task_id = 359983)
task = tsk("oml", task_id = 359983)
```

Continuing with our original goal of comparing the random forest implemented in `ranger` with modern AutoML tools, we define the learner of interest. 
As the adult dataset contains missing values, we perform out-of-range imputation before fitting the model using `r ref_pkg("mlr3pipelines")`.
We suppress the output during training by specifying `verbose = FALSE`.

```{r}
learner = po("imputeoor") %>>% lrn("classif.ranger", verbose = FALSE)
learner
```

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- learner = as_learner(po("imputeoor") %>>% lrn("classif.ranger")) -->
<!-- if (file.exists(file.path(getwd(), "openml", "rr.rds"))) { -->
<!--   rr = readRDS(file.path(getwd(), "openml", "rr.rds")) -->
<!-- } else { -->
<!--   rr = resample(task, learner, resampling) -->
<!--   saveRDS(rr, file.path(getwd(), "openml", "rr.rds")) -->
<!-- } -->
<!-- rr$aggregate(msr("classif.acc")) -->
<!-- ``` -->

We can then run the experiment as usual by calling `r ref("resample")`.

```{r}
rr = resample(task, learner, resampling)
rr$aggregate(msr("classif.acc"))
```

Because we used the same task definition and data-splits as in the AutoML benchmark, the resulting `r ref("ResampleResult")` is comparable with the results from the AutoML benchmark, that we can e.g. access through its website <https://openml.github.io/automlbenchmark/>.
The highest accuracy achieved on the adult dataset at the time of writing this book was achieved by the TPOT AutoML system (@olson2016tpot) with score of 88.27%, unsurprisingly beating our vanilla random forest.

:::{.callout-warning}
It is important to ensure that not only the `r ref("ResampleResult")` is comparable with the results of a previous study, but also that the same `r ref("Measure")` definition is used.
:::

While we have now learned how to access and use OpenML tasks with known IDs, the second common scenario is when one wants to find specific tasks on the OpenML website. 
As an example, suppose we are only interested in binary classification problems.
We can run this query by using the `r ref("mlr3oml::list_oml_tasks")` function and specifying the `task_type` and `number_classes` arguments.
We limit the response to 5 and show only selected columns for readability.

```{r}
otasks = list_oml_tasks(
  type = "classif", 
  number_classes = 2, 
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

We can confirm that only classification tasks with two classes are contained in the response.

### Task Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the task collection, which bundles existing OpenML tasks in a container object.
This allows for the creation of *benchmark suites*, which are curated collections of tasks, usually satisfying certain quality criteria.
Many research areas have such agreed upon benchmarks that are used to compare the strengths and weaknesses of methods empirically and to measure the progress of the field over time. 
One example for such a benchmark suite is the aforementioned AutoML benchmark that is used to compare existing AutoML methods.
The classification tasks of this benchmark suite are contained in the collection with ID 271, which can be accessed through <https://openml.org/s/271>.
Other benchmark suites that are available on OpenML are e.g. the OpenML CC-18 which contains curated classification tasks (@bischl2021openml) or a benchmark for tabular deep learning which includes regression and classification problems (@grinsztajn2022why).

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML). 
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output informs us that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r}
otask_collection
```


We can get an overview of these tasks by accessing the `$tasks` field.
For compactness, we only show a subset of the columns.

```{r}
otask_collection$tasks[, .(id, data, target, task_splits)]
```

Coming back to our original goal of comparing the `ranger` random forest implementation with existing AutoML systems, the next step would be to evaluate the it on the whole benchmark suite as opposed to only the *adult* task from above.
To do this, we first need to create the tasks and resamplings from the collection.
If we wanted to get *all* tasks and resamplings, we could achieve this using the converters `r ref("as_tasks")` and `r ref("as_resamplings")`.
For illustrative purposes (and in order to keep the runtime manageable), we only use the first 10 of the tasks.
<!-- And also because the albert dataset fails ... -->

```{r}
#| eval: false
ids = otask_collection$task_ids[1:10]
tasks = lapply(ids, function(id) tsk("oml", task))
resamplings = lapply(ids, function(id) rsmp("oml", task_id = id))

learner = po("imputeoor") %>>% lrn("classif.ranger")
```

```{r}
#| echo: false

learner = po("imputeoor") %>>% lrn("classif.ranger")

if (file.exists(file.path(getwd(), "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(getwd(), "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(otask_collection$task_ids[1:10], function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(getwd(), "openml", "resamplings.rds"))
}

if (file.exists(file.path(getwd(), "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(getwd(), "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(otask_collection$task_ids[1:10], function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(getwd(), "openml", "tasks.rds"))
}
```

Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality, which creates an experiment design from the cartesian product of all tasks, learners and resamplings.
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml")` function which can be used in such a scenario.

```{r}
large_design = benchmark_grid_oml(tasks, learner, resamplings)
head(large_design)
```

Because this is already a relatively large experiment (especially if we used all tasks), we will use this as the starting point for illustrating how the R package `r ref_pkg("batchtools")` can be used to run such an experiment on a HPC cluster.

## Batchtools {#sec-batchtools}

In this section we are concerned with the question of how to execute large-scale experiments on HPC clusters.

**HPC Cluster Architecture**

A High-Performance Computing (HPC) cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve. 
They are used to solve complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.
An HPC cluster typically consists of multiple compute nodes, each with multiple CPU and GPU cores, memory, and storage.
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task.
These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously.
The cluster's scheduling system manages the allocation of resources to different users or applications, ensuring that the resources are used efficiently and fairly. 
HPC clusters are used in a wide range of applications, including weather forecasting, scientific simulations, data analysis, and machine learning.

The most important difference between such a cluster and a personal computer (PC), is that one cannot directly control when jobs are executed, but has to submit them to a scheduling system.
This is because the system is shared with others.
A scheduling system for a High-Performance Computing (HPC) cluster is a software tool that manages the allocation of computing resources to users or applications on the cluster. 
The scheduling system ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the resources.

@fig-hpc below contains a rough sketch of such an architecture.

```{r}
#| label: fig-hpc
#| fig-cap: "Illustration of a HPC cluster architecture."
#| fig-align: "center"
#| fig-alt: "A rough sketch of the architecture of a HPC cluster."
#| echo: false
knitr::include_graphics("Figures/hpc.drawio.pdf")
```

As a guiding example, we use the random forest experiment where we have left off at the end of the previous section. 
We will first show how such an experiment can be executed on an HPC cluster using only a few lines of code. 
Afterwards we will walk through the code step-by-step and explain what is going on in detail.

The first step is always to create (or load) an experiment registry, using the function `makeExperimentRegistry()` (or `loadRegistry()`), which creates a special folder on the file-system at the provided path.
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")`. 
For example the algorithms $A_i$ and problems $P_j$ -- as defined in the introduction of this chapter -- are stored there..
Also the final results or log files can be found in this folder. 
The interaction with the objects stored in the registry however usually works via R functions. 

Afterwards, we have to replace the typical `benchmark()` call with the `batchmark()` function.
This defines the algorithms, problems, and experiments, but does not yet submit them.
Afterwards, the jobs can be submitted to the HPC cluster bu calling `submitJobs()`, which passes the job definitions to the scheduling system.
Once the results are computed -- i.e. `waitForJobs()` finishes -- we can obtain the benchmark result using `reduceBenchmarkResult()` which returns the actual `r ref("BenchmarkResult")`.



```{r}
#| eval: false
library(mlr3batchmark)
library(batchtools)

makeExperimentRegistry(NA)

batchmark(design)
submitJobs()
waitForJobs()

bmr = reduceBenchmarkResult()
```

We will now start with explaining the inner workings of `r ref_pkg("batchtools")` and then come back to this quick start example and explain it in more detail.
We will start with explaining the purpose of the experiment registry.
This is the basic folder through which everything in `r ref_pkg("batchtools")` happens.
One can pass a file directory (in this case `"registry"`) to determine where this folder is created. 
Note that you can use a temporary directory by setting this argument to `NA`.

```{r, cache = FALSE}
reg = makeExperimentRegistry(NA)
```

One can add Problems algorithms $A_i$, problems $P_j$, and Experiments to this Registry using the functions `addAlgorithm`, `addProblem()`, and `addExperiment()`.
In the standard benchmarking scenario, a problem represents a `r ref("Task")`-`r ref("Resampling")` combination and an algorithm is a learner. 
The experiment is then "Apply Algorithm $A_i$ to task $T_j$ using resampling $R_j$".


To add a problem we can call the `addProblem()` function.

However, batchtools not only supports algorithms and problems, but also allows to parametrize them.
Instead of adding different algorithms in the sense of learners, we add one algorithm that essentially does a 
"apply learner L to problem P. 

A algorithm in `r ref_pkg("batchtool")`. 
Such a problem has a `data` and `fun` argument, where the `data` part is the static part and the `fun` argument is a dynamic (possibly random part). The function `fun` is applied to the `data` and returns an `instance`. 

The algorithm is a function that takes in arguments `job`, `instance` and `data` and must perform an algorithm. 
<!-- It can take additional parameters. -->

We start by adding the problems that we have obtained from the AutoML benchmark in the previous section. 
Each data-point therefore consists of a resampling-task combination
```{r}
problems = lapply(1:10, function(i) list(resampling = resamplings[[i]], task = tasks[[i]]))
names(problems) = mlr3misc::ids(tasks)
```

We can now add the problems by walking along the list and adding the problem. 
The function that creates the problem from the data in our case just returns the data, we could make the problem here stochastic if we wanted to.
It would also be possible to parametrize the problem, which we don't need in this scenario. 
It works analogously to how we can parametrize algorithms, which we will show later.

```{r}
#| output: false
fun = function(job, data) data
mlr3misc::iwalk(problems, function(data, name) {
  addProblem(name, data = data, fun = fun)
})
```

Note that the argument `job` is created when running a job and contains information necessary to execute a single computational job. 
More on that will be shown later.
After adding the problems we can inspect them by adding the `$problems` field of the registry.

```{r}
reg$problems
```

As we have added all problems that we are considering, it's time to add the algorithms.
We have two ways to do this: 

1. Add a different Algorithm for each Learner
1. Add one parametrized algorithm.

The function that we are using to do this is the `addAlgorithm()` function that is similar to the `addProblem()` function in the sense that it registers an algorithm that can later be used to add experiments.
It takes in a unique identifier of the algorithm, i.e. its name, as well as an argument `fun`.
The function `fun` needs to take in the arguments `job`, `data`, and `instance` and possible furher arguments. 
The first of the two are identical to the `job` and `data` from the `addProblem()` explanation above. 
The `instance` is the output of the `fun` function from the problem. 
Further arguments can be used to parametrize the algorithm. 
These parameters can then be specified when actually creating experiments using the algorithms and problems.

In this case, we will use a parametrized algorithm. 
We will 

```{r}
run_learner = function(job, data, instance, num.trees) {
  resampling = instance$resampling
  task = instance$task

  learner = lrn("classif.ranger")

  rr = resample(task, learner, resampling, store_backends = FALSE)
}
addAlgorithm("run_learner", fun = run_learner)
```

In order for this code to work, we need to ensure that the `r ref_pkg("mlr3")` package is loaded in the worker. 
We can ensure this, by adding the package to the `$packages` field of the registry.
We need to save the registry afterwards to this information is stored.

```{r}
# reg$packages = "mlr3"
# saveRegistry("registry")
```

Once the problems and the algorithms are registered, we can start adding experiments on them!
This can be done using the `addExperiments()` function.
It takes in arguments `prob.designs` and `algo.designs.
Each of these argument has to be a named list, where the names are the names of the algorithm and problem respectively and the values are `data.frame`s with the parameters for the problems. 

In our example, we want to execute the random forest with 1, 10, 100 and 500 trees. 
In addition to specifying parameters for the algorithm or problem, we can also set the number of replications for the experiment using the parameter `repls`. 
We repeat every experiment 5 times.

```{r}
addExperiments(
  algo.designs = list(run_learner = data.frame(num.trees = c(1, 10, 100, 5000))), 
  repls = 5
)
```


Note however that we have still not submitted these jobs. 
All we have done is added the information how to run jobs to the registry.
Using the function `getJobTable()` we can access the jobs we have defined.
We only show a subset of the columns for simplicity and only print the first 5 rows.

```{r}
getJobTable()[1:5, c("job.id", "repl", "problem", "algorithm", "algo.pars")]
```

As we have now defined all the jobs that we want to send, it is time so submit them to the cluster, yeah!

The function that allows to do so is `submitJobs()`.
We can pass the ids we want to submit to the function. 
Per default, all jobs are submitted.
In addition, we can speicify the `resources` of a job that is a named list of arguments. 
Which resources can be specified and how to specify them depends on the underlying scheduling system.

But before we actually submit the jobs, we want to modify one more thing. 
Per default, each experiment gets submitted on a node as one job. 
However it sometimes is helpful to not execute each job ob one node, e.g. when one job does not take a large amount of time. 
Training a random forest with 4 different parameters does not take an insane amount of time, so a sensible decision seems to be to execute all the random forest experiments on a specific task on one node.
We can do this using the `chunk()` function hat takes in a `data.frame` with columns `job.id` and `chunk`. 

```{r}
chunks = getJobTable()[, list(job.id = .I, chunk = rep(.GRP, len = .N)), by = "problem"]
head(chunks)
```

We now submit all experiments using the same resources: 1 CPU Core, 1 hour walltime and 8 GB RAM.

```{r}
#| eval: false
submitJobs(chunks, resources = list(ncpus = 1, walltime = 3600, memory = 8000))
```

:::{.callout-tip}
Many functions like `getJobStatus()` are available that allow to track the progress of the jobs from within R. 
It is recommended to submit the jobs within a R Session that is running persistently even when closing the SSH connection. 
One option is to use TMUX.
:::

Once all the jobs are finished, we can obtain the results using e.g. the function `loadResult`. 
The results are also accessible through the `results` folder in the registry directory and are stored there as rds files.

```{r}
#| eval: false
results = lapply(getJobTable()$job.id, function(id) loadResult)

bmr = Reduce(results, c)

```





Before starting the jobs, it is recommended to use the function `testJob()` with a specified ID to make sure, that there is no bug in the code, before submitting the job to the actual cluster. 

All of this can be also tested manually on the personal machine!

Once we have started the job, we can inspect the status of a job using XXX.

The package `r ref_pkg("batchtools")` has much more functionality and we have just looked at only a part of its capabilities that is enough to get started. 

