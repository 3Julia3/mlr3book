---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  When computational experiments are large the need to execute them on High-performance computing (HPC) clusters arises.
  Furthermore, in order to empirically evaluate machine learning algorithms, researchers not only need software tools, but also datasets.
  [OpenML](http://www.openml.org/) is an open platform that facilitates the sharing and dissemination of machine learning research data and addresse the first of these issues.
  It provides unique identifiers and standardized (meta-)data formats, which allow to easily find relevant datasets by querying for specific properties and to share them using their identifier.
  We will first show how the execution of benchmark experiments on HPC clusters can be simplified using the R package [batchtools](https://github.com/mllg/batchtools), which provides a convenience layer for interoperating with different scheduling systems in a standardized way.
  The second part of this chapter teaches the OpenML basics and shows how it can be used via the interface package mlr3oml that conveniently integrates mlr3 and OpenML through its REST API.
---

# OpenML and Large-Scale Experiments {#sec-openml-benchmarking}

{{< include _setup.qmd >}}

```{r, cache = FALSE}
#| output: false
#| echo: false
#| eval: true
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")

library(mlr3batchmark)
library(batchtools)
```

We have seen earlier in chapter @sec-performance how `r ref_pkg("mlr3")` can be used for benchmarking machine learning algorithms.
However, when computational experiments are large the need to execute them on High-performance computing (HPC) clusters arises and the means of parallelization shown in @sec-parallelization might not be enough. 
Furthermore, in order to empirically evaluate machine learning methods, one not only needs software tools, but also datasets.
To solves these issues, this chapter covers how to

1. execute large-scale benchmark experiments on High-performance computing (HPC) clusters and 
1. easily find relevant datasets.

For the first issue, we will show how the R package `r ref_pkg("batchtools")` (@batchtools) and its mlr3 integration `r ref_pkg("mlr3batchmark")` can greatly simplify the efforts required to operate a scheduling system of a HPC cluster.
The second point will be addressed with the help of the OpenML platform by @openml2013.

Because the R package `r ref_pkg("batchtools")` is not tied to machine learning but supports the executing of arbitrary benchmark experiments, we have to introduce some new terminology.
A common benchmarking scenario is the comparison of a set of algorithms $A_1, ..., A_n$ by evaluating their performance on a set of problems $P_1, ..., P_k$, using a performance metric $M$.
For example, the algorithms $A_i$ might be black-box optimizers and the problems $P_j$ functions to minimize.
The accompanying performance metric $M$ could be the absolute difference between the global optimum and the best point found by the optimizer.
One experiment is then defined by a tuple $(A_i, P_j)$.
Running the benchmark consists of executing each of these experiment and scoring the results using the measure $M$.
This is depicted in the table below, where each row corresponds to one experiment result.

| Algorithm | Problem  | Measure    |
|-----------|----------|------------|
| $A_1$     | $P_1$    | $m_{1,1}$  |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_1$     | $P_k$    | $m_{n, 1}$ |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_n$     | $P_1$    | $m_{n, 1}$ |
| $\vdots$  | $\vdots$ | $\vdots$   |
| $A_n$     | $P_k$    | $m_{n, k}$ |

: Illustration of a Benchmark Experiment. {#tbl-generic-benchmark}


In the machine learning case, the algorithms $A_i$ are usually `r ref("Learner")`s and the problems are `r ref("Task")`-`r ref("Resampling")` combinations $P_j = (T_j, A_j)$.
One experiment then consists of conducting a `r ref("Resampling")` using the tuple $(A_i, T_j, R_j)$ and evaluating the result using measure $M$, which is a `r ref("mlr3::Measure")`.
Note that it would of course also be possible to define the `r ref("Resampling")`  method as the algorithm and `r ref("Task")`-`r ref("Resampling")` combinations as the problem.
What counts as the algorithm and what as the problem is therefore defined by whose properties are investigated in the experiment.

In the standard scenario however, we are interested in the learner as the algorithm and the experiment can be depicted like below.
Here $m_{i,j}$ is a scalar performance metric that arises from evaluating measure $M$ using the `r ref("ResampleResult")` that arises from the application of algorithm $A_i$ on task $T_j$ using resampling method $M_j$.

| Algorithm | Problem      | Measure    |
|-----------|--------------|------------|
| $L_1$     | $(T_1, R_1)$ | $m_{1,1}$  |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_1$     | $(T_k, R_k)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_1, R_1)$ | $m_{1, k}$ |
| $\vdots$  | $\vdots$     | $\vdots$   |
| $L_n$     | $(T_k, R_k)$ | $m_{n, k}$ |

: Illustration of a Machine-Learning Benchmark Experiment. {#tbl-ml-benchmark}

The table above should seem familiar as it is essentially the result of calling the `$score()` method on a `r ref("BenchmarkResult")`.
The code below, compares a classification and partition tree with a random forest using a simple holdout resampling and three classification ctasks.

```{r}
library(mlr3learners)

design = benchmark_grid(
  tsks(c("german_credit", "sonar", "breast_cancer")),
  lrns(c("classif.rpart", "classif.ranger")), 
  rsmp("holdout")
)

print(design)

bmr = benchmark(design)

bmr$score(msr("classif.acc"))
```


The initial question for the conduction of such an experiment is usually how algorithm $A_i$ compares with other algorithms $A_j$ where $i \neq j$.
We have already seen in @sec-benchmarking how the `r ref_pkg("mlr3benchmark")` extension can be used to analyze such results.

**Executing Large-Scale Experiments**

When dealing with large-scale benchmark experiments, the parallelization via future as shown e.g. in @sec-parallelization might not be enough and the need to run jobs on HPC clusters arises.
While access to such clusters is nowadays standard, the effort required to harness the computational power of these systems is still substantial.
But even after overcoming this hurdle, practitioners are burdened by spartanic tooling and little or no automation to support their typical computing workflow.

The R package `r ref_pkg("batchtools")` provides a framework for conveniently executing large batches of computational experiments in parallel from within R.
It provides functions for parallel execution, error handling, and result retrieval.
It is designed to be highly flexible, making it suitable for a wide range of computational experiments, including machine learning, optimization, simulation, and more.
The main features of the R package are:

* Convenience: All relevant batch system operations (submitting, listing, killing) are either handled internally or abstracted via simple R functions
* Portability: With a well-defined interface, the source is independent from the underlying batch system - prototype locally, deploy on any high performance cluster
* Reproducibility: Every computational part has an associated seed stored in a data base which ensures reproducibility even when the underlying batch system changes
* Abstraction: The code layers for algorithms, experiment definitions and execution are cleanly separated and allow to write readable and maintainable code to manage large scale computer experiments


The package allows users to define and execute a set of experiments such as shown in table @tbl-ml-benchmark, and automatically manage the process of executing them on a cluster or local machine. 
In @sec-large-experiments we will teach you how to simplify the execution of large-scale benchmark results using the package.

**OpenML**

In order to be able to derive meaningful conclusions from benchmark experiments, a good choice of the tasks $T_i$ used earlier is tantamount. 
It is therefore helpful to

1. have convenient access to a comprehensive collection of datasets and be able to filter datasets in this repository.
For example, we might be only interested in classification problems with many classes on large datasets.
2. be able to easily share these datasets with others, so that they can evaluate their methods on the same problems and thereby allow cross-study comparisons.

[OpenML](https://www.openml.org/) is a platform that facilitates the sharing and dissemination of machine learning research data and -- among many other things -- satisfies the two desiderata specified above.
Like mlr3, OpenML is free and open source.
Unlike mlr3, it is not tied to a programming language and is also useable from e.g. its Python interface by @feurer2021openml.
Its goal is to make it easier for researchers to find the data they need to answer scientific questions and to compare the results of their work with others.
Its design was guided by the [FAIR](https://www.go-fair.org/fair-principles/) principles, which stand for **F**indability, **A**ccessibility, **I**nteroperability and **R**eusability.
The goal of these principles is to make scientific data and results more easily discoverable and reusable, promoting transparency, reproducibility, and collaboration in scientific research.

More concretely, OpenML is a repository for storing, organizing, and retrieving datasets, algorithms, and experimental results in a standardized way.
Entities have unique identifiers as well as standardized (meta-)data.
Everything is accessible through a REST API or via the web interface: <https://openml.org>.

In this chapter, we will cover the most important features of OpenML and how to use it via the interface package `r ref_pkg("mlr3oml")`.
This will give you an idea of what you can do with the platform, as well as how to do it.

OpenML has different types of objects that can be shared and accessed through the website.
The following is a brief summarization of the OpenML entities that we will cover:

* [**OML Dataset**](https://www.openml.org/search?type=data\&status=active): A (usually tabular) data set with additional meta-data. 
The latter includes e.g. a description of the dataset and its features, a license, or meta-features of the data.
When accessed through `r ref_pkg("mlr3oml")`, it can be converted into a `r ref("DataBackend", text = "mlr3::DataBackend")`.
* [**OML Task**](https://www.openml.org/search?type=task\&sort=runs): A machine learning task, i.e. a concrete problem specification on an OML Dataset.
    This also includes a split into train and test set, thereby differing from the mlr3 `r ref("Task")` definition. 
    For that reason, it can either be converted to a `r ref("Task", text = "mlr3::Task")`, or a `r ref("Resampling", text = "mlr3::Resampling")`.
* [**OML Task Collection**](https://www.openml.org/search?type=study&sort=tasks_included&study_type=task): A container object, which contains tasks. 
This allows for the creation of benchmark suites, such as the OpenML CC-18 (@bischl2021openml) which is a curated collection of classification problems.

For example, the dataset with ID 31 is the "credit-g" dataset and can be accessed through the link <https://openml.org/d/31> or the `r ref_pkg("mlr3oml")` package:
<!---->
<!-- ```{r} -->
<!-- library(mlr3oml) -->
<!-- library(mlr3) -->
<!-- odata = odt(31) -->
<!-- odata -->
<!-- as_data_backend(odata) -->
<!-- ``` -->
<!---->
Note that while OpenML also contains representations of algorithms (flows) and experiment results (runs), we will only cover the OpenML Dataset, Task, and Task Collection in this chapter.
While the `r ref_pkg("mlr3oml")` package also supports working with Flows and runs, datasets and tasks are what OpenML is primarily used for.
For more information about these features, we refer to the OpenML website or the documentation of the `r ref_pkg("mlr3oml")` package.


## OpenML {#sec-openml}

### Dataset {#sec-openml-dataset}

The arguably most important entity on OpenML is the dataset.
When a researcher wants to access datasets on OpenML, the two possible situations are that they want to

1.  access specific datasets whose IDs they know, or to
2.  find datasets with specific properties.

We will start with the first scenario.
We might for example be interested in how the random forest implementation in `r ref_pkg("ranger")` compares with modern AutoML tools.
After some research, one might find the AutoML benchmark [@amlb2022] that compares 9 AutoML systems on over 100 classification and regression problems.
A sensible thing to do, would be to compare the performance of the random forest with the results from the AutoML benchmark.

Because the AutoML benchmark uses datasets from OpenML, we can easily retrieve these datasets through their IDs.
As an example, we will access the dataset with ID 1590 which we can load it into R using the function `r ref("mlr3oml::odt")`, which returns an object of class `r ref("OMLData")`.


The example dataset we are using here, contains information about 48842 adults such as the *age* or the *education*, and the goal is usually to predict whether a person has an income above 50K dollars a year, which is indicated by the variable *class*.
A difference between a `r ref("OMLData")` oject and a typical `data.frame` in R is that the former comes with additional metadata that is accessible through its fields.
This includes for example a license or data qualities, which are properties of the dataset, including for example the number of features.


```{r}
library("mlr3oml")

odata = odt(id = 1590)
odata
odata$license
odata$qualities["NumberOfFeatures", value]
```

The actual dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```

:::{.callout: tip}
Many objects can be cached, so they don't have to be downloaded every time the same dataset is accessed. 
You can enable caching by specifying the option `mlr3oml.cache` to either `TRUE` or to a specific path.
:::

The `r mlr3` class that comes closest to the OpenML Dataset is the `r ref("DataBackend")` and it is possible to convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.

```{r}
backend = as_data_backend(odata)
backend
```

In order to compare the random forest with the results from the AutoML benchmark, we could now create an mlr3 task from the backend and use the mlr3 toolbox as usual.

```{r}
task = as_task_classif(backend, target = "class")
task
```

However, it is not always a given that someone has already preselected appropriate datasets to evaluate a method.
I.e. we might be in the scenario where we first have to find datasets that fit our needs.
We might for example only be interested in datasets with less than 4 features and 100 to 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for querying the existing datasets for certain properties.
The `r ref("mlr3oml::list_oml_data")` allows to execute such queries.
To keep the output readable, we only show the first 5 results from that query by setting the limit to 5.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(1, 4), 
  number_instances = c(100, 1000)
)
```

We can now inspect the returned `data.table`, where we only show a subset of the columns for readability.
We see indeed, that only datasets with the specified properties were returned.

```{r}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking at the resulting datasets (accessible through their IDs) in more detail, in order to verify whether they are suitable for our purposes.

:::{.callout-info}
While the most common filters are hard-coded into the `r ref("list_oml_data")` function, other filters can be passed as well.
For an overview, see the *REST* tab of the OpenML API documentation: <https://www.openml.org/apis>.
:::

### Task {#sec-openml-task}

While the previous section showed how to access and filter datasets, we will now focus on OpenML **tasks** that are built on top of datasets.
Like mlr3, OpenML has different task types, including for example regression and classification.
And while we have ignored this issue in the previous section, merely using the same dataset that was used in a previous study is not enough to allow for cross-study comparisons.
Other important aspects are e.g. the choice of features and how the data was split into train- and test sets.
Luckily, the AutoML benchmark we mentioned earlier shares not only the data, but also the task IDs.

The accompanying task for the adult data that we used used earlier has ID 7592 and can be accessed through <https://openml.org/t/7592>.
We can load it into R using the `r ref("mlr3oml::otsk")` function from the mlr3oml package, which returns an `r ref("OMLTask")` object.
When printing the task, we see that the dataset that is used by the task is indeed the adult data with ID 1590 from earlier.
Beyond the data ID, the printed output also informs us about the target variable^[This can deviate from default target used in the previous section.] and the data-split.

```{r}
otask = otsk(7592)
otask
```

Because an `r ref("OMLTask")` is built upon a dataset, we can also access the dataset via the `$data` field.

```{r}
otask$data
```

Furthermore, the OpenML task also contains train and test splits that can be accessed through the field `$task_splits`.

```{r}
head(otask$task_splits)
```

For this reason, the OpenML task can be converted to both an mlr3 `r ref("Task")` or an instantiated `r ref("Resampling")`.

```{r}
resampling = as_resampling(otask)
task = as_task(otask)
```

As a short-form, it is also possible to create theses objects by using the `"oml"` task or resampling and providing the corresponding task ID.

```{r}
resampling = rsmp("oml", task_id = 7592)

task = tsk("oml", task_id = 7592)
```

If we wanted to compare the random forest with the results from the AutoML benchmark, we can simply run the experiment as follows. 
    Note that because the dataset contains missing values, we conduct out of range imputation before.
```{r}
#| eval: false
learner = po("imputeoor") %>>% lrn("classif.ranger")
rr = resample(task, learner, resampling)
rr$score(msr("classif.acc"))
```

```{r}
#| echo: false
learner = as_learner(po("imputeoor") %>>% lrn("classif.ranger"))
if (file.exists(file.path(getwd(), "openml", "rr.rds"))) {
  rr = readRDS(file.path(getwd(), "openml", "rr.rds"))
} else {
  rr = resample(task, learner, resampling)
  saveRDS(rr, file.path(getwd(), "openml", "rr.rds"))
}
rr$aggregate(msr("classif.acc"))
```

Analogously to the previous section, we might not know beforehand in which task IDs we are interested and we want to instead filter OpenML tasks for certain properties.
We might have e.g. come up with an algorithm that only works for binary classification tasks.
We can run this query by using the `r ref("mlr3oml::list_oml_tasks")` function.
Live above, we limit the response to 5 and show only a subset of the response for readability.

```{r}
otasks = list_oml_tasks(
  type = "classif", 
  number_classes = 2, 
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

### Collection {#sec-openml-collection}

The third and last OpenML entity that we will cover in this chapter is the **task collection**, which bundles tasks.
This allows for the creation of benchmark suites, which are curated sets of tasks that satisfy certain quality criteria.
The use of standardized benchmarks helps researchers and developers to identify the strengths and weaknesses of different algorithms and to improve their performance over time.
Even if one does not want to use a whole benchmark suites for one experiments, these curated collections are a good starting point to look for high-quality data as they have been selected with care.

The classification tasks that were used in the AutoML benchmark study are summarized in the collection with ID 271, which can be accessed through [https://openml.org/s/271](https://openml.org/s/271). 
Other benchmark suites that are available on OpenML are e.g. the OpenML CC-18 which contains curated classification tasks (@bischl2021openml) or a benchmark for tabular deep learning which includes regression and classification problems (@grinsztajn2022why).

<!-- The OpenML website prints the wrong number of tasks but this is an old bug that I have already reported long ago-->

```{r}
#| echo: false
#| output: false

# Collections are not cached (because they can be altered on OpenML). 
# This is why we load it from disk
otask_collection = readRDS("./openml/otask_collection.rds")
```

We can create an `r ref("OMLCollection")` object using the `r ref("mlr3oml::ocl")` function.
The printed output shows that the AutoML benchmark for classification contains 71 classification tasks on different datasets.

```{r}
#| eval: false
otask_collection = ocl(id = 271)
```

```{r}
otask_collection
```


We can get an overview of these tasks by accessing the `$tasks` field.
For compactness, we only show a subset of the column.

```{r}
otask_collection$tasks[, .(id, data, target, task_splits)]
```

Combing back to our original goal of comparing the random forest with existing AutoML systems, the next step would now be to evaluate the random forest on all these tasks.
To do so, we first need to create the the tasks and resamplings from the collection by calling the converters `r ref("as_tasks")` and `r ref("as_resamplings")`.
Because the tasks and resamplings are paired, we cannot simply use the `r ref("benchmark_grid")` functionality.
Instead, we can use the `r ref("mlr3oml::benchmark_grid_oml")` function.
Note that we only use a subset of the collection, to keep the runtime reasonable
<!-- And also because the albert dataset fails ... -->

```{r}
#| echo: true

learner = po("imputeoor") %>>% lrn("classif.ranger")

if (file.exists(file.path(getwd(), "openml", "resamplings.rds"))) {
  resamplings = readRDS(file.path(getwd(), "openml", "resamplings.rds"))
} else {
  resamplings = mlr3misc::map(otask_collection$task_ids[1:10], function(id) rsmp("oml", task_id = id))
  saveRDS(resamplings, file.path(getwd(), "openml", "resamplings.rds"))
}

if (file.exists(file.path(getwd(), "openml", "tasks.rds"))) {
  tasks = readRDS(file.path(getwd(), "openml", "tasks.rds"))
} else {
  tasks = mlr3misc::map(otask_collection$task_ids[1:10], function(id) tsk("oml", task_id = id))
  saveRDS(tasks, file.path(getwd(), "openml", "tasks.rds"))
}

large_design = benchmark_grid_oml(tasks, learner, resamplings)

head(large_design)
```


```{r}
#| eval: false
tasks = mlr3misc::map(otask_collection$task_ids[1:10], function(id) tsk("oml", task_id = id))
resamplings = mlr3misc::map(otask_collection$task_ids[1:10], function(id) tsk("oml", task_id = id))
learner = po("imputeoor") %>>% lrn("classif.ranger")

large_design = benchmark_grid_oml(tasks, learner, resamplings)

head(large_design)
```

Because this is already a relatively large-scale experiment, we will pick this example up in the next section, where we show how to use `r ref_pkg("batchtools")` for the execution of large experiments on HPC clusters.

## Large-Scale Experiments {#sec-large-experiments}

In this section we are concerned with the question of how to execute large-scale experiments on HPC clusters.

**What is a HPC Cluster**

A High-Performance Computing (HPC) cluster is a collection of interconnected computers or servers that work together as a single system to provide computational power beyond what a single computer can achieve. 
They are used to solve complex problems in science, engineering, machine learning and other fields that require a large amount of computational resources.

An HPC cluster typically consists of multiple compute nodes, each with multiple CPU cores, memory, and storage. 
These nodes are connected together by a high-speed network, which enables the nodes to communicate and work together on a given task. 

These clusters are also designed to run parallel applications that can be split into smaller tasks and distributed across multiple compute nodes to be executed simultaneously. 
The cluster's scheduling system manages the allocation of resources to different users or applications, ensuring that the resources are used efficiently and fairly. 
HPC clusters are used in a wide range of applications, including weather forecasting, scientific simulations, data analysis, and machine learning.


The most important difference between such a cluster and a personal computer (PC), is that one cannot directly control when jobs are executed, but has to submit them to a scheduling system.
A scheduling system for a High-Performance Computing (HPC) cluster is a software tool that manages the allocation of computing resources to users or applications on the cluster. 
The scheduling system ensures that multiple users and applications can access the resources of the cluster in a fair and efficient manner, and also helps to maximize the utilization of the resources.

The graphic below sketches the architecture of such a compute cluster.

```{r}
knitr::include_graphics("Figures/cluster_user_diagram.svg")
```

As a guiding example, we use the random forest experiment where we have left off at the end of the previous section. 
We will first show how such an experiment can be executed on an HPC cluster using only a few lines of code. 
Afterwards we will walk through the code step-by-step and explain what is going on in detail.

The first step is always to create (or load) an experiment registry, using the function `makeExperimentRegistry()` (or `loadRegistry()`), which creates a special folder on the file-system at the provided path.
This function constructs the inter-communication object for all functions in `r ref_pkg("batchtools")`. 
For example the algorithms $A_i$ and problems $P_j$ -- as defined in the introduction of this chapter -- are stored there..
Also the final results or log files can be found in this folder. 
The interaction with the objects stored in the registry however usually works via R functions. 

Afterwards, we have to replace the typical `benchmark()` call with the `batchmark()` function.
This defines the algorithms, problems, and experiments, but does not yet submit them.
Afterwards, the jobs can be submitted to the HPC cluster bu calling `submitJobs()`, which passes the job definitions to the scheduling system.
Once the results are computed -- i.e. `waitForJobs()` finishes -- we can obtain the benchmark result using `reduceBenchmarkResult()` which returns the actual `r ref("BenchmarkResult")`.



```{r}
#| eval: false
library(mlr3batchmark)
library(batchtools)

makeExperimentRegistry(NA)

batchmark(design)
submitJobs()
waitForJobs()

bmr = reduceBenchmarkResult()
```

We will now start with explaining the inner workings of `r ref_pkg("batchtools")` and then come back to this quick start example and explain it in more detail.
We will start with explaining the purpose of the experiment registry.
This is the basic folder through which everything in `r ref_pkg("batchtools")` happens.
One can pass a file directory (in this case `"registry"`) to determine where this folder is created. 
Note that you can use a temporary directory by setting this argument to `NA`.

```{r, cache = FALSE}
reg = makeExperimentRegistry(NA)
```

One can add Problems algorithms $A_i$, problems $P_j$, and Experiments to this Registry using the functions `addAlgorithm`, `addProblem()`, and `addExperiment()`.
In the standard benchmarking scenario, a problem represents a `r ref("Task")`-`r ref("Resampling")` combination and an algorithm is a learner. 
The experiment is then "Apply Algorithm $A_i$ to task $T_j$ using resampling $R_j$".


To add a problem we can call the `addProblem()` function.

However, batchtools not only supports algorithms and problems, but also allows to parametrize them.
Instead of adding different algorithms in the sense of learners, we add one algorithm that essentially does a 
"apply learner L to problem P. 

A algorithm in `r ref_pkg("batchtool")`. 
Such a problem has a `data` and `fun` argument, where the `data` part is the static part and the `fun` argument is a dynamic (possibly random part). The function `fun` is applied to the `data` and returns an `instance`. 

The algorithm is a function that takes in arguments `job`, `instance` and `data` and must perform an algorithm. 
<!-- It can take additional parameters. -->

We start by adding the problems that we have obtained from the AutoML benchmark in the previous section. 
Each data-point therefore consists of a resampling-task combination
```{r}
problems = lapply(1:10, function(i) list(resampling = resamplings[[i]], task = tasks[[i]]))
names(problems) = mlr3misc::ids(tasks)
```

We can now add the problems by walking along the list and adding the problem. 
The function that creates the problem from the data in our case just returns the data, we could make the problem here stochastic if we wanted to.
It would also be possible to parametrize the problem, which we don't need in this scenario. 
It works analogously to how we can parametrize algorithms, which we will show later.

```{r}
fun = function(job, data) data
mlr3misc::imap(problems, function(data, name) {
  addProblem(name, data = data, fun = fun)
})
```

Note that the argument `job` is created when running a job and contains information necessary to execute a single computational job. 
More on that will be shown later.
After adding the problems we can inspect them by adding the `$problems` field of the registry.

```{r}
reg$problems
```

As we have added all problems that we are considering, it's time to add the algorithms.
We have two ways to do this: 

1. Add a different Algorithm for each Learner
1. Add one parametrized algorithm.

The function that we are using to do this is the `addAlgorithm()` function that is similar to the `addProblem()` function in the sense that it registers an algorithm that can later be used to add experiments.
It takes in a unique identifier of the algorithm, i.e. its name, as well as an argument `fun`.
The function `fun` needs to take in the arguments `job`, `data`, and `instance` and possible furher arguments. 
The first of the two are identical to the `job` and `data` from the `addProblem()` explanation above. 
The `instance` is the output of the `fun` function from the problem. 
Further arguments can be used to parametrize the algorithm. 
These parameters can then be specified when actually creating experiments using the algorithms and problems.

In this case, we will use a parametrized algorithm. 
We will 

```{r}
run_learner = function(job, data, instance, num.trees) {
  resampling = instance$resampling
  task = instance$task

  learner = lrn("classif.ranger")

  rr = resample(task, learner, resampling, store_backends = FALSE)
}
addAlgorithm("run_learner", fun = run_learner)
```

In order for this code to work, we need to ensure that the `r ref_pkg("mlr3")` package is loaded in the worker. 
We can ensure this, by adding the package to the `$packages` field of the registry.
We need to save the registry afterwards to this information is stored.

```{r}
# reg$packages = "mlr3"
# saveRegistry("registry")
```

Once the problems and the algorithms are registered, we can start adding experiments on them!
This can be done using the `addExperiments()` function.
It takes in arguments `prob.designs` and `algo.designs.
Each of these argument has to be a named list, where the names are the names of the algorithm and problem respectively and the values are `data.frame`s with the parameters for the problems. 

In our example, we want to execute the random forest with 1, 10, 100 and 500 trees. 
In addition to specifying parameters for the algorithm or problem, we can also set the number of replications for the experiment using the parameter `repls`. 
We repeat every experiment 5 times.

```{r}
addExperiments(
  algo.designs = list(run_learner = data.frame(num.trees = c(1, 10, 100, 5000))), 
  repls = 5
)
```


Note however that we have still not submitted these jobs. 
All we have done is added the information how to run jobs to the registry.
Using the function `getJobTable()` we can access the jobs we have defined.
We only show a subset of the columns for simplicity and only print the first 5 rows.

```{r}
getJobTable()[1:5, c("job.id", "repl", "problem", "algorithm", "algo.pars")]
```

As we have now defined all the jobs that we want to send, it is time so submit them to the cluster, yeah!

The function that allows to do so is `submitJobs()`.
We can pass the ids we want to submit to the function. 
Per default, all jobs are submitted.
In addition, we can speicify the `resources` of a job that is a named list of arguments. 
Which resources can be specified and how to specify them depends on the underlying scheduling system.

But before we actually submit the jobs, we want to modify one more thing. 
Per default, each experiment gets submitted on a node as one job. 
However it sometimes is helpful to not execute each job ob one node, e.g. when one job does not take a large amount of time. 
Training a random forest with 4 different parameters does not take an insane amount of time, so a sensible decision seems to be to execute all the random forest experiments on a specific task on one node.
We can do this using the `chunk()` function hat takes in a `data.frame` with columns `job.id` and `chunk`. 

```{r}
chunks = getJobTable()[, list(job.id = .I, chunk = rep(.GRP, len = .N)), by = "problem"]
head(chunks)
```

We now submit all experiments using the same resources: 1 CPU Core, 1 hour walltime and 8 GB RAM.

```{r}
#| eval: false
submitJobs(chunks, resources = list(ncpus = 1, walltime = 3600, memory = 8000))
```

:::{.callout: tip}
Many functions like `getJobStatus()` are available that allow to track the progress of the jobs from within R. 
It is recommended to submit the jobs within a R Session that is running persistently even when closing the SSH connection. 
One option is to use TMUX.
:::

Once all the jobs are finished, we can obtain the results using e.g. the function `loadResult`. 
The results are also accessible through the `results` folder in the registry directory and are stored there as rds files.

```{r}
#| eval: false
results = lapply(getJobTable()$job.id, function(id) loadResult)

bmr = Reduce(results, c)

```





Before starting the jobs, it is recommended to use the function `testJob()` with a specified ID to make sure, that there is no bug in the code, before submitting the job to the actual cluster. 

All of this can be also tested manually on the personal machine!

Once we have started the job, we can inspect the status of a job using XXX.

The package `r ref_pkg("batchtools")` has much more functionality and we have just looked at only a part of its capabilities that is enough to get started. 

