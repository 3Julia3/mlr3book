---
author:
  - name: Sebastian Fischer
    orcid: 0000-0002-9609-3197
    email: sebastian.fischer@stat.uni-muenchen.de
    affiliations:
      - name: Ludwig-Maximilians-Universität München
abstract:
  To empirically evaluate machine learning algorithms, researchers not only need software tools, but also datasets. 
  OpenML is a platform that facilitates the sharing and dissemination of machine learning research data. 
  By providing standardized (meta-)data and unique identifiers, one can easily find research dataset and share it with others.
  This chapter teaches the basic building of OpenML and how it can be used via the interface package mlr3oml, 
  that conveniently integrates OpenML and mlr3.
---

# OpenML {#openml}

{{< include _setup.qmd >}}

```{r}
#| output: false
#| echo: false
set.seed(1)
options(mlr3oml.cache = file.path(getwd(), "openml", "cache"))
lgr::get_logger("mlr3oml")$set_threshold("off")
```

mlr3 is a versatile machine learning framework that caters to the needs of both researchers focused on benchmarking, as well as applied data scientists seeking to solve real-world problems.
Practitioners and researchers in this field have different goals.
While the former are interested in solving concrete problems on specific datasets determined by the problem at hand, the latter want to develop algorithms that perform well on a wide range of tasks.
Although applied data scientists may also find some parts of this chapter interesting, it is primarily intended for researchers developing new methods.

The starting point for machine learning researchers is usually that they want to compare some algorithms $A\_1, ... A\_m$ by comparing their performance on a set of tasks $T\_1, ..., T\_n$.
While the mlr3verse gives easy access to [many learners](https://mlr-org.com/learners.html) and the implementation of custom learners is covered in @sec-extending, the question of accessing relevant tasks $T\_i$ has so far not been addressed.
However, to be able to derive meaningful conclusions, a good choice of the tasks $T\_i$ is tantamount.
To do so, it is beneficial to have convenient access to a comprehensive collection of datasets and be able to filter this data repository for certain properties.
In order to enable other researchers to compare their algorithms with experiment results, it is also crucial to be able to easily share the results with others.

[OpenML](https://www.openml.org/) is a platform that aims to facilitate the sharing and dissemination of machine learning research data.
Its goal is to make it easier for researchers to find the data and algorithms they need to perform their experiments and to compare the results of their work with others.
Its design was guided by the [FAIR principles](https://www.go-fair.org/fair-principles/), which stands for **F**indable, **A**ccessible, **I**nteroperable and **R**eusable.
The goal of these principles is to make scientific data and results more easily discoverable and usable by others, promoting transparency, reproducibility, and collaboration in scientific research.
This means that OpenML is a repository for storing, organizing, and retrieving datasets, algorithms, and experimental results in a standardized way.
Entities have unique identifiers and standardized (meta-data) that can be accessed through a REST API, as well as access through a web interface: <https://openml.org>.
For example, the dataset with ID 31 -- the credit-g dataset -- can be accessed via <https://openml.org/d/31>.
Like mlr3, OpenML is free and open source.
In this chapter, we will teach the basic building blocks of OpenML and how to use it via the interface package `r ref_pkg("mlr3oml")`.

OpenML conceptualizes the machine learning workflow in terms of the Dataset, Task, Flow, and Run:
This ontology is similar to the central entities of mlr3 and mlr3oml allows to convert OpenML objects into their mlr3 pandeaunt.

*   [**Dataset**](https://www.openml.org/search?type=data\&status=active): A dataset with standardized meta-data. This includes e.g. a license, a data description and dataset properties. It can be converted into a `r ref("DataBackend", text = "mlr3::DataBackend")`.
*   [**Task**](https://www.openml.org/search?type=task\&sort=runs): A machine learning task, i.e. a concrete problem specification on a dataset.
    This also contains a data-split, which differs from mlr3. It can therefore be converted to a `r ref("Task", text = "mlr3::Task")`, as well as to a `r ref("Resampling", text = "mlr3::Resampling")` that is instantiated on this task
*   [**Flow**](https://www.openml.org/search?type=flow\&sort=runs): The representation of a machine learning algorithm or pipeline. The mlr3 counterpart is the `r ref("Learner", text = "mlr3::Learner")`.
*   [**Run**](https://www.openml.org/search?type=run\&sort=date): An experiment result, which results from the application of a flow on a task. A conversion to a `r ref("ResampleResult", text = "mlr3::ResampleResult")` is possible.

Furthermore, there is the Collection, which is a container object. A [**Task Collection**](https://www.openml.org/search?type=study\&study_type=task`) contains tasks and a [**Run Collection**](https://www.openml.org/search?type=study\&study_type=task) contains runs.
While a task collection would in mlr3 be simply a list of tasks, the run collection can be thought (and converted to) as a `r ref("BenchmarkResult")`.

While we will briefly touching upon flows and runs, this chapter will be mostly focused on datasets and tasks, as these are the things openml is currently most used for.

:::{.callout-note}
The mlr3oml package currently only supports downloading of objects.
Uploading objects to OpenML can be achieved via the website, for example.
:::

## Dataset {#openml-dataset}

The arguably most important entity on OpenML is the dataset.
When a researcher wants to access datasets on OpenML, the two possible situations are that she wants to

1.  access specific datasets with known IDs that are shared through OpenML.
2.  find datasets with specific properties.

We will start with the first scenario.
As a though experiment, you might imagine yourself having developed a new AutoML system, using the mlr3 tools we have covered in the book so far.
You are now interested in applying it to some datasets and comparing it with previous results.
After some research, you stumble upon the AutoML benchmark \[@amlb2022] that compare 9 AutoML systems on over 100 classification and regression problems.

Because they used datasets from OpenML and reported the IDs, we can now easily retrieve these datasets.
As an example, we will access the dataset with ID 1590 which we can load it into R using the function `r ref("odt")`, which returns an object of class `r ref("OMLData")`.
A difference to a typical `data.frame` in R is that this object comes with metadata that is accessible through its fields.
This includes for example a license or data qualities, which are properties of the dataset, including for example the number of features.
The dataset contains information about 48842 adults such as the *age* or the *education*, and the goal is usually to predict whether a person makes over 50K a year, which is indicated by the variable *class*.

```{r}
library("mlr3oml")

odata = odt(id = 1590)
odata
odata$license
odata$qualities["NumberOfFeatures", value]
```

The actual dataset can be accessed through the field `$data`.

```{r}
odata$data[1:2, ]
```

The corresponding `mlr3` object to the OpenML Dataset is the `r ref("DataBackend")` and we can convert the `r ref("OMLData")` object by calling `r ref("as_data_backend")`.

```{r}
backend = as_data_backend(odata)
backend
```

In order to evaluate your new AutoML system, you could now easily create a task from the backend and use your mlr3 toolbox as earlier in the book.
Note that we set the target value to the default target column *class*.

```{r}
task = as_task_classif(backend, target = "class")
task
```

However, one is not always so fortunate as in the example above, and we might have to go looking for datasets ourselves.
We might for example only be interested in datasets with less than 4 features with 100 - 1000 observations.
Because datasets on OpenML have such strict metadata, it allows for querying the existing datasets for certain properties.
The `r ref("mlr3oml::list_oml_data")` allows to execute such queries from R.
To keep the output readable, we only show the first 5 results from that query by setting `limit = 5`.

```{r}
odatasets = list_oml_data(
  limit = 5, 
  number_features = c(1, 4), 
  number_instances = c(100, 1000)
)
```

We can know inspect the returned `data.table`, where we only show a subset of the columns for readabilty.
We see indeed, that only datasets with the specified properties were returned.

```{r}
odatasets[, .(data_id, name, NumberOfFeatures, NumberOfInstances)]
```

We could now start looking more detailed at the description of the datasets to verify whether they are suitable for our purposes.
For those datasets that satisfy our requirements, we can access them using the `r ref("odt")` function as shown earlier.

:::{.callout-note}
While the most common filters are hard-coded into the `r ref("list_oml_data")` function, other filters can be passed as well.
For an overview, see the REST tab of the OpenML API documentation: <https://www.openml.org/apis>.
:::

## Task {#openml-task}

While the previous section showed how to access and filter datasets, we will now focus on **tasks** that are built on top of datasets.
Like in mlr3, OpenML has different task types, that requires certain metadata.
The task types include for example regression, classification, clustering and survival analysis.
And while we have omitted this issue in the previous section, merely sharing a dataset that was used in an experinent is not enough information to properly compare the new AutoML method with the results from the AutoML benchmark.
Another essential components is to share e.g. which features were used and how the data was split into train- and test set to make the results compareable.
Luckily, the AutoML benchmark we mentioned earlier actually shares the Task IDs.

The task for the adult data that was used has ID 7592 and can be accessed through <https://openml.org/t/7592>.
We can access it using the `r ref("otsk")` function from the mlr3oml package, which returns a `r ref("OMLTask")` object.
When printing the task, we see that indeed, the dataset the task uses is the adult dataset with ID 1590.
Beyond the data ID, the task also contains the target variable^\[This can deviate from default target used in the previous section.] or the data-split.

```{r}
otask = otsk(7592)
otask
```

Because an `r ref("OMLTask")` is built upon a dataset, we can also access the dataset via the `$data` field.

```{r}
otask$data
```

Because an OpenML Task contains also the training and test sets to be used, an `r ref("OMLTask")` object can be converted into either an mlr3 `r ref("Task")` or an instantiated `r ref("Resampling")`.

```{r}
resampling = as_resampling(otask)
task = as_task(otask)
```

As a short-form, it is also possible to create theses objects by using the `"oml"` task or resampling and providing the corresponding task ID.

```{r}
resampling = rsmp("oml", task_id = 7592)

task = tsk("oml", task_id = 7592)
```

We can know try out are awesome AutoML system and inspect its accuracy. 

```{r}
#| eval: false
awesome_automl_system = lrn("classif.awesome_automl_system")
rr_awesome = resample(task, awesome_automl_system, resampling)
rr_awesome$score(msr("classif.acc"))
```

```{r}
#| echo: false
awesome_automl_system = as_learner(po("imputeoor") %>>% lrn("classif.ranger"))
awesome_automl_system$id = "awesome_automl_system"
if (file.exists(file.path(getwd(), "openml", "rr_awesome.rds"))) {
  rr_awesome = readRDS(file.path(getwd(), "openml", "rr_awesome.rds"))
} else {
  rr_awesome = resample(task, awesome_automl_system, resampling)
  saveRDS(rr_awesome, file.path(getwd(), "openml", "rr_awesome.rds"))
}
rr_awesome$aggregate(msr("classif.acc"))
```

In the next section we will compare this result with another AutoML system.
But before we do so, we will show how it is possible to query OpenML for specific tasks.
We might have for example come up with an algorithm, that only works for binary classification tasks.
We can form this query by using the `r ref("list_oml_tasks")` function.
Like before, we only look for tasks with 100 - 1000 observations, 1 - 4 features.
As earlier, we limit the response to 5 and show only a subset of the response for readability.

```{r}
otasks = list_oml_tasks(
  type = "classif", 
  number_classes = 2, 
  limit = 5
)

otasks[, .(task_id, task_type, data_id, name, NumberOfClasses)]
```

## Flow and Run {#openml-flow-run}

OpenML not only allows to share datasets and tasks, but also the results of machine learning experiments.
To do so, OpenML uses the notion of the **flow** to represent machine learning algorithms and pipelines.
Continuing with the AutoML example from earlier, we can not only access the datasets and tasks that were used in the study, but also look at the results from the experiment.
One of the methods that was compared in the study is the h2o AutoML suyste
One of the methods that we will use an example is the h2o AutoML system @ledell2020h2o.

In order to be able to assign experiment results, there is an OpenML flow that corresponds to the h2o AutoML system.
It was assigned the ID 16115 and can be accessed using the function `r ref("oflw")` that creates an object of class `r ref("OMLFlow")`.
This flow contains information about its software dependencies and its parameters
This flow contains information about its software dependencies and its parameters
Because it is an AutoML system, there are only three, that allow to set the number of cores, the allowed memory and the runtime.

```{r}
oflow = oflw(id = 16115)
oflow$parameter
```

The flow itself however only becomes interesting in combination with the runs it generated
A **run** is the application of a flow with a specific hyperparameter setting to a task.

The run that resuled from from applying the h2o AutoML system to the adult task from earlier has ID 10363351.
It can be constructed using `r ref("orn")` and returns an object of class `r ref("OMLRun")`.
When printing it, we can see that it points to the flow and the task that we have mentioned earlier.
It also shows which resampling was used in the task.

```{r}
orun = orn(id = 10363351)
orun
```

We can now convert the run to an mlr3 `r ref("ResampleResult")`.

```{r}
rr_h2o = as_resample_result(orun)
```

Note that to compare a run to a benchmark result, the OpenML Flow is also converted to a learner.
Because the flows from OpenML are not necessarily mlr3 learners, the resulting learner is not really executable and is essentially just a placeholder.

If we wanted to compare out previous result from our new awesome AutoML system with h2o, we can simply combine both resample results to a benchmark result.
We we see that our results are worse than the h2o AutoML system, so there still seems to be room for improvement.

```{r}
bmr = c(rr_awesome, rr_h2o)

bmr$aggregate(msr("classif.acc"))[, .(learner_id, classif.acc)]
```


## Collection {#openml-collection}

The last concept that we will cover in this chapter is the **collection**.
A collection can either be a **run collection** or a **task collection**.

The former, can be used to represent benchmark experiments.
The latter is a way, to bundle tasks into a benchmark suite. 
Such benchmark suites, can e.g. be subsets of tasks with certain quality criteria.
These collections address the problem, that because of its inherent open nature, the quality of datasets and tasks is not verified.
For that reason, OpenML hosts some task collections that specify certain data quality criteria and are more trustworthy.

Once again, we will use the AutoML benchmark as an example, for which both types of collections exist.
The classification tasks for the study are summarized in the collection with ID 271.
We can create this object using the `r ref("ocl")` function for both types of collections.
It returns an object of type `r ref("OMLCollection")`.

```{r}
otask_collection = ocl(id = 271)
otask_collection
otask_collection$tasks[, .(id, data, target, task_splits)]
```

The results of the study are contained in the run collection with ID 226.

```{r}
orun_collection = ocl(id = 226)
orun_collection
orun_collection$runs[, .(id, data, flow, task_splits)]
```

TODO: 1. Ensure that no download info is shown
TODO: 2. Move the collections and the list returns to the manual cache
TODO: 3. Iterate again over the content
TODO: 4. Send it to Bernd
