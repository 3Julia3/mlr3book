## Resampling {#resampling}

Resampling strategies are usually used to assess the performance of a learning algorithm.
`mlr3` entails the following predefined [resampling](#resampling) strategies:
`r ref("mlr_resamplings_cv", text = "cross validation")` (`"cv"`),
`r ref("mlr_resamplings_loo", text = "leave-one-out cross validation")` (`"loo"`),
`r ref("mlr_resamplings_repeated_cv", text = "repeated cross validation")` (`"repeated_cv"`),
`r ref("mlr_resamplings_bootstrap", text = "bootstrapping")` (`"bootstrap"`),
`r ref("mlr_resamplings_subsampling", text = "subsampling")` (`"subsampling"`),
`r ref("mlr_resamplings_holdout", text = "holdout")` (`"holdout"`),
`r ref("mlr_resamplings_insample", text = "in-sample resampling")` (`"insample"`), and
`r ref("mlr_resamplings_custom", text = "custom resampling")` (`"custom"`).
The following sections provide guidance on how to set and select a resampling strategy and how to subsequently instantiate the resampling process.

Below you can find a graphical illustration of the resampling process:

```{r 02-basics-resampling-001, echo=FALSE}
knitr::include_graphics("images/ml_abstraction.svg")
```

### Settings {#resampling-settings}

In this example we use the **iris** task and a simple classification tree from the `r cran_pkg("rpart")` package.

```{r 02-basics-resampling-002}
task = tsk("iris")
learner = lrn("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach should be used.
`r mlr_pkg("mlr3")` resampling strategies and their parameters can be queried by looking at the data.table output of the `mlr_resamplings` dictionary:

```{r 02-basics-resampling-003}
as.data.table(mlr_resamplings)
```

Additional resampling methods for special use cases will be available via extension packages, such as `r gh_pkg("mlr-org/mlr3spatiotemporal")` for spatial data (still in development).

The model fit conducted in the [train/predict/score](#train-predict) chapter is equivalent to a "holdout resampling", so let's consider this one first.
Again, we can retrieve elements from the dictionary `r ref("mlr_resamplings")` via `$get()` or with the convenience function`r ref("rsmp()")`:

```{r 02-basics-resampling-004}
resampling = rsmp("holdout")
print(resampling)
```

Note that the `$is_instantiated` field is set to `FALSE`.
This means we did not actually apply the strategy on a dataset yet.
Applying the strategy on a dataset is done in the next section [Instantiation](#instantiation).

By default we get a .66/.33 split of the data.
There are two ways in which the ratio can be changed:

1. Overwriting the slot in `$param_set$values` using a named list:

```{r 02-basics-resampling-005}
resampling$param_set$values = list(ratio = 0.8)
```

2. Specifying the resampling parameters directly during construction:

```{r 02-basics-resampling-006}
rsmp("holdout", ratio = 0.8)
```

### Instantiation {#resampling-inst}

So far we just set the stage and selected the resampling strategy.

To actually perform the splitting and obtain indices for the training and the test split the resampling needs a `r ref("Task")`.
By calling the method `instantiate()`, we split the indices of the data into indices for training and test sets.
These resulting indices are stored in the `r ref("Resampling")` object:

```{r 02-basics-resampling-007}
resampling = rsmp("cv", folds = 3L)
resampling$instantiate(task)
resampling$iters
str(resampling$train_set(1))
str(resampling$test_set(1))
```

### Execution {#resampling-exec}

With a `r ref("Task")`, a `r ref("Learner")` and a `r ref("Resampling")` object we can call `r ref("resample()")`, which fits the learner to the task at hand according to the given resampling strategy.
This in turn creates a `r ref("ResampleResult")` object.

Before we go into more detail, let's change the resampling to a "3-fold cross-validation" to better illustrate what operations are possible with a `r ref("ResampleResult")`.
Additionally, when actually fitting the models, we tell `r ref("resample()")` to keep the fitted models by setting the `store_models` option to `TRUE`:

```{r 02-basics-resampling-008}
task = tsk("pima")
learner = lrn("classif.rpart", maxdepth = 3, predict_type = "prob")
resampling = rsmp("cv", folds = 3L)

rr = resample(task, learner, resampling, store_models = TRUE)
print(rr)
```

The following operations are supported with `r ref("ResampleResult")` objects:

Calculate the average performance across all resampling iterations:

```{r 02-basics-resampling-009}
rr$aggregate(msr("classif.ce"))
```

Extract the performance for the individual resampling iterations:

```{r 02-basics-resampling-010}
rr$score(msr("classif.ce"))
```

Check for warnings or errors:

```{r 02-basics-resampling-011}
rr$warnings
rr$errors
```

Extract and inspect the resampling splits:

```{r 02-basics-resampling-012}
rr$resampling
rr$resampling$iters
str(rr$resampling$test_set(1))
str(rr$resampling$train_set(1))
```

Retrieve the [learner](#learners) of a specific iteration and inspect it:

```{r 02-basics-resampling-013}
lrn = rr$learners[[1]]
lrn$model
```

Extract the predictions:

```{r 02-basics-resampling-014}
rr$prediction() # all predictions merged into a single Prediction
rr$predictions()[[1]] # prediction of first resampling iteration
```

Note that if you want to compare multiple [Learners](#learners) in a fair manner, it is important to ensure that each learner operates on the same resampling instance.
This can be achieved by manually instantiating the instance before fitting model(s) on it.

Hint: If your aim is to compare different `r ref("Task")`, `r ref("Learner")` or `r ref("Resampling")`, you are better off using the `r ref("benchmark()")` function which is covered in the [next section on benchmarking](#benchmarking).
It is a wrapper around `r ref("resample()")`, simplifying the handling of large comparison grids.

If you discover this only after you've run multiple `r ref("resample()")` calls, don't worry.
You can combine multiple `r ref("ResampleResult")` objects into a `r ref("BenchmarkResult")` (also explained in the [section benchmarking](#benchmarking)).

### Custom resampling {#resamp-custom}

Sometimes it is necessary to perform resampling with custom splits.
If you want to do that because you are coming from a specific modeling field, first take a look at the mlr3 extension packages, to check wheter your resampling method has been implemented already.
If this is not the case, feel welcome to extend an existing package or create your own extension package.

A manual resampling instance can be created using the `"custom"` template.

```{r 02-basics-resampling-015}
resampling = rsmp("custom")
resampling$instantiate(task,
  train = list(c(1:10, 51:60, 101:110)),
  test = list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

### Plotting Resample Results {#autoplot-resampleresult}

Again, `r mlr_pkg("mlr3viz")` provides a `r ref("ggplot2::autoplot()", text = "autoplot()")` method.

```{r 02-basics-resampling-016}
library("mlr3viz")

autoplot(rr)
autoplot(rr, type = "roc")
```

All available plot types are listed on the manual page of `r ref("autoplot.ResampleResult()")`.


### Train-Test-Validation split

In many situation, we want to work with a single set of fixed train, validation and test sets. As there is confusion wrt. the exact terms used for **train**, **test** and **validation** sets, we will follow the terminology provided in this [wikipedia article](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).


- **Train Set**: Used to train ML algorithms and preprocessing methods.

- **Validation Set**: Used to compare and choose hyperparameters, early stopping etc.

- **Test Set**: Used to obtain an un-biased estimate of the desired metric.

The attentive reader has surely observed, that those concepts do not directly match to the way **mlr3** handles such data splitting procedures.
As **mlr3** tries to stay general in order to offer a maximum of flexibility, simple splitting procedures are abstrated into **Resampling Methods** such as the ones obtained via `r ref("rsmp()")`.
In order to tune and select hyperparameters, instead of using a **validation split**, we choose a **inner** resampling method that is used for this selection.
This e.g. happens inside an `r ref("AutoTuner")` from `r mlr_pkg("mlr3tuning")`, where we provide an additional inner resampling method for tuning. If we now `r ref("resample()")` this `r ref("AutoTuner")`, we get the exact desired effect:
The `r ref("AutoTuner")` internally tunes the  hyperparameters of our algorithm and `r ref("resample()")` provides us with an uniased estimate of this tuned algorithms performance.

There are two approaches towards obtaining unbiased performance estimates from simple train-validation-test splits in **mlr3**.
Either using a `r ref("AutoTuner")` together with `r ref("resample()")`, or using by keeping test data out using mlr3's `row_roles` mechanism.
While the first approach is less flexible it is easy to understand and simple to apply. The second approach is slightly more complex, but also allows for a lot more flexibility and automation.


#### How can I keep out test data for prediction?


Let us assume we have the following pre-defined **train** and **test** `row_ids`.

```{r}
t = tsk("iris")
train = c(1:30, 51:80, 101:130)
test = c(31:50, 81:100, 131:150)
```

We can now look at the `task`'s `row_roles`

```{r}
t$row_roles
```

We can see, that there is one set for `r names(t$row_roles)[1]`  and one for `r names(t$row_roles)[2]`. We can now set our `test` set to `validation` in order to keep it out from future computations.

```{r}
t$set_row_role(test, "validation")
```

If we now create and instantiate a `holdout` resampling instance, we can see that
those rows will not be used:

```{r}
rdesc = rsmp("holdout")$instantiate(t)
sort(c(rdesc$train_set(1), rdesc$test_set(1)))
```

We are now going to use this instance to compare two versions of the `r ref("rpart")` learner with different hyperparameter settings, choose the better one and train this on the full data.

```{r}
resample(t, lrn("classif.rpart", cp = .1), rdesc)$score()
resample(t, lrn("classif.rpart", cp = .5), rdesc)$score()

# cp = 0.1 works better, train it and predict on validation rows.
l = lrn("classif.rpart", cp = .1)$train(t)
l$predict(t, row_ids = t$row_roles$validation)$score()
```


#### How can I get my pre-defined train-valid-test splits into mlr3?

This example will make use of the `r ref("AutoTuner")`, described later on in the [tuning](#tuning) section. If you are not famililar with those concepts yet, it is advised to read up that section and come back here. Conceptually what happens here is similar to the example above, we split into fixed sets, select the better hyperparameter configuration and fit the final model.
The difference is, that here the design (which configurations are tried), the selection and re-fitting are automated within the `r ref("AutoTuner")`.

First, we will create an **outer** and an **inner** resampling strategy.
Let us assume we have the following situation:

```{r}
t = tsk("iris")
train = c(1:30, 51:80, 101:130)
valid = c(25:40, 75:80, 125:140)
test = c(41:50, 91:100, 141:150)
```

We now instantiate two holdout sets as done in the [custom resamplings section](#resamp-custom). In the outer loop, we use `train` and `valid` during training and the test set during testing.

```{r}
outer = rsmp("custom")$instantiate(t,
  train = list(c(train, valid)),
  test = list(test)
)
```

The inner loop now takes care of splitting between `train` and `valid` for the selection of hyperparameters.

```{r}
inner = rsmp("custom")$instantiate(t,
  train = list(train),
  test = list(valid)
)
```

Now we create the `r ref("AutoTuner")`, choosing `inner` for hyperparameter tuning.

```{r}
library(paradox)
library(mlr3tuning)
learner = lrn("classif.rpart")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = trm("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = inner,
  measure = msr("classif.ce"),
  search_space = tune_ps,
  terminator = terminator,
  tuner = tuner
)
```

and resample this `r ref("AutoTuner")` using the `outer` resampling we defined above.

```{r}
resample(at, t, outer)
```


```{r, eval = FALSE}
library(mlr3)
t = tsk("iris")
train = c(1:30, 51:80, 101:130)
valid = c(25:40, 75:80, 125:140)
test = c(41:50, 91:100, 141:150)

outer = rsmp("custom")$instantiate(t,
  train = list(c(train, valid)),
  test = list(test)
)
inner = rsmp("custom")$instantiate(t,
  train = list(train),
  test = list(valid)
)

library(paradox)
library(mlr3tuning)
learner = lrn("classif.rpart")
tune_ps = ParamSet$new(list(
  ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  ParamInt$new("minsplit", lower = 1, upper = 10)
))
terminator = trm("evals", n_evals = 10)
tuner = tnr("random_search")

at = AutoTuner$new(
  learner = learner,
  resampling = inner,
  measure = msr("classif.ce"),
  search_space = tune_ps,
  terminator = terminator,
  tuner = tuner
)
```
>> Error: Resampling 'custom' may not be instantiated
