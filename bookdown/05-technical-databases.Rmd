## Database Backends {#backends}

In mlr3, `r ref("Task")`s store their data in an abstract data format, the `r ref("DataBackend")`.
The default backend uses `r cran_pkg("data.table")` via the `r ref("DataBackendDataTable")` as an in-memory database.

For larger data, or when working with many tasks in parallel, it can be advantageous to interface an out-of-memory data.
There are two alternative `r ref("DataBackend")`s which allow to work seamlessly with databases.

First, there is the excellent R package `r cran_pkg("dbplyr")` which extends `r cran_pkg("dplyr")` to work on many popular databases like [MariaDB](https://mariadb.org/), [PostgreSQL](https://www.postgresql.org/) or [SQLite](https://www.sqlite.org).
Second, we provide a special backend for [DuckDB](https://duckdb.org/) in `r ref("DataBackendDuckDB")` using package `r cran_pkg("duckdb")`.


### Use Case: NYC Flights

To generate a halfway realistic scenario, we use the NYC flights data set from package `r cran_pkg("nycflights13")`:

```{r 05-technical-databases-001}
# load data
requireNamespace("DBI")
requireNamespace("RSQLite")
requireNamespace("nycflights13")
data("flights", package = "nycflights13")
str(flights)

# add column of unique row ids
flights$row_id = 1:nrow(flights)

# create sqlite database in temporary file
path = tempfile("flights", fileext = ".sqlite")
con = DBI::dbConnect(RSQLite::SQLite(), path)
tbl = DBI::dbWriteTable(con, "flights", as.data.frame(flights))
DBI::dbDisconnect(con)

# remove in-memory data
rm(flights)
```

### Preprocessing with `dplyr`

With the SQLite database in `path`, we now re-establish a connection and switch to `r cran_pkg("dplyr")`/`r cran_pkg("dbplyr")` for some essential preprocessing.

```{r 05-technical-databases-002}
# establish connection
con = DBI::dbConnect(RSQLite::SQLite(), path)

# select the "flights" table, enter dplyr
library("dplyr")
library("dbplyr")
tbl = tbl(con, "flights")
```

First, we select a subset of columns to work on:

```{r 05-technical-databases-003}
keep = c("row_id", "year", "month", "day", "hour", "minute", "dep_time",
  "arr_time", "carrier", "flight", "air_time", "distance", "arr_delay")
tbl = select(tbl, keep)
```

Additionally, we remove those observations where the arrival delay (`arr_delay`) has a missing value:

```{r 05-technical-databases-004}
tbl = filter(tbl, !is.na(arr_delay))
```

To keep runtime reasonable for this toy example, we filter the data to only use every second row:

```{r 05-technical-databases-005}
tbl = filter(tbl, row_id %% 2 == 0)
```

The factor levels of the feature `carrier` are merged so that infrequent carriers are replaced by level "other":

```{r 05-technical-databases-006}
tbl = mutate(tbl, carrier = case_when(
    carrier %in% c("OO", "HA", "YV", "F9", "AS", "FL", "VX", "WN") ~ "other",
    TRUE ~ carrier)
)
```

### DataBackendDplyr

The processed table is now used to create a `r ref("mlr3db::DataBackendDplyr")` from `r mlr_pkg("mlr3db")`:

```{r 05-technical-databases-007}
library("mlr3db")
b = as_data_backend(tbl, primary_key = "row_id")
```

We can now use the interface of `r ref("DataBackend")` to query some basic information of the data:

```{r 05-technical-databases-008}
b$nrow
b$ncol
b$head()
```

Note that the `r ref("DataBackendDplyr")` does not know about any rows or columns we have filtered out with `r cran_pkg("dplyr")` before, it just operates on the view we provided.

### Model fitting

We create the following `r mlr_pkg("mlr3")` objects:

* A `r ref("TaskRegr", text = "regression task")`, based on the previously created `r ref("mlr3db::DataBackendDplyr")`.
* A regression learner (`r ref("mlr_learners_regr.rpart", text = "regr.rpart")`).
* A resampling strategy: 3 times repeated subsampling using 2\% of the observations for training ("`r ref("mlr_resamplings_subsampling", text = "subsampling")`")
* Measures "`r ref("mlr_measures_regr.mse", text = "mse")`", "`r ref("mlr_measures_time_train", text = "time_train")`" and "`r ref("mlr_measures_time_predict", text = "time_predict")`"

```{r 05-technical-databases-009}
task = TaskRegr$new("flights_sqlite", b, target = "arr_delay")
learner = lrn("regr.rpart")
measures = mlr_measures$mget(c("regr.mse", "time_train", "time_predict"))
resampling = rsmp("subsampling")
resampling$param_set$values = list(repeats = 3, ratio = 0.02)
```

We pass all these objects to `r ref("resample()")` to perform a simple resampling with three iterations.
In each iteration, only the required subset of the data is queried from the SQLite data base and passed to `r ref("rpart::rpart()")`:

```{r 05-technical-databases-010}
rr = resample(task, learner, resampling)
print(rr)
rr$aggregate(measures)
```

### Cleanup

Finally, we remove the `tbl` object and close the connection.

```{r 05-technical-databases-011}
rm(tbl)
DBI::dbDisconnect(con)
```


### Converter from in-memory to out-of-memory

If you happen to be in a situation where you already have the data in memory, `mlr3` provides some convenient converters.
This makes sense for the following scenarios:

* You have a rather large task on which you want to operate on in parallel and communicating the data to the workers is too expensive.
* You want to work with many medium sized tasks simultaneously, e.g. benchmark them.
  Holding some of them in the main memory is no problem, but their combined memory requirement exceeds your available memory.

Instead of setting up a real DBMS server and transferring the data over there, we instead use a self-contained file-based database to convert a data frame or `r ref("DataBackend")`.
We show how to convert to a DuckDB with `r ref("as_duckdb_backend()")` (replace the call with `r ref("as_sqlite_backend()")` to use SQLite):

```{r}
# convert from data.frame:
data("flights", package = "nycflights13")
backend = as_duckdb_backend(flights)
task = TaskRegr$new("flights", backend, target = "arr_delay")

# convert from backend:
task = TaskRegr$new("flights", flights, target = "arr_delay")
task$backend = as_duckdb_backend(task$backend)
```

The default location to store the database files can be set via `r ref("as_duckdb_backend()")`'s argument `path` or configured via option `"mlr3db.duckdb_dir"`.
In the defaults, the database is stored in the temporary directory of the currently running R session.
Note that this will not work well if you distribute your computations on remote workers.
If you have a shared home directory, set the `path` to `":user:"` instead which uses `r ref("tools::R_user_dir()")` to determine a standard-compliant location for your operating system.
```{r, eval = FALSE}
backend = as_duckdb_backend(flights, path = ":user:")
```

Note that the converted backends come with two additional features:

1. Automatic (re-)connection if the connection was lost or has not been established (e.g., on a remote machine).
2. Automatic disconnection if the backend is garbage collected via a finalizer (see `r ref("reg.finalizer")`).
