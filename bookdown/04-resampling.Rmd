# Resampling {#resampling}

```{r setup-04, include = FALSE, comment="", results="asis"}
library(fansi)
options(crayon.enabled = TRUE)
old.hooks <- fansi::set_knit_hooks(knitr::knit_hooks)

knitr::opts_chunk$set(collapse = TRUE)
```

## Structure {#resamp-structure}

In this example we use the _iris_ task and a simple classification tree (package `rpart`).

```{r 04-resampling-1}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
```

When performing resampling with a dataset, we first need to define which approach should be used.
The resampling strategies of _mlr3_ can be queried using the `.$keys()` function of the `mlr_resampling` dictionary.

```{r 04-resampling-2}
mlr_resamplings$keys()
```

Additional resampling methods for special use cases will be available via extension packages, such as [mlr3spatiotemporal](https://github.com/mlr-org/mlr3spatiotemporal) for spatial data (still in development).

The experiment conducted in the [train/predict/score](#train-predict) chapter is equivalent to "holdout", so let's consider this one first.

```{r 04-resampling-3}
resampling = mlr_resamplings$get("holdout")
print(resampling)
print(resampling$param_set$values)
```

Note that the `Instantianated` field is set to `FALSE`.
This means we did not actually apply the strategy on a dataset yet but just performed a dry-run.
Applying the strategy on a dataset is done in section next [Instantation](#instantation).

By default we get a .66/.33 split of the data.
There are two ways how the ratio can be changed:

1. Overwriting the slot in `.$param_set$values` using a named list.

  ```{r 04-resampling-4}
  resampling$param_set$values = list(ratio = 0.8)
  ```
  
2. Specifying the resampling parameters directly during creation using the `param_vals` argument:

  ```{r}
  mlr_resamplings$get("holdout", param_vals = list(ratio = 0.8))
  ```
  
## Instantation {#resamp-inst}

So far we just set the stage and selected the resampling strategy.
To actually apply the splitting, we need to apply the settings on a dataset.
This can be donerelied on automatic instantiation of the resampling strategy when calling `resample()`.

If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation.
Until now, we have just passed a resampling strategy to `resample()`, without specifying the actual splits into training and test.
Here, we manually instantiate the resampling:

```{r 04-resampling-10}
resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
resampling$instantiate(task)
resampling$iters
resampling$train_set(1)
```

If we now pass this instantiated object to `resample()`, the precalculated training and test splits will be used for both learners:

```{r 04-resampling-11}
learner1 = mlr_learners$get("classif.rpart") # simple classification tree
learner2 = mlr_learners$get("classif.featureless") # featureless learner, prediction majority class
rr1 = resample(task, learner1, resampling)
rr2 = resample(task, learner2, resampling)

setequal(rr1$experiment(1)$train_set, rr2$experiment(1)$train_set)
```

We can also combine the created result objects into a `BenchmarkResult` (see below for an introduction to simple benchmarking):

```{r 04-resampling-12}
bmr = rr1$combine(rr2)
bmr$aggregated(objects = FALSE)
```


## Execution {#resamp-exec}

After we created a [resampling instance](#resamp-inst) we can pass it to the `resample()` function.
Together with a [task](#tasks) and a [learner](#learners) we can create an object of class `ResampleResult`:

```{r 04-resampling-5}
rr = resample(task, learner, resampling)
print(rr)
```

Before we go into more detail, let's change the resampling to a 3-fold cross-validation to better illustrate what operations are possible with a `ResampleResult`.

```{r 04-resampling-6}
resampling = mlr_resamplings$get("cv", param_vals = list(folds = 3L))
rr = resample(task, learner, resampling)
print(rr)
```

We can do different things with resampling results, e.g.:

* Extract the performance for the individual resampling iterations:

  ```{r 04-resampling-7}
  rr$performance("classif.ce")
  ```

* Extract and inspect the resampling splits:

  ```{r 04-resampling-8}
  rr$resampling
  rr$resampling$iters
  rr$resampling$test_set(1)
  rr$resampling$train_set(3)
  ```

* Retrieve the experiment of a specific iteration and inspect it:

  ```{r 04-resampling-9}
  e = rr$experiment(iter = 1)
  e$model
  ```

## Custom resampling

Sometimes it is necessary to perform resampling with custom splits, e.g. to reproduce a study.
For this purpose, splits can be manually set for `ResamplingCustom`:

```{r 04-resampling-13}
resampling = mlr_resamplings$get("custom")
resampling$instantiate(task,
  list(c(1:10, 51:60, 101:110)),
  list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```

