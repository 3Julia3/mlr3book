## Cost-Sensitive Classification

Imagine you are an analyst for a big credit institution.
Let's also assume that a correct decision of the bank would result in 35% of the profit at the end of a specific period.
A correct decision means that the bank predicts that a customer will pay their bills (hence would obtain a loan), and the customer indeed has good credit.
On the other hand, a wrong decision means that the bank predicts that the customer's credit is in good standing, but the opposite is true.
This would result in a loss of 100% of the given loan.

|                           | Good Customer (truth)       | Bad Customer (truth)       |
| :-----------------------: | :-------------------------: | :------------------------: |
| Good Customer (predicted) | + 0.35                      | - 1.0                      |
| Bad Customer (predicted)  | 0                           | 0                          |


Expressed as costs (instead of profit), we can write the this in R as matrix as follows:
```{r}
costs = matrix(c(-0.35, 0, 1, 0), nrow = 2)
dimnames(costs) = list(response = c("good", "bad"), truth = c("good", "bad"))
print(costs)
```

We will work on the data from the `r ref("mlr_tasks_german_credit", text = "German Credit Data")` task:
```{r}
library(mlr3)
task = mlr_tasks$get("german_credit")
table(task$truth())
```
The data has 70% good customers and 30% bad customers.
A manager, who doesn't have any model and who gives everybody the loan would yield the following profit per customer:
```{r}
confusion = matrix(c(700, 0, 300, 0), nrow = 2)
dimnames(confusion) = dimnames(costs)
print(confusion)

avg_costs = sum(confusion * costs) / 1000
print(avg_costs)
```
If the average loan is $20,000, the credit institute would loose more than one million dollar:
```{r}
# average profit * average loan * number of customers
0.055 * 20000 * 1000
```
Our goal is to find a model which minimizes the costs (and thereby maximizes the expected profit).

### A First Model

For our first model, we choose an ordinary logistic regression (implemented in the add-on package `r mlr_pkg("mlr3learners")`).
We first create a classification task, then resample the model using a 10-fold cross validation and extract the resulting confusion matrix:
```{r}
library(mlr3learners)
learner = mlr_learners$get("classif.log_reg")
rr = resample(task, learner, "cv")

confusion = rr$prediction$confusion
print(confusion)
```

To calculate the average costs, we again multiply with the previously introduced cost matrix:
```{r}
avg_costs = sum(confusion * costs) / 1000
print(avg_costs)
```
With an average loan of \$20,000, the logistic regression yields the following costs:
```{r}
avg_costs * 20000 * 1000
```
Instead of loosing over \$1,000,000, we now easily earn more than \$1,000,000.


### Cost-sensitive Measure

Our natural next step would be to further improve the modeling step in order to maximize the profit.
For this purpose we first create a cost-sensitive classification measure which calculates the costs based on our cost matrix.
This allows us to conveniently compare different modeling decisions.
Fortunately, there already is a predefined measure `r ref("Measure")` for this purpose: `r ref("MeasureClassifCosts")`:

```{r}
cost_measure = MeasureClassifCosts$new("credit_costs", costs)
print(cost_measure)
```
We now define this measure as default measure for our task.
```{r}
task$measures = list(cost_measure)
```
If we now call `r ref("resample()")` or `r ref("benchmark()")`, the cost-sensitive measures will be evaluated.
We compare the logistic regression to a simple featureless learner and a random forest from package `r cran_pkg("ranger")` :
```{r}
learners = mlr_learners$mget(c("classif.log_reg", "classif.featureless", "classif.ranger"))
bmr = benchmark(expand_grid(task, learners, "cv"))
print(bmr)
```
As expected, the featureless learner is performing comparably bad.
The logistic regression and the random forest work equally well.


### Thresholding

Although we now correctly evaluate the models in a cost-sensitive fashion, the models themselves are unaware of the classification costs.
They assume the same costs for both wrong classification decisions.
Some learners natively support cost-sensitive classification (e.g., XXX).
However, we will concentrate on a more generic approach which works for all models which can predict probabilities for class labels: thresholding.
Most learners can calculate the probability $p$ for the positive class.
If $p$ exceeds the threshold $0.5$, they predict the positive class, and the negative class otherwise.

For our binary classification case of the credit data, the we primarily want to minimize the errors where the model predicts "good", but truth is "bad" (i.e., the number of false positives) as this is the more expensive error.
If we now increase the threshold to values $> 0.5$, we reduce the number of false negatives.

```{r}
# fit models with probability prediction
learner = mlr_learners$get("classif.log_reg", predict_type = "prob")
rr = resample(task, learner, "cv")
p = rr$prediction
print(p)

# helper function to try different threshold values interactively
with_threshold = function(p, th) {
  p$response = p$set_threshold(th)$response
  list(confusion = p$confusion, costs = cost_measure$calculate(prediction = p))
}

with_threshold(p, 0.5)
with_threshold(p, 0.75)
with_threshold(p, 1.0)

# TODO: include plot of threshold vs performance
```

Instead of manually trying different threshold values, we here use `r ref("optimize()")` to find a good threshold value.
```{r}
# simple wrapper function which takes a threshold and returns the resulting model performance
# this wrapper is passed to optimize() to find its minimum for thresholds in [0.5, 1]
f = function(th) {
  with_threshold(p, th)$costs
}
best = optimize(f, c(0.5, 1))
print(best)

# optimized confusion matrix:
with_threshold(p, best$minimum)$confusion
```
```{block, type = "warning"}
The function `optimize()` is intended for unimodal functions and therefore may converge to a local optimum here.
See below for better alternatives to find good threshold values.
```


### Threshold Tuning

To be continued...

* threshold tuning as pipeline operator
* joint hyperparameter optimization
