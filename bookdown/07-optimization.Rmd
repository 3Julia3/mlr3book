# Model Optimization

## Hyperparameter Tuning {#tuning}

Hyperparameter tuning is supported via the extension package `r gh_pkg("mlr-org/mlr3tuning")`.
The heart of `r gh_pkg("mlr-org/mlr3tuning")` are the R6 classes `r ref("mlr3tuning::FitnessFunction")` and the `Tuner*` classes.
They store the settings, perform the tuning and save the results.

### The `Fitness Function` class

The `r ref("mlr3tuning::FitnessFunction")` class requires the following inputs from the user:

- `r ref("Task")`
- `r ref("Learner")`
- `r ref("Resampling")`
- `r ref("Measure")`
- `r ref("paradox::ParamSet")` 

It is similar to `r ref("resample")` and `r ref("benchmark")` with the additional requirement of a "Parameter Set" (`r ref("paradox::ParamSet")` ) specifying the Hyperparameters of the given learner which should be optimized.

An exemplary definition could looks as follows:

```{r 07-optimization-1 }
task = mlr3::mlr_tasks$get("iris")
learner = mlr3::mlr_learners$get("classif.rpart")
resampling = mlr3::mlr_resamplings$get("holdout")
measures = mlr3::mlr_measures$mget("classif.ce")
task$measures = measures
param_set = paradox::ParamSet$new(params = list(
  paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  paradox::ParamInt$new("minsplit", lower = 1, upper = 10)))

ff = FitnessFunction$new(
  task = task,
  learner = learner,
  resampling = resampling,
  param_set = param_set
)
```

**Evaluation of Single Parameter Settings**

Using the method `.$eval()`, the `r ref("mlr3tuning::FitnessFunction")` is able to tune a specific set of hyperparameters on the given inputs.
The parameters have to be handed over wrapped in a `r ref("data.table")`:

```{r 07-optimization-2 }
ff$eval(data.table::data.table(cp = 0.05, minsplit = 5))
```

The results are stored in a `r ref("BenchmarkResult")` class within the `ff` object.
Note that this is the "bare bone" concept of using hyperparameters during `r ref("Resampling")`.
Usually you want to [optimize the parameters in an automated fashion](#tuning-spaces).

### Tuning Hyperparameter Spaces {#tuning-spaces}

Most often you do not want to only check the performance of fixed hyperparameter settings sequentially but optimize the outcome using different hyperparameter choices in an automated way.

To achieve this, we need a definition of the search spaced that should be optimized.
Let's use again the space we defined in the [introduction](#tuning-intro).

```{r 07-optimization-3, eval = FALSE}
paradox::ParamSet$new(params = list(
  paradox::ParamDbl$new("cp", lower = 0.001, upper = 0.1),
  paradox::ParamInt$new("minsplit", lower = 1, upper = 10)))
```

To start the tuning, we still need to select how the optimization should take place - in other words, we need to choose the **optimization algorithm**.

The following algorithms are currently implemented in `r gh_pkg("mlr-org/mlr3")`:

- Grid Search (`r ref("mlr3tuning::TunerGridSearch")`) 
- Random Search (`r ref("mlr3tuning::TunerRandomSearch")`) [@bergstra2012]
- Generalized Simulated Annealing (`r ref("mlr3tuning::TunerGenSA")`) 

In this example we will use a simple "Grid Search".
Since we have only numeric parameters and specified the upper and lower bounds for the search space, `r ref("mlr3tuning::TunerGridSearch")` will create a grid of equally-sized steps. 
By default, `r ref("mlr3tuning::TunerGridSearch")` creates ten equal-sized steps. 
The number of steps can be changed with the `resolution` argument.
In this example we use 15 steps and create a new class `r ref("mlr3tuning::TunerGridSearch")` using the `r ref("mlr3tuning::FitnessFunction")` `ff` and the resolution.

```{r 07-optimization-4, error=TRUE}
tuner_gs = TunerGridSearch$new(ff, resolution = 15)
```

Oh! The error message tells us that we need to specify an addition argument called `terminator`.

### Defining the Terminator

What is a "Terminator"? 
The `r ref("mlr3tuning::Terminator")` defines when the tuning should be stopped.
This setting can have various instances:

- Terminate after a given time (`r ref("mlr3tuning::TerminatorRuntime")`)
- Terminate after a given amount of iterations (`r ref("mlr3tuning::TerminatorEvaluations")`)
- Terminate after a specific performance is reached (`r ref("mlr3tuning::Performance")`)

Often enough one termination criterion is not enough.
For example, you will not know beforehand if all of your given evaluations will finish within a given amount of time.
This highly depends on the `r ref("Learner")` and the `r ref("paradox::ParamSet")` given.
However, you might not want to exceed a certain tuning time for each learner.
In this case, it makes sense to combine both criteria using `r ref("mlr3tuning::TerminatorMultiplexer")`.
Tuning will stop as soon as one Terminator signals to be finished.

In the following example we create two terminators and then combine them into one:

```{r 07-optimization-5 }
tr = TerminatorRuntime$new(max_time = 5, units = "secs")
te = TerminatorEvaluations$new(max_evaluations = 50)

tm = TerminatorMultiplexer$new(list(te, tr))
tm
```

### Executing the Tuning

Now that we have all required inputs (`r ref("paradox::ParamSet")`, `r ref("mlr3tuning::Terminator")` and the optimization algorithm), we can perform the hyperparameter tuning.

The first step is to create the respective "Tuner" class, here `r ref("mlr3tuning::TunerGridSearch")`.

```{r 07-optimization-6 }
tuner_gs = TunerGridSearch$new(ff = ff, terminator = tm,  
  resolution = 15)
```

After it has been initialized, we can call its member function `.$tune()` to run the tuning.

```{r 07-optimization-7 }
tuner_gs$tune()
```

`.$tune()` simply performs a `r ref("benchmark")` on the parameter values generated by the tuner and writes the results into a `r ref("BenchmarkResult")` object which is stored in field `.$bmr` of the `r ref("mlr3tuning::FitnessFunction")` object that we passed to it.

The actual tuning result is 


### Inspecting Results

During the `.$tune()` call not only the `r ref("BenchmarkResult")` output was written to the `.$bmr` slot of the `r ref("mlr3tuning::FitnessFunction")` but also the `r ref("mlr3tuning::Terminator")` got updated.

We can take a look by directly printing the `r ref("mlr3tuning::Terminator")` object:

```{r 07-optimization-8 }
print(tm)
```

We can easily see that all evaluations were executed before the time limit kicked in.

Now let's take a closer look at the actual tuning result.
It can be queried using `.$tune_result()` from the respective `r ref("mlr3tuning::Tuner")` class that generated it.
Internally, the function scrapes the data from the `r ref("BenchmarkResult")` that was generated during tuning and stored in `.$ff$bmr`.

```{r 07-optimization-9 }
tuner_gs$tune_result()
```

It returns the scored performance and the values of the optimized hyperparameters.
Note that each measure “knows” if it was minimized or maximized during tuning:

```{r 07-optimization-10 }
task$measures$classif.ce$minimize
```

A summary of the `r ref("BenchmarkResult")` created by the tuning can be queried using the `.$aggregated()` function of the `Tuner` class. 

```{r 07-optimization-11 }
tuner_gs$aggregated()
```

### Summary

- Use `FitnessFunction$eval()` for manual execution of parameters in `r ref("Resampling")`
- Define a `Tuner` of your choice using a `r ref("mlr3tuning::FitnessFunction")` with the following inputs
  - `r ref("Learner")`
  - `r ref("Task")`
  - `r ref("Resampling")`
  - `r ref("paradox::ParamSet")`
  - `r ref("mlr3tuning::Terminator")`
- Inspect the tuning result using `Tuner*$tune_result()`
- Get a summary view of all runs based on the `r ref("BenchmarkResult")` object created during tuning using `Tuner*$aggregated()`

## Feature Selection / Filtering {#fs}

Often, data sets include a large number of features. 
The technique of extracting a subset of relevant features is called "feature selection". 
Feature selection can enhance the interpretability of the model, speed up the learning process and improve the learner performance. 
Different approaches exist to identify the relevant features. 
In the literature two different approaches exist: One is called “Filtering” and the other approach is often referred to as “feature subset selection” or “wrapper methods”.

What is the difference?

- **Filter**: An external algorithm computes a rank of the variables (e.g. based on the correlation to the response). 
  Then, features are subsetted by a certain criteria, e.g. an absolute number or a percentage of the number of variables. 
  The selected features will then be used to fit a model (with optional hyperparameters selected by tuning). 
  This calculation is usually cheaper than “feature subset selection” in terms of computation time.
- **Feature subset selection**: Here, no ranking of features is done.
  Features are selected by a (random) subset of the data. 
  Then, a model is fit and the performance is checked. 
  This is done for a lot of feature combinations in a CV setting and the best combination is reported. 
  This method is very computational intense as a lot of models are fitted. 
  Also, strictly all these models would need to be tuned before the performance is estimated which would require an additional nested level in a CV setting. 
  After all this, the selected subset of features is again fitted (with optional hyperparameters selected by tuning).

There is also a third approach which can be attributed to the "filter" family: The embedded feature-selection methods of some `r ref("Learner")`.
Read more about how to work with these in the section about [embedded feature-selection methods](#fs-embedded).

All feature selection related functionality is implented via the extension package `r gh_pkg("mlr-org/mlr3featsel")`.

### Filters {#fs-filter}

Filter methods assign an importance value to each feature. 
Based on these values the features can be ranked and a feature subset can be selected. 
You can see here which algorithms are implemented.


### Wrapper Methods {#fs-wrapper}

```{block 07-optimization-12, type='warning'}
Work in progress :)
```

### Embedded Methods {#fs-embedded}

```{block 07-optimization-13, type='warning'}
Work in progress :)
```

### Ensemble Methods {#fs-ensemble}

```{block 07-optimization-14, type='warning'}
Work in progress :)
```

## Preprocessing Operations {#preproc}

```{block 07-optimization-15, type='warning'}
Work in progress :)
```

## Ensemble Models / Stacking {#stacking}

```{block 07-optimization-16, type='warning'}
Work in progress :)
```
