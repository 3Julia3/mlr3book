# Pipelines {#pipelines}

```{r vis_setup, message = FALSE, warning = FALSE, echo = FALSE, include = FALSE}
knitr::opts_knit$set(root.dir = fs::dir_create(tempfile()))
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
library(visNetwork)
```


```{r}
library(mlr3pipelines)
```

## The Pipelines Operator {#pipe-operator}

## Nodes, Edges and Graphs {#pipe-nodes-edges-graphs}

## Graph Operators {#pipe-graph-operators}

## Preprocessing {#pipe-preprocessing}

### Imputation

## Model Ensembles {#pipe-model-ensembles}

We can leverage the different operations available to connect `PipeOps` to form very powerful graphs.
This vignette introduces two such well-known structures, that allow us to enhance single learners
to more powerful combinations.

Before we go into details, we split the task into train
and test indices.

```{r}
  library(mlr3)
  library(mlr3pipelines)
  task = mlr_tasks$get("iris")
  train.idx = sample(seq_len(task$nrow), 120)
  test.idx = setdiff(seq_len(task$nrow), train.idx)
```


### Bagging

We first examine Bagging introduced by Breimann (1994).
The basic idea is to create multiple predictors and then aggregate those to a single, more powerfull predictor.

> "... multiple versions are formed
> by making bootstrap replicates of the learning set
> and using these as new learning sets" (Breimann 1994)

Bagging then aggregates a set of predictors by averaging (regression) or majority vote (classification).
The idea behind bagging is, that a set of weak, but different predictors can be combined in order to arrive
at a single, better predictor.

We can achieve this by downsampling our data before training a learner, repeating this for say $10$ times and then performing a majority vote on the predictions.


First, we create a simple pipeline, that uses
`PipeOpSubsample` before a `PipeOpLearner` is trained:

```{r}
  single_pred = PipeOpSubsample$new(param_vals = list(frac = 0.7)) %>>%
    PipeOpLearner$new(mlr_learners$get("classif.rpart"))
```

We can now copy this operation $10$ times using `greplicate`.

```{r}
  pred_set = greplicate(single_pred, 10L)
```

Afterwards we need to aggregate the 10 pipelines
to form a single model:

```{r}
  bagging = pred_set %>>%
    PipeOpMajorityVote$new(innum = 10L)
```

and plot again to see what happens:

```{r, fig.width=7.5}
  bagging$plot(html = TRUE)
```

This pipeline can again be used in conjunction with
`GraphLearner` in order for Bagging to be used like a
[mlr3::Learner].

```{r}
  baglrn = GraphLearner$new(bagging)
  baglrn$train(task, train.idx)
  baglrn$predict(task, test.idx)
```

In conjunction with different `Backends`, this can be a very powerful tool, as in cases, where the data does not fully fit in memory, we can easily just obtain a fraction of the data for each learner from a `DataBaseBackend` and then aggregate predictions from all learners.


### Stacking

Stacking (Wolpert, 1992) is another technique that can improve model performance. The basic idea behind stacking is, that
using predictions from one model as features of a subsequent model can possibly improve performance. 

A very simple possibility would be to train a
decision tree and use the predictions from this
model in conjunction with the original features in order to train an additional model on top.
The basic idea behind this is, that patterns a model detected in the data can be used by a higher level model, and thus result in a better performance.

In order to limit overfitting, we additionally do not predict on
the original predictions of the learner, but instead on out-of-bag predictions. This is implemented in `PipeOpLearnerCV`.

`PipeOpLearnerCV` performs nested cross-validation on the training data, fitting
a model in each fold. Each of the models is then used to predict on the out-of-fold
data. As a result, we obtain predictions for every data point in our input data.

We first create a level $0$ learner, which is used to
extract a lower level prediction.
We additionally `clone()` the learner object to obtain a copy
of the learner, and set a custom id for the `PipeOp`.

```{r}
  lrn = mlr_learners$get("classif.rpart")
  lrn_0 = PipeOpLearnerCV$new(lrn$clone())
  lrn_0$id = "rpart_cv"
```

Additionally, we use a `PipeOpNULL` in parallel to
the level 0 learner, in order to send the unchanged
Task to the next level, where it is then combined with the 
predictions from our decision tree learner.

```{r}
  level_0 = gunion(list(lrn_0, PipeOpNULL$new()))
```

Afterwards, we want to concatenate the predictions from
`PipeOpLearnerCV` and the original Task using `PipeOpFeatureUnion`.

```{r}
  combined = level_0 %>>% PipeOpFeatureUnion$new(2)
```

We can now train another learner on top of the combined
features.

```{r, fig.width=7.5}
  stack = combined %>>% PipeOpLearner$new(lrn$clone())
  stack$plot(html = TRUE)
```

```{r}
  stacklrn = GraphLearner$new(stack)
  stacklrn$train(task, train.idx)
  stacklrn$predict(task, test.idx)
```

In this vignette, we only showcase a very simple usecase for stacking.
In many real-world applications, stacking is done for multiple levels and on multiple representations of the dataset. On a lower level, different preprocessing methods can for example
be defined in conjunction with several learners. On a higher level, we can then combine
those predictions in order to form a very powerfull model.



#### Multilevel Stacking

In order to showcase the power of `mlr3pipelines`, we will quickly introduce a more complicated
stacking example.

In this case, we train a **glmnet** and 2 different *rpart* models (some transform its inputs using `PipeOpPCA`) on our task in the level 0 and concatenate them with the original features (via `PipeOpNull`).
This is then passed on to level 1, where we copy the concatenated features
$3$ times and put this task into a **rpart** and a **glmnet** model.
Additionally, we also keep a version of the level 0 output (via `PipeOpNull`) and pass this on to level 2. In level 2 we simply concatenate all level 1 outputs and train a final decision tree.


```{r, eval = FALSE}
  library(mlr3learners)
  rprt = mlr_learners$get("classif.rpart")
  rprt$predict_type = "prob"
  glmn = mlr_learners$get("classif.glmnet")
  glmn$predict_type = "prob"

  #  Create Learner CV Operators
  lrn_0 = PipeOpLearnerCV$new(rprt, id = "rpart_cv_1")
  lrn_0$values$maxdepth = 5L
  lrn_1 = PipeOpPCA$new(id = "pca1") %>>% PipeOpLearnerCV$new(rprt, id = "rpart_cv_2")
  lrn_1$values$maxdepth = 1L
  lrn_2 = PipeOpPCA$new(id = "pca2") %>>% PipeOpLearnerCV$new(glmn)

  # Union them with a PipeOpNULL to keep original features
  level_0 = gunion(list(lrn_0, lrn_1,lrn_2, PipeOpNULL$new(id = "NULL1")))

  # Cbind the output 3 times, train 2 learners but also keep level
  # 0 predictions
  level_1 = level_0 %>>%
    PipeOpFeatureUnion$new(4) %>>%
    PipeOpCopy$new(3) %>>%
    gunion(list(
      PipeOpLearnerCV$new(rprt, id = "rpart_cv_l1"),
      PipeOpLearnerCV$new(glmn, id = "glmnt_cv_l1"),
      PipeOpNULL$new(id = "NULL_l1")
    ))

  # Cbind predicitions, train a final learner.
  level_2 = level_1 %>>%
    PipeOpFeatureUnion$new(3, id = "u2") %>>%
    PipeOpLearner$new(rprt,
      id = "rpart_l2")

  # Plot the resulting graph
  level_2$plot(html = TRUE)

  task = mlr_tasks$get("iris"),
  lrn = GraphLearner$new(level_2)

  lrn$
    train(task, train.idx)$
    predict(task, test.idx)$
    score()
```

## Branching {#pipe-branching}

Consider a scenario, in which there are different options for a given modeling step.
A researcher might, in an initial step want to try different options for this.
This can be done by specifying **branching** and **unbranching** operators.
In the long term, this also allows to tune over a conditional space, i.e. to learn which preprocessing options make sense
for our data.

We will investigate the inner workings of a Graph containing conditional operations in this chapter.

In this example we consider three options for preprocessing our data before we train a model. In a first step we define the different preprocessing steps:
We will consider scaling the data, transforming it using PCA, or not transforming the data at all (using `PipeOpNULL`).

The `gunion` function puts PO's in parallel to each other. This means they are currently not connected to each other, but will
instead all be connected to a preceding operator.

```{r}
op1 = PipeOpScale$new()
op2 = PipeOpPCA$new()
op3 = PipeOpNULL$new()
opts = gunion(list(op1, op2, op3))
```

Additionally, we create the learner `PipeOp`, we want to use after transformation.

```{r}
polrn = PipeOpLearner$new(mlr_learners$get("classif.rpart"))
```

In order to create a mental image of our Pipeline, we have to consider, how the data flows through our
computational graph. In case we simply want  to do scaling, our pipeline would look like this:

```{r}
(op1 %>>% polrn)$plot(html=TRUE)
```

If we want to actively select one `PipeOp` in a set, we need to add a `PipeOp` that orchestrates the selection of
the following `PipeOp` using `PipeOpBranch`. 

After choosing one `PipeOp`, all not-selected operators simply recieve a `NO_OP` object.
We can now collect the activated/deactivated `PipeOp`s again using `PipeOpUnBranch`.
This will become clearer in a short example.

### PipeOpBranch

We create a `PipeOpBranch` that let's us select one of
the following $3$ operators. We have to initialize
this `PipeOp` with the number of output branches.

```{r}
po_b = PipeOpBranch$new(3)
```

This can be connected to the different preprocessing features.

```{r}
g = po_b %>>% opts
```

### PipeOpUnbranch

In order to collect the different inputs again, we use the
`PipeOpUnbranch` operator. Again, we have to initialize this
operator with the number of inputs it expects.

```{r}
po_ub = PipeOpUnbranch$new(3)
```

```{r}
g = g %>>% po_ub
```

We can visualize the resulting graph before diving into a
more detailed explanation.

```{r}
g$plot(html = TRUE)
```

## Tuning {#pipe-tuning}
