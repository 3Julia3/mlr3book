---
title: "Chap 3 Exercises"
format: html
editor_options: 
  chunk_output_type: console
---

Workshopping exercises & solutions here until they can be moved into place

```{r setup}
library(mlr3verse)
```

1. You want to evaluate a decision tree classification learner (`classif.rpart`) on the `zoo` task, but you're unsure which resampling strategy is appropriate.
One of your colleagues recommends to use 5-fold cross-validation while another colleague swears by leave-one-out cross-validation ("LOO").
Your goal is to maximize classification accuracy.
Which strategy would you use?
Try out both and visualize the resulting scores per resampling iteration.
Which strategy seems to be more reliable in this scenario? Why?

FIXME: This is a confusing edge case and needs better explanation


```{r}
set.seed(3)
rr1 = resample(
  task = tsk("zoo"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("cv", folds = 5)
)

rr2 = resample(
  task = tsk("zoo"),
  learner = lrn("classif.rpart"),
  resampling = rsmp("loo")
)


measure = msr("classif.acc")

rr1$aggregate(measure)
rr2$aggregate(measure)

mlr3viz::autoplot(rr1, measure = measure)
mlr3viz::autoplot(rr2, measure = measure)
```


```{r}
rr2$score(measure)[, .(resampling_id, iteration, classif.acc)]
```


## Vague ideas

- Case where one CV fold errors / needs further investigation?
- Can't think of smaller projects that don't cut into the optimization chapter (e.g. nested resampling, tuning :/)

## Manual Resampling Pitfalls

You are working the the simplified `penguins` task (`tsk("penguins_simple")`)), and want to evaluate the `rpart` classification learner using a 50% train-test split.
Create a custom resampling strategy that uses the first 166 rows as a training set and the remaining rows as a test set.
Calculate the classification accuracy for both the train and the test set.
Do you achieve the performance you would have expected?
Repeat the procedure but using the `"holdout"` strategy with a `ratio` of 0.5.
What causes the difference in test set performance?

-> Goal is to illustrate how custom resampling can be done, and how it can be done badly if one doesn't account for e.g. pre-sorted data.
Should illustrate that the built-in resampling procedures are usually a better place to start than home-brewing something.
Also showcases `predict_sets` I guess?

```{r}
task = tsk("penguins_simple")
resampling_custom = rsmp("custom")

resampling_custom$instantiate(
  task = task,
  train_sets = list(1:166),
  test_sets = list(167:333)
)

# train_ids = resampling_custom$train_set(1)
# test_ids = resampling_custom$test_set(1)
# task$data(rows = train_ids)
# task$data(rows = test_ids)

rr1 = resample(
  task = task,
  learner = lrn("classif.rpart", predict_sets = c("train", "test")),
  resampling = resampling_custom
)

measures = list(
  msr("classif.acc", id = "train_acc", predict_sets = "train"), 
  msr("classif.acc", id = "test_acc", predict_sets = "test")
)

rr1$aggregate(measures)
```

Now using the regular `"holdout"` strategy

```{r}
set.seed(3)

resampling_holdout = rsmp("holdout", ratio = 0.5)
resampling_holdout$instantiate(task = task)

# train_ids = resampling_holdout$train_set(1)
# test_ids = resampling_holdout$test_set(1)
# task$data(rows = train_ids)
# task$data(rows = test_ids)

rr2 = resample(
  task = task,
  learner = lrn("classif.rpart", predict_sets = c("train", "test")),
  resampling = resampling_holdout
)

rr2$aggregate(measures)

```

Checking balancing of the `target` in the train sets explain the difference:
In the first case, the learner never got to see the `"Chinstrap"` class during training.

```{r}
table(task$data(cols = task$target_names, rows = resampling_custom$train_set(1)))

table(task$data(cols = task$target_names, rows = resampling_holdout$train_set(1)))
```


## <Better Benchmarking Exercise?>
