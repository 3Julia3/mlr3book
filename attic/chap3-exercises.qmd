---
title: "Chap 3 Exercises"
format: html
editor_options: 
  chunk_output_type: console
---

Workshopping exercises & solutions here until they can be moved into place

```{r setup}
library(mlr3verse)
```

## Vague ideas

- Case where one CV fold errors / needs further investigation?
- Can't think of smaller projects that don't cut into the optimization chapter (e.g. nested resampling, tuning :/)

## Manual Resampling Pitfalls

You are working the the simplified `penguins` task (`tsk("penguins_simple")`)), and want to evaluate the `rpart` classification learner using a 50% train-test split.
Create a custom resampling strategy that uses the first 166 rows as a training set and the remaining rows as a test set.
Calculate the classification accuracy for both the train and the test set.
Do you achieve the performance you would have expected?
Repeat the procedure but using the `"holdout"` strategy with a `ratio` of 0.5.
What causes the difference in test set performance?

-> Goal is to illustrate how custom resampling can be done, and how it can be done badly if one doesn't account for e.g. pre-sorted data.
Should illustrate that the built-in resampling procedures are usually a better place to start than home-brewing something.
Also showcases `predict_sets` I guess?

```{r}
task = tsk("penguins_simple")
resampling_custom = rsmp("custom")

resampling_custom$instantiate(
  task = task,
  train_sets = list(1:166),
  test_sets = list(167:333)
)

# train_ids = resampling_custom$train_set(1)
# test_ids = resampling_custom$test_set(1)
# task$data(rows = train_ids)
# task$data(rows = test_ids)

rr1 = resample(
  task = task,
  learner = lrn("classif.rpart", predict_sets = c("train", "test")),
  resampling = resampling_custom
)

measures = list(
  msr("classif.acc", id = "train_acc", predict_sets = "train"), 
  msr("classif.acc", id = "test_acc", predict_sets = "test")
)

rr1$aggregate(measures)
```

Now using the regular `"holdout"` strategy

```{r}
set.seed(3)

resampling_holdout = rsmp("holdout", ratio = 0.5)
resampling_holdout$instantiate(task = task)

# train_ids = resampling_holdout$train_set(1)
# test_ids = resampling_holdout$test_set(1)
# task$data(rows = train_ids)
# task$data(rows = test_ids)

rr2 = resample(
  task = task,
  learner = lrn("classif.rpart", predict_sets = c("train", "test")),
  resampling = resampling_holdout
)

rr2$aggregate(measures)

```

Checking balancing of the `target` in the train sets explain the difference:
In the first case, the learner never got to see the `"Chinstrap"` class during training.

```{r}
table(task$data(cols = task$target_names, rows = resampling_custom$train_set(1)))

table(task$data(cols = task$target_names, rows = resampling_holdout$train_set(1)))
```


## <Better Benchmarking Exercise?>
